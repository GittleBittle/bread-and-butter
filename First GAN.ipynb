{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NumberGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GittleBittle/bread-and-butter/blob/main/First%20GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJp-D51g0IDd"
      },
      "source": [
        "## **1) Importing Python Packages for GAN**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k5mFBuzzl2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12579af-418f-45d2-9195-d416aa8a2d32"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Dense, Reshape, Flatten\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "!mkdir generated_images"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘generated_images’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr-eZOzg0X79"
      },
      "source": [
        "## **2) Variables for Neural Networks & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RThZMDruz9cB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836cb985-276d-4624-fa9c-34b2a0b32073"
      },
      "source": [
        "img_width = 28\n",
        "img_height = 28\n",
        "channels = 1\n",
        "img_shape = (img_width, img_height, channels)\n",
        "latent_dim = 100\n",
        "adam = Adam(lr=0.0001)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3bcJZZg0cqy"
      },
      "source": [
        "## **3) Building Generator**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdiqZpri0iQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846811b5-107c-4376-9039-167f201e295d"
      },
      "source": [
        "from keras import activations\n",
        "def build_generator():\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Dense(256, input_dim=latent_dim))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(1024))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "  model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "  model.add(Reshape(img_shape))\n",
        "\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "generator = build_generator()\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 256)               25856     \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 256)              1024      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               131584    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 512)              2048      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1024)              525312    \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 1024)              0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1024)             4096      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 784)               803600    \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,493,520\n",
            "Trainable params: 1,489,936\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt6QsJCW0mcI"
      },
      "source": [
        "## **4) Building Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2JzEAPv0lKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2238d5-9b42-4a3a-dc48-7b97b6f3c2ff"
      },
      "source": [
        "def build_discriminator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               401920    \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 533,505\n",
            "Trainable params: 533,505\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbcKcKmA0q2S"
      },
      "source": [
        "## **5) Connecting Neural Networks to build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Ue3TEd0xLy"
      },
      "source": [
        "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')\n",
        "\n",
        "GAN = Sequential()\n",
        "discriminator.trainable = False\n",
        "GAN.add(generator)\n",
        "GAN.add(discriminator)\n",
        "\n",
        "GAN.compile(loss='binary_crossentropy', optimizer='adam')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WaNhBDwRwTG"
      },
      "source": [
        "## **6) Outputting Images**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQEJ0WbjRppy"
      },
      "source": [
        "#@title\n",
        "## **7) Outputting Images**\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "\n",
        "save_name = 0.00000000\n",
        "\n",
        "def save_imgs(epoch):\n",
        "    #generate 25 images to fit on a 5 x 5 grid for our animation!\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "    global save_name\n",
        "    save_name += 0.00000001\n",
        "    print(\"%.8f\" % save_name)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "            # axs[i,j].imshow(gen_imgs[cnt])\n",
        "            axs[i,j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(\"generated_images/%.8f.png\" % save_name)\n",
        "    print('saved')\n",
        "    plt.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE57Lk5V0xs2"
      },
      "source": [
        "## **7) Training GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egSJJvik00Iq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02e4d7e4-49da-4197-eeb8-852ef9f9d378"
      },
      "source": [
        "def train(epochs, batch_size=64, save_interval=200):\n",
        "  (X_train, _), (_, _) = fashion_mnist.load_data()\n",
        "  X_train = X_train / 127.5 -1.\n",
        "\n",
        "  valid = np.ones((batch_size, 1))\n",
        "  fakes = np.zeros((batch_size, 1))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "    imgs = X_train[idx]\n",
        "\n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    #Train the discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fakes)\n",
        "    d_loss = np.add(d_loss_real, d_loss_fake) *0.5\n",
        "  \n",
        "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
        "\n",
        "    #Training The GANNNN\n",
        "    g_loss = GAN.train_on_batch(noise, valid)\n",
        "    print(\"********* %d [D loss: %f, acc:, %.2f] [G loss: %f]\" % (epoch, d_loss[0], d_loss[1] *100, g_loss))\n",
        "  if(epoch % save_interval) == 0:\n",
        "      save_imgs(epoch)\n",
        "\n",
        " # print(X_train.shape)\n",
        "\n",
        "train(30000, batch_size=64, save_interval=200)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "********* 311 [D loss: 0.031848, acc:, 98.44] [G loss: 9.247357]\n",
            "********* 312 [D loss: 0.013083, acc:, 100.00] [G loss: 7.569630]\n",
            "********* 313 [D loss: 0.019721, acc:, 100.00] [G loss: 6.947188]\n",
            "********* 314 [D loss: 0.018079, acc:, 99.22] [G loss: 8.888227]\n",
            "********* 315 [D loss: 0.006919, acc:, 100.00] [G loss: 10.865485]\n",
            "********* 316 [D loss: 0.024414, acc:, 99.22] [G loss: 10.636990]\n",
            "********* 317 [D loss: 0.006487, acc:, 100.00] [G loss: 9.853662]\n",
            "********* 318 [D loss: 0.012895, acc:, 100.00] [G loss: 8.205372]\n",
            "********* 319 [D loss: 0.013578, acc:, 100.00] [G loss: 9.132437]\n",
            "********* 320 [D loss: 0.005106, acc:, 100.00] [G loss: 10.197159]\n",
            "********* 321 [D loss: 0.006363, acc:, 100.00] [G loss: 10.403900]\n",
            "********* 322 [D loss: 0.004798, acc:, 100.00] [G loss: 10.425439]\n",
            "********* 323 [D loss: 0.052290, acc:, 99.22] [G loss: 7.524697]\n",
            "********* 324 [D loss: 0.028244, acc:, 100.00] [G loss: 7.267754]\n",
            "********* 325 [D loss: 0.024036, acc:, 99.22] [G loss: 9.432603]\n",
            "********* 326 [D loss: 0.008140, acc:, 100.00] [G loss: 9.887727]\n",
            "********* 327 [D loss: 0.038936, acc:, 99.22] [G loss: 7.143793]\n",
            "********* 328 [D loss: 0.015748, acc:, 100.00] [G loss: 6.129668]\n",
            "********* 329 [D loss: 0.036386, acc:, 99.22] [G loss: 7.024542]\n",
            "********* 330 [D loss: 0.009865, acc:, 100.00] [G loss: 9.647898]\n",
            "********* 331 [D loss: 0.032013, acc:, 99.22] [G loss: 7.699115]\n",
            "********* 332 [D loss: 0.015381, acc:, 100.00] [G loss: 7.024440]\n",
            "********* 333 [D loss: 0.014561, acc:, 99.22] [G loss: 7.599629]\n",
            "********* 334 [D loss: 0.015438, acc:, 100.00] [G loss: 8.171614]\n",
            "********* 335 [D loss: 0.051581, acc:, 98.44] [G loss: 7.305125]\n",
            "********* 336 [D loss: 0.006156, acc:, 100.00] [G loss: 8.212440]\n",
            "********* 337 [D loss: 0.006178, acc:, 100.00] [G loss: 9.034027]\n",
            "********* 338 [D loss: 0.028258, acc:, 99.22] [G loss: 6.930648]\n",
            "********* 339 [D loss: 0.020506, acc:, 100.00] [G loss: 8.578615]\n",
            "********* 340 [D loss: 0.003386, acc:, 100.00] [G loss: 11.339128]\n",
            "********* 341 [D loss: 0.014686, acc:, 100.00] [G loss: 10.714149]\n",
            "********* 342 [D loss: 0.026950, acc:, 99.22] [G loss: 10.590322]\n",
            "********* 343 [D loss: 0.038762, acc:, 98.44] [G loss: 7.547689]\n",
            "********* 344 [D loss: 0.011968, acc:, 100.00] [G loss: 7.444017]\n",
            "********* 345 [D loss: 0.007667, acc:, 100.00] [G loss: 7.768736]\n",
            "********* 346 [D loss: 0.009621, acc:, 99.22] [G loss: 7.905918]\n",
            "********* 347 [D loss: 0.017093, acc:, 100.00] [G loss: 7.702068]\n",
            "********* 348 [D loss: 0.012801, acc:, 99.22] [G loss: 9.226257]\n",
            "********* 349 [D loss: 0.024505, acc:, 99.22] [G loss: 9.244311]\n",
            "********* 350 [D loss: 0.003414, acc:, 100.00] [G loss: 8.942684]\n",
            "********* 351 [D loss: 0.009416, acc:, 100.00] [G loss: 9.833942]\n",
            "********* 352 [D loss: 0.027176, acc:, 99.22] [G loss: 6.175952]\n",
            "********* 353 [D loss: 0.032419, acc:, 100.00] [G loss: 8.465498]\n",
            "********* 354 [D loss: 0.004307, acc:, 100.00] [G loss: 12.545860]\n",
            "********* 355 [D loss: 0.029190, acc:, 99.22] [G loss: 11.096842]\n",
            "********* 356 [D loss: 0.056549, acc:, 98.44] [G loss: 7.546724]\n",
            "********* 357 [D loss: 0.193714, acc:, 91.41] [G loss: 15.130874]\n",
            "********* 358 [D loss: 0.038645, acc:, 98.44] [G loss: 32.266304]\n",
            "********* 359 [D loss: 1.090941, acc:, 79.69] [G loss: 4.583387]\n",
            "********* 360 [D loss: 2.871888, acc:, 54.69] [G loss: 7.514354]\n",
            "********* 361 [D loss: 0.119929, acc:, 93.75] [G loss: 32.650627]\n",
            "********* 362 [D loss: 0.701721, acc:, 94.53] [G loss: 42.431778]\n",
            "********* 363 [D loss: 2.416301, acc:, 82.81] [G loss: 25.362648]\n",
            "********* 364 [D loss: 0.477879, acc:, 89.84] [G loss: 10.372555]\n",
            "********* 365 [D loss: 0.378129, acc:, 85.16] [G loss: 6.702406]\n",
            "********* 366 [D loss: 0.038727, acc:, 98.44] [G loss: 6.957888]\n",
            "********* 367 [D loss: 0.166046, acc:, 96.09] [G loss: 8.560291]\n",
            "********* 368 [D loss: 0.021569, acc:, 99.22] [G loss: 12.295181]\n",
            "********* 369 [D loss: 0.076948, acc:, 96.09] [G loss: 12.836618]\n",
            "********* 370 [D loss: 0.026185, acc:, 99.22] [G loss: 11.975714]\n",
            "********* 371 [D loss: 0.062976, acc:, 98.44] [G loss: 11.946594]\n",
            "********* 372 [D loss: 0.019467, acc:, 99.22] [G loss: 10.058846]\n",
            "********* 373 [D loss: 0.019556, acc:, 100.00] [G loss: 8.908010]\n",
            "********* 374 [D loss: 0.017999, acc:, 100.00] [G loss: 8.479115]\n",
            "********* 375 [D loss: 0.023302, acc:, 99.22] [G loss: 7.718817]\n",
            "********* 376 [D loss: 0.014642, acc:, 100.00] [G loss: 7.591159]\n",
            "********* 377 [D loss: 0.006983, acc:, 100.00] [G loss: 9.416354]\n",
            "********* 378 [D loss: 0.022379, acc:, 99.22] [G loss: 9.260859]\n",
            "********* 379 [D loss: 0.035897, acc:, 99.22] [G loss: 8.393523]\n",
            "********* 380 [D loss: 0.013304, acc:, 100.00] [G loss: 7.938159]\n",
            "********* 381 [D loss: 0.009413, acc:, 100.00] [G loss: 8.143154]\n",
            "********* 382 [D loss: 0.013974, acc:, 99.22] [G loss: 7.921350]\n",
            "********* 383 [D loss: 0.005464, acc:, 100.00] [G loss: 7.360993]\n",
            "********* 384 [D loss: 0.018147, acc:, 99.22] [G loss: 6.673632]\n",
            "********* 385 [D loss: 0.018307, acc:, 100.00] [G loss: 7.029109]\n",
            "********* 386 [D loss: 0.025026, acc:, 99.22] [G loss: 7.452429]\n",
            "********* 387 [D loss: 0.017047, acc:, 100.00] [G loss: 7.171496]\n",
            "********* 388 [D loss: 0.006000, acc:, 100.00] [G loss: 7.212129]\n",
            "********* 389 [D loss: 0.026648, acc:, 99.22] [G loss: 7.190205]\n",
            "********* 390 [D loss: 0.027046, acc:, 99.22] [G loss: 6.627385]\n",
            "********* 391 [D loss: 0.019272, acc:, 100.00] [G loss: 6.702051]\n",
            "********* 392 [D loss: 0.020326, acc:, 100.00] [G loss: 5.672602]\n",
            "********* 393 [D loss: 0.019259, acc:, 100.00] [G loss: 6.167408]\n",
            "********* 394 [D loss: 0.018894, acc:, 100.00] [G loss: 6.789182]\n",
            "********* 395 [D loss: 0.046902, acc:, 99.22] [G loss: 6.155603]\n",
            "********* 396 [D loss: 0.050399, acc:, 98.44] [G loss: 5.902558]\n",
            "********* 397 [D loss: 0.023053, acc:, 99.22] [G loss: 7.186647]\n",
            "********* 398 [D loss: 0.027047, acc:, 100.00] [G loss: 7.276896]\n",
            "********* 399 [D loss: 0.058574, acc:, 97.66] [G loss: 6.081217]\n",
            "********* 400 [D loss: 0.059182, acc:, 98.44] [G loss: 5.639450]\n",
            "********* 401 [D loss: 0.014870, acc:, 100.00] [G loss: 6.776457]\n",
            "********* 402 [D loss: 0.032269, acc:, 99.22] [G loss: 7.115248]\n",
            "********* 403 [D loss: 0.022594, acc:, 100.00] [G loss: 6.562091]\n",
            "********* 404 [D loss: 0.033561, acc:, 100.00] [G loss: 5.324313]\n",
            "********* 405 [D loss: 0.048648, acc:, 98.44] [G loss: 4.219117]\n",
            "********* 406 [D loss: 0.025630, acc:, 100.00] [G loss: 6.060412]\n",
            "********* 407 [D loss: 0.046570, acc:, 98.44] [G loss: 6.369398]\n",
            "********* 408 [D loss: 0.022023, acc:, 99.22] [G loss: 6.311239]\n",
            "********* 409 [D loss: 0.026519, acc:, 100.00] [G loss: 6.776365]\n",
            "********* 410 [D loss: 0.070392, acc:, 96.88] [G loss: 5.517583]\n",
            "********* 411 [D loss: 0.017369, acc:, 100.00] [G loss: 6.346880]\n",
            "********* 412 [D loss: 0.010358, acc:, 100.00] [G loss: 6.649570]\n",
            "********* 413 [D loss: 0.076933, acc:, 98.44] [G loss: 6.203167]\n",
            "********* 414 [D loss: 0.031185, acc:, 99.22] [G loss: 5.971718]\n",
            "********* 415 [D loss: 0.028846, acc:, 100.00] [G loss: 6.429296]\n",
            "********* 416 [D loss: 0.025917, acc:, 100.00] [G loss: 6.389022]\n",
            "********* 417 [D loss: 0.028427, acc:, 100.00] [G loss: 6.415944]\n",
            "********* 418 [D loss: 0.036671, acc:, 98.44] [G loss: 6.719881]\n",
            "********* 419 [D loss: 0.031041, acc:, 99.22] [G loss: 5.904681]\n",
            "********* 420 [D loss: 0.015065, acc:, 99.22] [G loss: 5.907844]\n",
            "********* 421 [D loss: 0.032173, acc:, 99.22] [G loss: 5.444512]\n",
            "********* 422 [D loss: 0.010420, acc:, 100.00] [G loss: 6.291911]\n",
            "********* 423 [D loss: 0.022556, acc:, 99.22] [G loss: 6.446758]\n",
            "********* 424 [D loss: 0.015334, acc:, 100.00] [G loss: 6.527001]\n",
            "********* 425 [D loss: 0.036371, acc:, 98.44] [G loss: 5.923973]\n",
            "********* 426 [D loss: 0.021125, acc:, 100.00] [G loss: 6.244990]\n",
            "********* 427 [D loss: 0.039782, acc:, 99.22] [G loss: 6.971573]\n",
            "********* 428 [D loss: 0.046068, acc:, 99.22] [G loss: 7.480435]\n",
            "********* 429 [D loss: 0.017272, acc:, 99.22] [G loss: 7.239768]\n",
            "********* 430 [D loss: 0.049921, acc:, 98.44] [G loss: 6.050298]\n",
            "********* 431 [D loss: 0.054712, acc:, 98.44] [G loss: 5.878376]\n",
            "********* 432 [D loss: 0.021909, acc:, 99.22] [G loss: 7.919299]\n",
            "********* 433 [D loss: 0.029105, acc:, 98.44] [G loss: 9.289457]\n",
            "********* 434 [D loss: 0.020958, acc:, 100.00] [G loss: 8.035204]\n",
            "********* 435 [D loss: 0.025771, acc:, 100.00] [G loss: 5.812195]\n",
            "********* 436 [D loss: 0.064874, acc:, 98.44] [G loss: 5.721700]\n",
            "********* 437 [D loss: 0.015209, acc:, 100.00] [G loss: 9.110220]\n",
            "********* 438 [D loss: 0.064710, acc:, 98.44] [G loss: 7.732529]\n",
            "********* 439 [D loss: 0.047005, acc:, 98.44] [G loss: 5.840092]\n",
            "********* 440 [D loss: 0.060980, acc:, 98.44] [G loss: 7.211455]\n",
            "********* 441 [D loss: 0.035594, acc:, 98.44] [G loss: 10.227501]\n",
            "********* 442 [D loss: 0.035575, acc:, 98.44] [G loss: 7.788313]\n",
            "********* 443 [D loss: 0.024708, acc:, 99.22] [G loss: 6.697203]\n",
            "********* 444 [D loss: 0.012101, acc:, 100.00] [G loss: 6.368326]\n",
            "********* 445 [D loss: 0.023565, acc:, 100.00] [G loss: 6.415433]\n",
            "********* 446 [D loss: 0.012456, acc:, 100.00] [G loss: 7.460890]\n",
            "********* 447 [D loss: 0.028935, acc:, 99.22] [G loss: 7.488920]\n",
            "********* 448 [D loss: 0.009916, acc:, 100.00] [G loss: 6.956139]\n",
            "********* 449 [D loss: 0.014833, acc:, 100.00] [G loss: 6.686995]\n",
            "********* 450 [D loss: 0.016826, acc:, 100.00] [G loss: 6.668519]\n",
            "********* 451 [D loss: 0.007976, acc:, 100.00] [G loss: 7.163105]\n",
            "********* 452 [D loss: 0.007284, acc:, 100.00] [G loss: 7.387993]\n",
            "********* 453 [D loss: 0.014016, acc:, 99.22] [G loss: 8.348701]\n",
            "********* 454 [D loss: 0.037357, acc:, 98.44] [G loss: 6.761029]\n",
            "********* 455 [D loss: 0.013897, acc:, 100.00] [G loss: 6.555621]\n",
            "********* 456 [D loss: 0.023821, acc:, 99.22] [G loss: 6.686626]\n",
            "********* 457 [D loss: 0.029334, acc:, 99.22] [G loss: 6.010455]\n",
            "********* 458 [D loss: 0.024233, acc:, 99.22] [G loss: 6.855161]\n",
            "********* 459 [D loss: 0.030148, acc:, 98.44] [G loss: 7.282056]\n",
            "********* 460 [D loss: 0.037880, acc:, 99.22] [G loss: 6.526546]\n",
            "********* 461 [D loss: 0.034985, acc:, 99.22] [G loss: 5.702165]\n",
            "********* 462 [D loss: 0.013926, acc:, 100.00] [G loss: 6.740181]\n",
            "********* 463 [D loss: 0.009658, acc:, 100.00] [G loss: 7.593868]\n",
            "********* 464 [D loss: 0.011774, acc:, 100.00] [G loss: 7.409604]\n",
            "********* 465 [D loss: 0.012746, acc:, 100.00] [G loss: 7.089602]\n",
            "********* 466 [D loss: 0.010810, acc:, 100.00] [G loss: 6.808046]\n",
            "********* 467 [D loss: 0.008517, acc:, 100.00] [G loss: 7.019304]\n",
            "********* 468 [D loss: 0.024590, acc:, 99.22] [G loss: 7.229468]\n",
            "********* 469 [D loss: 0.027047, acc:, 99.22] [G loss: 8.565094]\n",
            "********* 470 [D loss: 0.027278, acc:, 98.44] [G loss: 7.659432]\n",
            "********* 471 [D loss: 0.064049, acc:, 97.66] [G loss: 4.978504]\n",
            "********* 472 [D loss: 0.060377, acc:, 98.44] [G loss: 5.859339]\n",
            "********* 473 [D loss: 0.017006, acc:, 100.00] [G loss: 8.345549]\n",
            "********* 474 [D loss: 0.028725, acc:, 98.44] [G loss: 7.816851]\n",
            "********* 475 [D loss: 0.032183, acc:, 99.22] [G loss: 6.948879]\n",
            "********* 476 [D loss: 0.043568, acc:, 99.22] [G loss: 8.495214]\n",
            "********* 477 [D loss: 0.006130, acc:, 100.00] [G loss: 11.536196]\n",
            "********* 478 [D loss: 0.013376, acc:, 100.00] [G loss: 12.071486]\n",
            "********* 479 [D loss: 0.077110, acc:, 96.88] [G loss: 8.241430]\n",
            "********* 480 [D loss: 0.069847, acc:, 98.44] [G loss: 6.548074]\n",
            "********* 481 [D loss: 0.038047, acc:, 98.44] [G loss: 7.467688]\n",
            "********* 482 [D loss: 0.004456, acc:, 100.00] [G loss: 10.990250]\n",
            "********* 483 [D loss: 0.044481, acc:, 98.44] [G loss: 8.819487]\n",
            "********* 484 [D loss: 0.055139, acc:, 97.66] [G loss: 8.097080]\n",
            "********* 485 [D loss: 0.032943, acc:, 98.44] [G loss: 8.257816]\n",
            "********* 486 [D loss: 0.003512, acc:, 100.00] [G loss: 9.010957]\n",
            "********* 487 [D loss: 0.003652, acc:, 100.00] [G loss: 10.014565]\n",
            "********* 488 [D loss: 0.015838, acc:, 100.00] [G loss: 9.236088]\n",
            "********* 489 [D loss: 0.071131, acc:, 95.31] [G loss: 8.525370]\n",
            "********* 490 [D loss: 0.035808, acc:, 97.66] [G loss: 8.695969]\n",
            "********* 491 [D loss: 0.015800, acc:, 100.00] [G loss: 9.377298]\n",
            "********* 492 [D loss: 0.142066, acc:, 96.09] [G loss: 8.526235]\n",
            "********* 493 [D loss: 0.016454, acc:, 99.22] [G loss: 8.759832]\n",
            "********* 494 [D loss: 0.006407, acc:, 100.00] [G loss: 10.684851]\n",
            "********* 495 [D loss: 0.057597, acc:, 97.66] [G loss: 7.411890]\n",
            "********* 496 [D loss: 0.088348, acc:, 97.66] [G loss: 8.466991]\n",
            "********* 497 [D loss: 0.005388, acc:, 100.00] [G loss: 10.160437]\n",
            "********* 498 [D loss: 0.006551, acc:, 100.00] [G loss: 12.735891]\n",
            "********* 499 [D loss: 0.014701, acc:, 99.22] [G loss: 11.760153]\n",
            "********* 500 [D loss: 0.023585, acc:, 99.22] [G loss: 8.645367]\n",
            "********* 501 [D loss: 0.045743, acc:, 98.44] [G loss: 9.836477]\n",
            "********* 502 [D loss: 0.001772, acc:, 100.00] [G loss: 12.199905]\n",
            "********* 503 [D loss: 0.043677, acc:, 98.44] [G loss: 10.085640]\n",
            "********* 504 [D loss: 0.020736, acc:, 99.22] [G loss: 6.940601]\n",
            "********* 505 [D loss: 0.027302, acc:, 100.00] [G loss: 6.766666]\n",
            "********* 506 [D loss: 0.018367, acc:, 100.00] [G loss: 8.291332]\n",
            "********* 507 [D loss: 0.004252, acc:, 100.00] [G loss: 11.426125]\n",
            "********* 508 [D loss: 0.039832, acc:, 98.44] [G loss: 10.881403]\n",
            "********* 509 [D loss: 0.023917, acc:, 99.22] [G loss: 7.955215]\n",
            "********* 510 [D loss: 0.015705, acc:, 100.00] [G loss: 7.055373]\n",
            "********* 511 [D loss: 0.033698, acc:, 99.22] [G loss: 8.449526]\n",
            "********* 512 [D loss: 0.017528, acc:, 99.22] [G loss: 11.753973]\n",
            "********* 513 [D loss: 0.052852, acc:, 98.44] [G loss: 10.084799]\n",
            "********* 514 [D loss: 0.008031, acc:, 100.00] [G loss: 8.568813]\n",
            "********* 515 [D loss: 0.069241, acc:, 98.44] [G loss: 9.319326]\n",
            "********* 516 [D loss: 0.010015, acc:, 100.00] [G loss: 11.733419]\n",
            "********* 517 [D loss: 0.056163, acc:, 99.22] [G loss: 9.754047]\n",
            "********* 518 [D loss: 0.021656, acc:, 100.00] [G loss: 7.140004]\n",
            "********* 519 [D loss: 0.025521, acc:, 100.00] [G loss: 7.065971]\n",
            "********* 520 [D loss: 0.008250, acc:, 100.00] [G loss: 8.388118]\n",
            "********* 521 [D loss: 0.029523, acc:, 99.22] [G loss: 9.527689]\n",
            "********* 522 [D loss: 0.017707, acc:, 99.22] [G loss: 10.706490]\n",
            "********* 523 [D loss: 0.041926, acc:, 97.66] [G loss: 11.082273]\n",
            "********* 524 [D loss: 0.015232, acc:, 99.22] [G loss: 11.873332]\n",
            "********* 525 [D loss: 0.013676, acc:, 100.00] [G loss: 10.906256]\n",
            "********* 526 [D loss: 0.044318, acc:, 99.22] [G loss: 10.972361]\n",
            "********* 527 [D loss: 0.120195, acc:, 94.53] [G loss: 7.835614]\n",
            "********* 528 [D loss: 0.002981, acc:, 100.00] [G loss: 7.903567]\n",
            "********* 529 [D loss: 0.019313, acc:, 99.22] [G loss: 7.677027]\n",
            "********* 530 [D loss: 0.090653, acc:, 96.09] [G loss: 10.048820]\n",
            "********* 531 [D loss: 0.251457, acc:, 93.75] [G loss: 4.685557]\n",
            "********* 532 [D loss: 0.510229, acc:, 85.94] [G loss: 11.026968]\n",
            "********* 533 [D loss: 0.027338, acc:, 99.22] [G loss: 24.795101]\n",
            "********* 534 [D loss: 0.384969, acc:, 90.62] [G loss: 20.067783]\n",
            "********* 535 [D loss: 0.130605, acc:, 95.31] [G loss: 8.397696]\n",
            "********* 536 [D loss: 0.107255, acc:, 95.31] [G loss: 4.754023]\n",
            "********* 537 [D loss: 0.039118, acc:, 98.44] [G loss: 7.870396]\n",
            "********* 538 [D loss: 0.004306, acc:, 100.00] [G loss: 11.680633]\n",
            "********* 539 [D loss: 0.175545, acc:, 96.88] [G loss: 10.381727]\n",
            "********* 540 [D loss: 0.089704, acc:, 96.88] [G loss: 7.030285]\n",
            "********* 541 [D loss: 0.010997, acc:, 100.00] [G loss: 6.103543]\n",
            "********* 542 [D loss: 0.020645, acc:, 99.22] [G loss: 7.516641]\n",
            "********* 543 [D loss: 0.002951, acc:, 100.00] [G loss: 10.331673]\n",
            "********* 544 [D loss: 0.001222, acc:, 100.00] [G loss: 12.247285]\n",
            "********* 545 [D loss: 0.003932, acc:, 100.00] [G loss: 12.859344]\n",
            "********* 546 [D loss: 0.018523, acc:, 99.22] [G loss: 13.346314]\n",
            "********* 547 [D loss: 0.029788, acc:, 99.22] [G loss: 12.020689]\n",
            "********* 548 [D loss: 0.004295, acc:, 100.00] [G loss: 9.931866]\n",
            "********* 549 [D loss: 0.020412, acc:, 99.22] [G loss: 8.759199]\n",
            "********* 550 [D loss: 0.013463, acc:, 99.22] [G loss: 9.260480]\n",
            "********* 551 [D loss: 0.015801, acc:, 99.22] [G loss: 9.661518]\n",
            "********* 552 [D loss: 0.011654, acc:, 99.22] [G loss: 10.081614]\n",
            "********* 553 [D loss: 0.014626, acc:, 99.22] [G loss: 8.623102]\n",
            "********* 554 [D loss: 0.003194, acc:, 100.00] [G loss: 7.701558]\n",
            "********* 555 [D loss: 0.012504, acc:, 100.00] [G loss: 7.171463]\n",
            "********* 556 [D loss: 0.014388, acc:, 99.22] [G loss: 8.170346]\n",
            "********* 557 [D loss: 0.025897, acc:, 98.44] [G loss: 6.482262]\n",
            "********* 558 [D loss: 0.009513, acc:, 100.00] [G loss: 7.284489]\n",
            "********* 559 [D loss: 0.014812, acc:, 100.00] [G loss: 7.490977]\n",
            "********* 560 [D loss: 0.025359, acc:, 98.44] [G loss: 8.316672]\n",
            "********* 561 [D loss: 0.015289, acc:, 100.00] [G loss: 8.437349]\n",
            "********* 562 [D loss: 0.009885, acc:, 100.00] [G loss: 8.154449]\n",
            "********* 563 [D loss: 0.021921, acc:, 100.00] [G loss: 6.588125]\n",
            "********* 564 [D loss: 0.030298, acc:, 100.00] [G loss: 6.194954]\n",
            "********* 565 [D loss: 0.008095, acc:, 100.00] [G loss: 8.389527]\n",
            "********* 566 [D loss: 0.050220, acc:, 97.66] [G loss: 6.355081]\n",
            "********* 567 [D loss: 0.087473, acc:, 96.88] [G loss: 7.214505]\n",
            "********* 568 [D loss: 0.004191, acc:, 100.00] [G loss: 12.159658]\n",
            "********* 569 [D loss: 0.066303, acc:, 96.88] [G loss: 8.732821]\n",
            "********* 570 [D loss: 0.030959, acc:, 98.44] [G loss: 6.492919]\n",
            "********* 571 [D loss: 0.075926, acc:, 96.88] [G loss: 9.940252]\n",
            "********* 572 [D loss: 0.007695, acc:, 100.00] [G loss: 16.583384]\n",
            "********* 573 [D loss: 0.161264, acc:, 94.53] [G loss: 10.306576]\n",
            "********* 574 [D loss: 0.026664, acc:, 99.22] [G loss: 6.683158]\n",
            "********* 575 [D loss: 0.072773, acc:, 96.88] [G loss: 8.828807]\n",
            "********* 576 [D loss: 0.035048, acc:, 98.44] [G loss: 10.497690]\n",
            "********* 577 [D loss: 0.038317, acc:, 98.44] [G loss: 12.613559]\n",
            "********* 578 [D loss: 0.048255, acc:, 99.22] [G loss: 11.056513]\n",
            "********* 579 [D loss: 0.048569, acc:, 99.22] [G loss: 9.267494]\n",
            "********* 580 [D loss: 0.009470, acc:, 100.00] [G loss: 7.736010]\n",
            "********* 581 [D loss: 0.033735, acc:, 98.44] [G loss: 7.790892]\n",
            "********* 582 [D loss: 0.011266, acc:, 100.00] [G loss: 10.504109]\n",
            "********* 583 [D loss: 0.006867, acc:, 100.00] [G loss: 11.554268]\n",
            "********* 584 [D loss: 0.058005, acc:, 97.66] [G loss: 8.870132]\n",
            "********* 585 [D loss: 0.017410, acc:, 100.00] [G loss: 6.683945]\n",
            "********* 586 [D loss: 0.058348, acc:, 96.88] [G loss: 8.792044]\n",
            "********* 587 [D loss: 0.006865, acc:, 100.00] [G loss: 13.745058]\n",
            "********* 588 [D loss: 0.042681, acc:, 98.44] [G loss: 12.807475]\n",
            "********* 589 [D loss: 0.070670, acc:, 96.88] [G loss: 9.305241]\n",
            "********* 590 [D loss: 0.036922, acc:, 99.22] [G loss: 7.041386]\n",
            "********* 591 [D loss: 0.035431, acc:, 99.22] [G loss: 9.235540]\n",
            "********* 592 [D loss: 0.002218, acc:, 100.00] [G loss: 13.830510]\n",
            "********* 593 [D loss: 0.034056, acc:, 98.44] [G loss: 12.697924]\n",
            "********* 594 [D loss: 0.021304, acc:, 99.22] [G loss: 8.840265]\n",
            "********* 595 [D loss: 0.024896, acc:, 99.22] [G loss: 9.137775]\n",
            "********* 596 [D loss: 0.045966, acc:, 96.88] [G loss: 7.933054]\n",
            "********* 597 [D loss: 0.007135, acc:, 100.00] [G loss: 10.608553]\n",
            "********* 598 [D loss: 0.007306, acc:, 100.00] [G loss: 12.196575]\n",
            "********* 599 [D loss: 0.026931, acc:, 99.22] [G loss: 10.872725]\n",
            "********* 600 [D loss: 0.011107, acc:, 100.00] [G loss: 9.281389]\n",
            "********* 601 [D loss: 0.039168, acc:, 98.44] [G loss: 8.906686]\n",
            "********* 602 [D loss: 0.018782, acc:, 99.22] [G loss: 9.091801]\n",
            "********* 603 [D loss: 0.032516, acc:, 98.44] [G loss: 9.020458]\n",
            "********* 604 [D loss: 0.066774, acc:, 97.66] [G loss: 8.895909]\n",
            "********* 605 [D loss: 0.005315, acc:, 100.00] [G loss: 11.587412]\n",
            "********* 606 [D loss: 0.037520, acc:, 98.44] [G loss: 9.503618]\n",
            "********* 607 [D loss: 0.054056, acc:, 97.66] [G loss: 6.397888]\n",
            "********* 608 [D loss: 0.098434, acc:, 96.88] [G loss: 9.318983]\n",
            "********* 609 [D loss: 0.002767, acc:, 100.00] [G loss: 16.101624]\n",
            "********* 610 [D loss: 0.181624, acc:, 94.53] [G loss: 9.630938]\n",
            "********* 611 [D loss: 0.120221, acc:, 94.53] [G loss: 8.467408]\n",
            "********* 612 [D loss: 0.018414, acc:, 99.22] [G loss: 9.431766]\n",
            "********* 613 [D loss: 0.041369, acc:, 98.44] [G loss: 9.871819]\n",
            "********* 614 [D loss: 0.086494, acc:, 97.66] [G loss: 9.608577]\n",
            "********* 615 [D loss: 0.018182, acc:, 99.22] [G loss: 10.197422]\n",
            "********* 616 [D loss: 0.011801, acc:, 99.22] [G loss: 10.542961]\n",
            "********* 617 [D loss: 0.006204, acc:, 100.00] [G loss: 12.401995]\n",
            "********* 618 [D loss: 0.061264, acc:, 97.66] [G loss: 11.051202]\n",
            "********* 619 [D loss: 0.039989, acc:, 98.44] [G loss: 8.913794]\n",
            "********* 620 [D loss: 0.038992, acc:, 99.22] [G loss: 8.676668]\n",
            "********* 621 [D loss: 0.024766, acc:, 98.44] [G loss: 7.494111]\n",
            "********* 622 [D loss: 0.009398, acc:, 100.00] [G loss: 8.587828]\n",
            "********* 623 [D loss: 0.006652, acc:, 100.00] [G loss: 11.121475]\n",
            "********* 624 [D loss: 0.008336, acc:, 100.00] [G loss: 11.626381]\n",
            "********* 625 [D loss: 0.116830, acc:, 96.88] [G loss: 8.070208]\n",
            "********* 626 [D loss: 0.011510, acc:, 100.00] [G loss: 7.534316]\n",
            "********* 627 [D loss: 0.037381, acc:, 98.44] [G loss: 8.561392]\n",
            "********* 628 [D loss: 0.047429, acc:, 98.44] [G loss: 9.084908]\n",
            "********* 629 [D loss: 0.031482, acc:, 98.44] [G loss: 8.504097]\n",
            "********* 630 [D loss: 0.016637, acc:, 99.22] [G loss: 9.548912]\n",
            "********* 631 [D loss: 0.006797, acc:, 100.00] [G loss: 9.585024]\n",
            "********* 632 [D loss: 0.018114, acc:, 99.22] [G loss: 9.290806]\n",
            "********* 633 [D loss: 0.010371, acc:, 100.00] [G loss: 9.795675]\n",
            "********* 634 [D loss: 0.018760, acc:, 99.22] [G loss: 8.024610]\n",
            "********* 635 [D loss: 0.011133, acc:, 100.00] [G loss: 7.801518]\n",
            "********* 636 [D loss: 0.006835, acc:, 100.00] [G loss: 8.229261]\n",
            "********* 637 [D loss: 0.005345, acc:, 100.00] [G loss: 9.918902]\n",
            "********* 638 [D loss: 0.011486, acc:, 100.00] [G loss: 8.845242]\n",
            "********* 639 [D loss: 0.016661, acc:, 99.22] [G loss: 8.456115]\n",
            "********* 640 [D loss: 0.059943, acc:, 98.44] [G loss: 6.252862]\n",
            "********* 641 [D loss: 0.015715, acc:, 100.00] [G loss: 7.433051]\n",
            "********* 642 [D loss: 0.080905, acc:, 97.66] [G loss: 9.066183]\n",
            "********* 643 [D loss: 0.031990, acc:, 98.44] [G loss: 8.246283]\n",
            "********* 644 [D loss: 0.007567, acc:, 100.00] [G loss: 6.826977]\n",
            "********* 645 [D loss: 0.011183, acc:, 100.00] [G loss: 7.031147]\n",
            "********* 646 [D loss: 0.009052, acc:, 100.00] [G loss: 8.579609]\n",
            "********* 647 [D loss: 0.047015, acc:, 98.44] [G loss: 5.400162]\n",
            "********* 648 [D loss: 0.039007, acc:, 99.22] [G loss: 5.323825]\n",
            "********* 649 [D loss: 0.011272, acc:, 100.00] [G loss: 7.874036]\n",
            "********* 650 [D loss: 0.008052, acc:, 100.00] [G loss: 9.904784]\n",
            "********* 651 [D loss: 0.060805, acc:, 98.44] [G loss: 5.047115]\n",
            "********* 652 [D loss: 0.055848, acc:, 100.00] [G loss: 7.966844]\n",
            "********* 653 [D loss: 0.008158, acc:, 100.00] [G loss: 16.033390]\n",
            "********* 654 [D loss: 0.055029, acc:, 98.44] [G loss: 15.190920]\n",
            "********* 655 [D loss: 0.109180, acc:, 96.88] [G loss: 4.255303]\n",
            "********* 656 [D loss: 0.674696, acc:, 75.00] [G loss: 18.223003]\n",
            "********* 657 [D loss: 0.321737, acc:, 92.97] [G loss: 40.986359]\n",
            "********* 658 [D loss: 1.399976, acc:, 78.12] [G loss: 14.650684]\n",
            "********* 659 [D loss: 0.585401, acc:, 85.94] [G loss: 5.558208]\n",
            "********* 660 [D loss: 1.203401, acc:, 83.59] [G loss: 6.765556]\n",
            "********* 661 [D loss: 0.191007, acc:, 95.31] [G loss: 13.267113]\n",
            "********* 662 [D loss: 0.082424, acc:, 97.66] [G loss: 20.035381]\n",
            "********* 663 [D loss: 0.228758, acc:, 93.75] [G loss: 18.497368]\n",
            "********* 664 [D loss: 0.187022, acc:, 94.53] [G loss: 17.703575]\n",
            "********* 665 [D loss: 0.051238, acc:, 97.66] [G loss: 14.171559]\n",
            "********* 666 [D loss: 0.070032, acc:, 97.66] [G loss: 10.925889]\n",
            "********* 667 [D loss: 0.021113, acc:, 98.44] [G loss: 10.424961]\n",
            "********* 668 [D loss: 0.020300, acc:, 99.22] [G loss: 11.548566]\n",
            "********* 669 [D loss: 0.027012, acc:, 98.44] [G loss: 10.862222]\n",
            "********* 670 [D loss: 0.007935, acc:, 100.00] [G loss: 12.651388]\n",
            "********* 671 [D loss: 0.027674, acc:, 97.66] [G loss: 14.251688]\n",
            "********* 672 [D loss: 0.060754, acc:, 98.44] [G loss: 15.135529]\n",
            "********* 673 [D loss: 0.064389, acc:, 97.66] [G loss: 11.919749]\n",
            "********* 674 [D loss: 0.014273, acc:, 99.22] [G loss: 10.869699]\n",
            "********* 675 [D loss: 0.005057, acc:, 100.00] [G loss: 9.337957]\n",
            "********* 676 [D loss: 0.019700, acc:, 100.00] [G loss: 8.104533]\n",
            "********* 677 [D loss: 0.007440, acc:, 100.00] [G loss: 9.305218]\n",
            "********* 678 [D loss: 0.003546, acc:, 100.00] [G loss: 10.500099]\n",
            "********* 679 [D loss: 0.004356, acc:, 100.00] [G loss: 11.226603]\n",
            "********* 680 [D loss: 0.013196, acc:, 99.22] [G loss: 10.449991]\n",
            "********* 681 [D loss: 0.006537, acc:, 100.00] [G loss: 11.010874]\n",
            "********* 682 [D loss: 0.004835, acc:, 100.00] [G loss: 12.875867]\n",
            "********* 683 [D loss: 0.013346, acc:, 99.22] [G loss: 9.822889]\n",
            "********* 684 [D loss: 0.025686, acc:, 99.22] [G loss: 8.736458]\n",
            "********* 685 [D loss: 0.015247, acc:, 99.22] [G loss: 8.638956]\n",
            "********* 686 [D loss: 0.011928, acc:, 100.00] [G loss: 8.661686]\n",
            "********* 687 [D loss: 0.013170, acc:, 100.00] [G loss: 8.208569]\n",
            "********* 688 [D loss: 0.025964, acc:, 99.22] [G loss: 7.885349]\n",
            "********* 689 [D loss: 0.024860, acc:, 99.22] [G loss: 7.718837]\n",
            "********* 690 [D loss: 0.016951, acc:, 100.00] [G loss: 7.827506]\n",
            "********* 691 [D loss: 0.007320, acc:, 100.00] [G loss: 7.455410]\n",
            "********* 692 [D loss: 0.018768, acc:, 100.00] [G loss: 6.706106]\n",
            "********* 693 [D loss: 0.032406, acc:, 99.22] [G loss: 6.320300]\n",
            "********* 694 [D loss: 0.025243, acc:, 100.00] [G loss: 6.778003]\n",
            "********* 695 [D loss: 0.015987, acc:, 100.00] [G loss: 7.136122]\n",
            "********* 696 [D loss: 0.017188, acc:, 100.00] [G loss: 7.550550]\n",
            "********* 697 [D loss: 0.025285, acc:, 99.22] [G loss: 7.105259]\n",
            "********* 698 [D loss: 0.048007, acc:, 99.22] [G loss: 6.091416]\n",
            "********* 699 [D loss: 0.037353, acc:, 98.44] [G loss: 5.908510]\n",
            "********* 700 [D loss: 0.021909, acc:, 99.22] [G loss: 5.642858]\n",
            "********* 701 [D loss: 0.074836, acc:, 97.66] [G loss: 5.781133]\n",
            "********* 702 [D loss: 0.035227, acc:, 100.00] [G loss: 7.660208]\n",
            "********* 703 [D loss: 0.022612, acc:, 99.22] [G loss: 8.181206]\n",
            "********* 704 [D loss: 0.024542, acc:, 99.22] [G loss: 7.434275]\n",
            "********* 705 [D loss: 0.020042, acc:, 100.00] [G loss: 6.475619]\n",
            "********* 706 [D loss: 0.035984, acc:, 98.44] [G loss: 5.621185]\n",
            "********* 707 [D loss: 0.025822, acc:, 99.22] [G loss: 6.357721]\n",
            "********* 708 [D loss: 0.028387, acc:, 99.22] [G loss: 7.248201]\n",
            "********* 709 [D loss: 0.012438, acc:, 100.00] [G loss: 7.785152]\n",
            "********* 710 [D loss: 0.028083, acc:, 99.22] [G loss: 6.639555]\n",
            "********* 711 [D loss: 0.056498, acc:, 98.44] [G loss: 4.496705]\n",
            "********* 712 [D loss: 0.077431, acc:, 99.22] [G loss: 5.687362]\n",
            "********* 713 [D loss: 0.007530, acc:, 100.00] [G loss: 9.466785]\n",
            "********* 714 [D loss: 0.173134, acc:, 95.31] [G loss: 8.104430]\n",
            "********* 715 [D loss: 0.048659, acc:, 98.44] [G loss: 5.971876]\n",
            "********* 716 [D loss: 0.025682, acc:, 100.00] [G loss: 5.886046]\n",
            "********* 717 [D loss: 0.016219, acc:, 100.00] [G loss: 6.938821]\n",
            "********* 718 [D loss: 0.023376, acc:, 100.00] [G loss: 6.947086]\n",
            "********* 719 [D loss: 0.010646, acc:, 100.00] [G loss: 7.058895]\n",
            "********* 720 [D loss: 0.029950, acc:, 99.22] [G loss: 5.962054]\n",
            "********* 721 [D loss: 0.029592, acc:, 100.00] [G loss: 6.009135]\n",
            "********* 722 [D loss: 0.008416, acc:, 100.00] [G loss: 6.841806]\n",
            "********* 723 [D loss: 0.009288, acc:, 100.00] [G loss: 7.787642]\n",
            "********* 724 [D loss: 0.013661, acc:, 100.00] [G loss: 7.904667]\n",
            "********* 725 [D loss: 0.015602, acc:, 100.00] [G loss: 7.969581]\n",
            "********* 726 [D loss: 0.024709, acc:, 99.22] [G loss: 6.670758]\n",
            "********* 727 [D loss: 0.021190, acc:, 100.00] [G loss: 6.067966]\n",
            "********* 728 [D loss: 0.028113, acc:, 99.22] [G loss: 6.780849]\n",
            "********* 729 [D loss: 0.009313, acc:, 100.00] [G loss: 8.583644]\n",
            "********* 730 [D loss: 0.013508, acc:, 99.22] [G loss: 8.731633]\n",
            "********* 731 [D loss: 0.040634, acc:, 99.22] [G loss: 5.832279]\n",
            "********* 732 [D loss: 0.020388, acc:, 100.00] [G loss: 5.465281]\n",
            "********* 733 [D loss: 0.045205, acc:, 98.44] [G loss: 6.295991]\n",
            "********* 734 [D loss: 0.006712, acc:, 100.00] [G loss: 9.452545]\n",
            "********* 735 [D loss: 0.080323, acc:, 96.09] [G loss: 5.382162]\n",
            "********* 736 [D loss: 0.112178, acc:, 95.31] [G loss: 7.445060]\n",
            "********* 737 [D loss: 0.003858, acc:, 100.00] [G loss: 14.692490]\n",
            "********* 738 [D loss: 0.352787, acc:, 92.19] [G loss: 4.472084]\n",
            "********* 739 [D loss: 0.544621, acc:, 78.12] [G loss: 7.433443]\n",
            "********* 740 [D loss: 0.005924, acc:, 100.00] [G loss: 23.791681]\n",
            "********* 741 [D loss: 0.622760, acc:, 89.06] [G loss: 17.396267]\n",
            "********* 742 [D loss: 0.072481, acc:, 96.88] [G loss: 9.233905]\n",
            "********* 743 [D loss: 0.066916, acc:, 96.09] [G loss: 5.378191]\n",
            "********* 744 [D loss: 0.173563, acc:, 92.19] [G loss: 6.536057]\n",
            "********* 745 [D loss: 0.019942, acc:, 99.22] [G loss: 9.549222]\n",
            "********* 746 [D loss: 0.032675, acc:, 98.44] [G loss: 10.661776]\n",
            "********* 747 [D loss: 0.066096, acc:, 96.88] [G loss: 10.568577]\n",
            "********* 748 [D loss: 0.082609, acc:, 96.09] [G loss: 8.977195]\n",
            "********* 749 [D loss: 0.041462, acc:, 99.22] [G loss: 8.568283]\n",
            "********* 750 [D loss: 0.005157, acc:, 100.00] [G loss: 8.120394]\n",
            "********* 751 [D loss: 0.002641, acc:, 100.00] [G loss: 9.381911]\n",
            "********* 752 [D loss: 0.004356, acc:, 100.00] [G loss: 9.649527]\n",
            "********* 753 [D loss: 0.008837, acc:, 100.00] [G loss: 8.458651]\n",
            "********* 754 [D loss: 0.006758, acc:, 100.00] [G loss: 8.961744]\n",
            "********* 755 [D loss: 0.007768, acc:, 100.00] [G loss: 9.182949]\n",
            "********* 756 [D loss: 0.005448, acc:, 100.00] [G loss: 8.397293]\n",
            "********* 757 [D loss: 0.025929, acc:, 99.22] [G loss: 8.303876]\n",
            "********* 758 [D loss: 0.010376, acc:, 100.00] [G loss: 7.889794]\n",
            "********* 759 [D loss: 0.010208, acc:, 100.00] [G loss: 7.410661]\n",
            "********* 760 [D loss: 0.013173, acc:, 100.00] [G loss: 6.389266]\n",
            "********* 761 [D loss: 0.013881, acc:, 100.00] [G loss: 6.302926]\n",
            "********* 762 [D loss: 0.020473, acc:, 100.00] [G loss: 6.259875]\n",
            "********* 763 [D loss: 0.011100, acc:, 100.00] [G loss: 6.824138]\n",
            "********* 764 [D loss: 0.026680, acc:, 99.22] [G loss: 7.598421]\n",
            "********* 765 [D loss: 0.018808, acc:, 100.00] [G loss: 8.603968]\n",
            "********* 766 [D loss: 0.006792, acc:, 100.00] [G loss: 8.446512]\n",
            "********* 767 [D loss: 0.053871, acc:, 98.44] [G loss: 6.608479]\n",
            "********* 768 [D loss: 0.015941, acc:, 100.00] [G loss: 4.866026]\n",
            "********* 769 [D loss: 0.039151, acc:, 98.44] [G loss: 6.007164]\n",
            "********* 770 [D loss: 0.030408, acc:, 98.44] [G loss: 8.234975]\n",
            "********* 771 [D loss: 0.060765, acc:, 98.44] [G loss: 7.529226]\n",
            "********* 772 [D loss: 0.017643, acc:, 100.00] [G loss: 7.030351]\n",
            "********* 773 [D loss: 0.051034, acc:, 98.44] [G loss: 6.281469]\n",
            "********* 774 [D loss: 0.011443, acc:, 100.00] [G loss: 7.421028]\n",
            "********* 775 [D loss: 0.011469, acc:, 100.00] [G loss: 7.533237]\n",
            "********* 776 [D loss: 0.044803, acc:, 97.66] [G loss: 5.911407]\n",
            "********* 777 [D loss: 0.055218, acc:, 99.22] [G loss: 5.837273]\n",
            "********* 778 [D loss: 0.030339, acc:, 99.22] [G loss: 7.503267]\n",
            "********* 779 [D loss: 0.011570, acc:, 100.00] [G loss: 8.907110]\n",
            "********* 780 [D loss: 0.012475, acc:, 100.00] [G loss: 9.515641]\n",
            "********* 781 [D loss: 0.019370, acc:, 98.44] [G loss: 8.551128]\n",
            "********* 782 [D loss: 0.028447, acc:, 99.22] [G loss: 6.905697]\n",
            "********* 783 [D loss: 0.024001, acc:, 100.00] [G loss: 5.636930]\n",
            "********* 784 [D loss: 0.054891, acc:, 98.44] [G loss: 6.537139]\n",
            "********* 785 [D loss: 0.058412, acc:, 98.44] [G loss: 6.416965]\n",
            "********* 786 [D loss: 0.014667, acc:, 100.00] [G loss: 7.159669]\n",
            "********* 787 [D loss: 0.012577, acc:, 100.00] [G loss: 8.185205]\n",
            "********* 788 [D loss: 0.037633, acc:, 98.44] [G loss: 7.372966]\n",
            "********* 789 [D loss: 0.021041, acc:, 100.00] [G loss: 7.549123]\n",
            "********* 790 [D loss: 0.021379, acc:, 99.22] [G loss: 7.000130]\n",
            "********* 791 [D loss: 0.020524, acc:, 100.00] [G loss: 7.379987]\n",
            "********* 792 [D loss: 0.024239, acc:, 99.22] [G loss: 7.307544]\n",
            "********* 793 [D loss: 0.015927, acc:, 100.00] [G loss: 7.375784]\n",
            "********* 794 [D loss: 0.010901, acc:, 100.00] [G loss: 7.760242]\n",
            "********* 795 [D loss: 0.026689, acc:, 100.00] [G loss: 7.297201]\n",
            "********* 796 [D loss: 0.018753, acc:, 100.00] [G loss: 6.690725]\n",
            "********* 797 [D loss: 0.020281, acc:, 99.22] [G loss: 6.170780]\n",
            "********* 798 [D loss: 0.027108, acc:, 99.22] [G loss: 7.345458]\n",
            "********* 799 [D loss: 0.007197, acc:, 100.00] [G loss: 9.603894]\n",
            "********* 800 [D loss: 0.072109, acc:, 97.66] [G loss: 6.336317]\n",
            "********* 801 [D loss: 0.039778, acc:, 100.00] [G loss: 5.501092]\n",
            "********* 802 [D loss: 0.039189, acc:, 99.22] [G loss: 7.100881]\n",
            "********* 803 [D loss: 0.020523, acc:, 100.00] [G loss: 8.562593]\n",
            "********* 804 [D loss: 0.018171, acc:, 99.22] [G loss: 8.927708]\n",
            "********* 805 [D loss: 0.051201, acc:, 97.66] [G loss: 5.915668]\n",
            "********* 806 [D loss: 0.083027, acc:, 96.88] [G loss: 7.099101]\n",
            "********* 807 [D loss: 0.009519, acc:, 100.00] [G loss: 12.278944]\n",
            "********* 808 [D loss: 0.085307, acc:, 97.66] [G loss: 11.289566]\n",
            "********* 809 [D loss: 0.065397, acc:, 96.88] [G loss: 8.509719]\n",
            "********* 810 [D loss: 0.026964, acc:, 98.44] [G loss: 6.161794]\n",
            "********* 811 [D loss: 0.014202, acc:, 100.00] [G loss: 5.929507]\n",
            "********* 812 [D loss: 0.011569, acc:, 100.00] [G loss: 7.215064]\n",
            "********* 813 [D loss: 0.013295, acc:, 100.00] [G loss: 8.516779]\n",
            "********* 814 [D loss: 0.004847, acc:, 100.00] [G loss: 10.282915]\n",
            "********* 815 [D loss: 0.007445, acc:, 100.00] [G loss: 10.718440]\n",
            "********* 816 [D loss: 0.033714, acc:, 99.22] [G loss: 8.238149]\n",
            "********* 817 [D loss: 0.007574, acc:, 100.00] [G loss: 6.603803]\n",
            "********* 818 [D loss: 0.028337, acc:, 100.00] [G loss: 6.278534]\n",
            "********* 819 [D loss: 0.007917, acc:, 100.00] [G loss: 8.489859]\n",
            "********* 820 [D loss: 0.011377, acc:, 99.22] [G loss: 9.574232]\n",
            "********* 821 [D loss: 0.004750, acc:, 100.00] [G loss: 10.694938]\n",
            "********* 822 [D loss: 0.017332, acc:, 100.00] [G loss: 8.923639]\n",
            "********* 823 [D loss: 0.005797, acc:, 100.00] [G loss: 7.998108]\n",
            "********* 824 [D loss: 0.016354, acc:, 99.22] [G loss: 5.669927]\n",
            "********* 825 [D loss: 0.017376, acc:, 100.00] [G loss: 7.078649]\n",
            "********* 826 [D loss: 0.003235, acc:, 100.00] [G loss: 8.504554]\n",
            "********* 827 [D loss: 0.020369, acc:, 99.22] [G loss: 8.527363]\n",
            "********* 828 [D loss: 0.014744, acc:, 99.22] [G loss: 7.341644]\n",
            "********* 829 [D loss: 0.045540, acc:, 98.44] [G loss: 6.719242]\n",
            "********* 830 [D loss: 0.004441, acc:, 100.00] [G loss: 7.926409]\n",
            "********* 831 [D loss: 0.018797, acc:, 99.22] [G loss: 7.545263]\n",
            "********* 832 [D loss: 0.009999, acc:, 100.00] [G loss: 7.475913]\n",
            "********* 833 [D loss: 0.013958, acc:, 100.00] [G loss: 6.982072]\n",
            "********* 834 [D loss: 0.008521, acc:, 100.00] [G loss: 7.270426]\n",
            "********* 835 [D loss: 0.018130, acc:, 99.22] [G loss: 7.378129]\n",
            "********* 836 [D loss: 0.017645, acc:, 99.22] [G loss: 6.885736]\n",
            "********* 837 [D loss: 0.045524, acc:, 97.66] [G loss: 6.181246]\n",
            "********* 838 [D loss: 0.013878, acc:, 100.00] [G loss: 7.919752]\n",
            "********* 839 [D loss: 0.006636, acc:, 100.00] [G loss: 9.832527]\n",
            "********* 840 [D loss: 0.022366, acc:, 99.22] [G loss: 9.214312]\n",
            "********* 841 [D loss: 0.035699, acc:, 98.44] [G loss: 7.393678]\n",
            "********* 842 [D loss: 0.017839, acc:, 100.00] [G loss: 8.548688]\n",
            "********* 843 [D loss: 0.007429, acc:, 100.00] [G loss: 9.468132]\n",
            "********* 844 [D loss: 0.028520, acc:, 98.44] [G loss: 8.117691]\n",
            "********* 845 [D loss: 0.058665, acc:, 97.66] [G loss: 8.236643]\n",
            "********* 846 [D loss: 0.006219, acc:, 100.00] [G loss: 9.481909]\n",
            "********* 847 [D loss: 0.026720, acc:, 99.22] [G loss: 9.389032]\n",
            "********* 848 [D loss: 0.022400, acc:, 99.22] [G loss: 9.432563]\n",
            "********* 849 [D loss: 0.099503, acc:, 96.09] [G loss: 5.970654]\n",
            "********* 850 [D loss: 0.118112, acc:, 96.09] [G loss: 7.053813]\n",
            "********* 851 [D loss: 0.041890, acc:, 98.44] [G loss: 12.200891]\n",
            "********* 852 [D loss: 0.032714, acc:, 98.44] [G loss: 13.852924]\n",
            "********* 853 [D loss: 0.018856, acc:, 98.44] [G loss: 11.156849]\n",
            "********* 854 [D loss: 0.020898, acc:, 99.22] [G loss: 8.316581]\n",
            "********* 855 [D loss: 0.092800, acc:, 96.88] [G loss: 13.473379]\n",
            "********* 856 [D loss: 0.070283, acc:, 94.53] [G loss: 19.131001]\n",
            "********* 857 [D loss: 0.222490, acc:, 92.19] [G loss: 5.783590]\n",
            "********* 858 [D loss: 0.360440, acc:, 85.16] [G loss: 9.794476]\n",
            "********* 859 [D loss: 0.096321, acc:, 97.66] [G loss: 30.918522]\n",
            "********* 860 [D loss: 0.800421, acc:, 89.06] [G loss: 18.971441]\n",
            "********* 861 [D loss: 0.128396, acc:, 92.97] [G loss: 9.759939]\n",
            "********* 862 [D loss: 0.139652, acc:, 93.75] [G loss: 10.903728]\n",
            "********* 863 [D loss: 0.000908, acc:, 100.00] [G loss: 19.559731]\n",
            "********* 864 [D loss: 0.013029, acc:, 100.00] [G loss: 24.941025]\n",
            "********* 865 [D loss: 0.176500, acc:, 94.53] [G loss: 22.477032]\n",
            "********* 866 [D loss: 0.057377, acc:, 97.66] [G loss: 15.538461]\n",
            "********* 867 [D loss: 0.010716, acc:, 100.00] [G loss: 10.607634]\n",
            "********* 868 [D loss: 0.023688, acc:, 99.22] [G loss: 9.888294]\n",
            "********* 869 [D loss: 0.019738, acc:, 99.22] [G loss: 8.724686]\n",
            "********* 870 [D loss: 0.008648, acc:, 99.22] [G loss: 10.035907]\n",
            "********* 871 [D loss: 0.004868, acc:, 100.00] [G loss: 11.614504]\n",
            "********* 872 [D loss: 0.002604, acc:, 100.00] [G loss: 11.393881]\n",
            "********* 873 [D loss: 0.009454, acc:, 99.22] [G loss: 13.170155]\n",
            "********* 874 [D loss: 0.054177, acc:, 98.44] [G loss: 11.886813]\n",
            "********* 875 [D loss: 0.008574, acc:, 100.00] [G loss: 8.199284]\n",
            "********* 876 [D loss: 0.011623, acc:, 100.00] [G loss: 7.793478]\n",
            "********* 877 [D loss: 0.035719, acc:, 97.66] [G loss: 8.470440]\n",
            "********* 878 [D loss: 0.026993, acc:, 99.22] [G loss: 9.031927]\n",
            "********* 879 [D loss: 0.012559, acc:, 100.00] [G loss: 10.951075]\n",
            "********* 880 [D loss: 0.008872, acc:, 100.00] [G loss: 11.860250]\n",
            "********* 881 [D loss: 0.025393, acc:, 99.22] [G loss: 12.062372]\n",
            "********* 882 [D loss: 0.012026, acc:, 100.00] [G loss: 12.900944]\n",
            "********* 883 [D loss: 0.080282, acc:, 96.88] [G loss: 9.777780]\n",
            "********* 884 [D loss: 0.041553, acc:, 99.22] [G loss: 10.277760]\n",
            "********* 885 [D loss: 0.015709, acc:, 100.00] [G loss: 10.297989]\n",
            "********* 886 [D loss: 0.026034, acc:, 99.22] [G loss: 10.001515]\n",
            "********* 887 [D loss: 0.015775, acc:, 100.00] [G loss: 10.837282]\n",
            "********* 888 [D loss: 0.038948, acc:, 98.44] [G loss: 9.869222]\n",
            "********* 889 [D loss: 0.019127, acc:, 99.22] [G loss: 9.119008]\n",
            "********* 890 [D loss: 0.050853, acc:, 97.66] [G loss: 7.655457]\n",
            "********* 891 [D loss: 0.062381, acc:, 97.66] [G loss: 10.518550]\n",
            "********* 892 [D loss: 0.011431, acc:, 100.00] [G loss: 14.722521]\n",
            "********* 893 [D loss: 0.025278, acc:, 98.44] [G loss: 15.571264]\n",
            "********* 894 [D loss: 0.147778, acc:, 93.75] [G loss: 7.458904]\n",
            "********* 895 [D loss: 0.107817, acc:, 96.09] [G loss: 6.598513]\n",
            "********* 896 [D loss: 0.009127, acc:, 100.00] [G loss: 9.695569]\n",
            "********* 897 [D loss: 0.010750, acc:, 99.22] [G loss: 12.585896]\n",
            "********* 898 [D loss: 0.074176, acc:, 96.88] [G loss: 12.497700]\n",
            "********* 899 [D loss: 0.057745, acc:, 99.22] [G loss: 8.388682]\n",
            "********* 900 [D loss: 0.098964, acc:, 96.09] [G loss: 9.989523]\n",
            "********* 901 [D loss: 0.011863, acc:, 100.00] [G loss: 11.284338]\n",
            "********* 902 [D loss: 0.101330, acc:, 98.44] [G loss: 11.962206]\n",
            "********* 903 [D loss: 0.006531, acc:, 100.00] [G loss: 11.189183]\n",
            "********* 904 [D loss: 0.041044, acc:, 97.66] [G loss: 9.037951]\n",
            "********* 905 [D loss: 0.010224, acc:, 100.00] [G loss: 7.846632]\n",
            "********* 906 [D loss: 0.018693, acc:, 100.00] [G loss: 7.366176]\n",
            "********* 907 [D loss: 0.087750, acc:, 96.88] [G loss: 8.122425]\n",
            "********* 908 [D loss: 0.015158, acc:, 99.22] [G loss: 9.872417]\n",
            "********* 909 [D loss: 0.032735, acc:, 98.44] [G loss: 8.395664]\n",
            "********* 910 [D loss: 0.014502, acc:, 100.00] [G loss: 7.285448]\n",
            "********* 911 [D loss: 0.054788, acc:, 97.66] [G loss: 7.096314]\n",
            "********* 912 [D loss: 0.035881, acc:, 99.22] [G loss: 7.785661]\n",
            "********* 913 [D loss: 0.012193, acc:, 100.00] [G loss: 8.393663]\n",
            "********* 914 [D loss: 0.042514, acc:, 99.22] [G loss: 7.750301]\n",
            "********* 915 [D loss: 0.057169, acc:, 97.66] [G loss: 6.279324]\n",
            "********* 916 [D loss: 0.025648, acc:, 100.00] [G loss: 8.443674]\n",
            "********* 917 [D loss: 0.011647, acc:, 100.00] [G loss: 9.603153]\n",
            "********* 918 [D loss: 0.035902, acc:, 97.66] [G loss: 8.425364]\n",
            "********* 919 [D loss: 0.011389, acc:, 100.00] [G loss: 8.316050]\n",
            "********* 920 [D loss: 0.019345, acc:, 100.00] [G loss: 7.595263]\n",
            "********* 921 [D loss: 0.010077, acc:, 100.00] [G loss: 9.223852]\n",
            "********* 922 [D loss: 0.048863, acc:, 97.66] [G loss: 7.481658]\n",
            "********* 923 [D loss: 0.017474, acc:, 100.00] [G loss: 6.408761]\n",
            "********* 924 [D loss: 0.048970, acc:, 98.44] [G loss: 6.214954]\n",
            "********* 925 [D loss: 0.006892, acc:, 100.00] [G loss: 8.480410]\n",
            "********* 926 [D loss: 0.006977, acc:, 100.00] [G loss: 10.419894]\n",
            "********* 927 [D loss: 0.107017, acc:, 97.66] [G loss: 6.681355]\n",
            "********* 928 [D loss: 0.107888, acc:, 96.88] [G loss: 7.470954]\n",
            "********* 929 [D loss: 0.005932, acc:, 100.00] [G loss: 11.311224]\n",
            "********* 930 [D loss: 0.040110, acc:, 97.66] [G loss: 12.751471]\n",
            "********* 931 [D loss: 0.083569, acc:, 98.44] [G loss: 9.142480]\n",
            "********* 932 [D loss: 0.036042, acc:, 99.22] [G loss: 7.775726]\n",
            "********* 933 [D loss: 0.018181, acc:, 99.22] [G loss: 8.172417]\n",
            "********* 934 [D loss: 0.001825, acc:, 100.00] [G loss: 9.252470]\n",
            "********* 935 [D loss: 0.018711, acc:, 99.22] [G loss: 10.700621]\n",
            "********* 936 [D loss: 0.012925, acc:, 99.22] [G loss: 10.494970]\n",
            "********* 937 [D loss: 0.005844, acc:, 100.00] [G loss: 9.038968]\n",
            "********* 938 [D loss: 0.003979, acc:, 100.00] [G loss: 9.676325]\n",
            "********* 939 [D loss: 0.006401, acc:, 100.00] [G loss: 9.028320]\n",
            "********* 940 [D loss: 0.003846, acc:, 100.00] [G loss: 8.112951]\n",
            "********* 941 [D loss: 0.064741, acc:, 98.44] [G loss: 8.163902]\n",
            "********* 942 [D loss: 0.025094, acc:, 99.22] [G loss: 10.574560]\n",
            "********* 943 [D loss: 0.035530, acc:, 99.22] [G loss: 8.279493]\n",
            "********* 944 [D loss: 0.048841, acc:, 98.44] [G loss: 7.193895]\n",
            "********* 945 [D loss: 0.024739, acc:, 99.22] [G loss: 7.384704]\n",
            "********* 946 [D loss: 0.009799, acc:, 100.00] [G loss: 8.512106]\n",
            "********* 947 [D loss: 0.009698, acc:, 99.22] [G loss: 9.451783]\n",
            "********* 948 [D loss: 0.002385, acc:, 100.00] [G loss: 9.268675]\n",
            "********* 949 [D loss: 0.027108, acc:, 99.22] [G loss: 9.191952]\n",
            "********* 950 [D loss: 0.044734, acc:, 98.44] [G loss: 9.130075]\n",
            "********* 951 [D loss: 0.007621, acc:, 100.00] [G loss: 9.992317]\n",
            "********* 952 [D loss: 0.018780, acc:, 99.22] [G loss: 9.707941]\n",
            "********* 953 [D loss: 0.036828, acc:, 98.44] [G loss: 6.178233]\n",
            "********* 954 [D loss: 0.038357, acc:, 99.22] [G loss: 5.797785]\n",
            "********* 955 [D loss: 0.021755, acc:, 99.22] [G loss: 8.651800]\n",
            "********* 956 [D loss: 0.027637, acc:, 98.44] [G loss: 8.587593]\n",
            "********* 957 [D loss: 0.016672, acc:, 99.22] [G loss: 7.956030]\n",
            "********* 958 [D loss: 0.008475, acc:, 100.00] [G loss: 7.269230]\n",
            "********* 959 [D loss: 0.008052, acc:, 100.00] [G loss: 8.541368]\n",
            "********* 960 [D loss: 0.014827, acc:, 100.00] [G loss: 7.815343]\n",
            "********* 961 [D loss: 0.007023, acc:, 100.00] [G loss: 8.909899]\n",
            "********* 962 [D loss: 0.020403, acc:, 99.22] [G loss: 9.238711]\n",
            "********* 963 [D loss: 0.017323, acc:, 99.22] [G loss: 8.626877]\n",
            "********* 964 [D loss: 0.045463, acc:, 98.44] [G loss: 8.321035]\n",
            "********* 965 [D loss: 0.021247, acc:, 100.00] [G loss: 7.760840]\n",
            "********* 966 [D loss: 0.017091, acc:, 99.22] [G loss: 8.426381]\n",
            "********* 967 [D loss: 0.007025, acc:, 100.00] [G loss: 8.589312]\n",
            "********* 968 [D loss: 0.007965, acc:, 100.00] [G loss: 9.335999]\n",
            "********* 969 [D loss: 0.008495, acc:, 100.00] [G loss: 7.810199]\n",
            "********* 970 [D loss: 0.005320, acc:, 100.00] [G loss: 7.453019]\n",
            "********* 971 [D loss: 0.031569, acc:, 98.44] [G loss: 8.129403]\n",
            "********* 972 [D loss: 0.042712, acc:, 98.44] [G loss: 8.243032]\n",
            "********* 973 [D loss: 0.031830, acc:, 98.44] [G loss: 8.056889]\n",
            "********* 974 [D loss: 0.084646, acc:, 98.44] [G loss: 8.570195]\n",
            "********* 975 [D loss: 0.022388, acc:, 99.22] [G loss: 8.596460]\n",
            "********* 976 [D loss: 0.006854, acc:, 100.00] [G loss: 9.022187]\n",
            "********* 977 [D loss: 0.007980, acc:, 99.22] [G loss: 10.013173]\n",
            "********* 978 [D loss: 0.014258, acc:, 99.22] [G loss: 7.638215]\n",
            "********* 979 [D loss: 0.023962, acc:, 100.00] [G loss: 7.488213]\n",
            "********* 980 [D loss: 0.002537, acc:, 100.00] [G loss: 8.504543]\n",
            "********* 981 [D loss: 0.007257, acc:, 100.00] [G loss: 9.250902]\n",
            "********* 982 [D loss: 0.006835, acc:, 100.00] [G loss: 8.951124]\n",
            "********* 983 [D loss: 0.003772, acc:, 100.00] [G loss: 9.366465]\n",
            "********* 984 [D loss: 0.013284, acc:, 100.00] [G loss: 8.325142]\n",
            "********* 985 [D loss: 0.003007, acc:, 100.00] [G loss: 9.632889]\n",
            "********* 986 [D loss: 0.006840, acc:, 100.00] [G loss: 9.235526]\n",
            "********* 987 [D loss: 0.015511, acc:, 100.00] [G loss: 7.923194]\n",
            "********* 988 [D loss: 0.031260, acc:, 99.22] [G loss: 6.731946]\n",
            "********* 989 [D loss: 0.016836, acc:, 100.00] [G loss: 6.255219]\n",
            "********* 990 [D loss: 0.009272, acc:, 100.00] [G loss: 8.648476]\n",
            "********* 991 [D loss: 0.030382, acc:, 99.22] [G loss: 7.063347]\n",
            "********* 992 [D loss: 0.011799, acc:, 100.00] [G loss: 7.300078]\n",
            "********* 993 [D loss: 0.025174, acc:, 99.22] [G loss: 6.345310]\n",
            "********* 994 [D loss: 0.027290, acc:, 100.00] [G loss: 8.602639]\n",
            "********* 995 [D loss: 0.011240, acc:, 100.00] [G loss: 10.636843]\n",
            "********* 996 [D loss: 0.029474, acc:, 99.22] [G loss: 8.471969]\n",
            "********* 997 [D loss: 0.026859, acc:, 99.22] [G loss: 6.995126]\n",
            "********* 998 [D loss: 0.008942, acc:, 100.00] [G loss: 7.954258]\n",
            "********* 999 [D loss: 0.005593, acc:, 100.00] [G loss: 8.914125]\n",
            "********* 1000 [D loss: 0.009094, acc:, 100.00] [G loss: 10.432503]\n",
            "********* 1001 [D loss: 0.015280, acc:, 100.00] [G loss: 9.921552]\n",
            "********* 1002 [D loss: 0.006785, acc:, 100.00] [G loss: 8.957790]\n",
            "********* 1003 [D loss: 0.022478, acc:, 99.22] [G loss: 9.434978]\n",
            "********* 1004 [D loss: 0.005082, acc:, 100.00] [G loss: 10.272017]\n",
            "********* 1005 [D loss: 0.001155, acc:, 100.00] [G loss: 11.631872]\n",
            "********* 1006 [D loss: 0.041951, acc:, 98.44] [G loss: 10.675684]\n",
            "********* 1007 [D loss: 0.016609, acc:, 99.22] [G loss: 8.888638]\n",
            "********* 1008 [D loss: 0.004890, acc:, 100.00] [G loss: 6.907608]\n",
            "********* 1009 [D loss: 0.099305, acc:, 96.09] [G loss: 10.090257]\n",
            "********* 1010 [D loss: 0.000716, acc:, 100.00] [G loss: 15.901598]\n",
            "********* 1011 [D loss: 0.107713, acc:, 97.66] [G loss: 11.567680]\n",
            "********* 1012 [D loss: 0.069518, acc:, 97.66] [G loss: 11.078205]\n",
            "********* 1013 [D loss: 0.020913, acc:, 99.22] [G loss: 13.158262]\n",
            "********* 1014 [D loss: 0.004012, acc:, 100.00] [G loss: 14.586132]\n",
            "********* 1015 [D loss: 0.051869, acc:, 96.88] [G loss: 11.623407]\n",
            "********* 1016 [D loss: 0.063442, acc:, 96.09] [G loss: 12.255846]\n",
            "********* 1017 [D loss: 0.010710, acc:, 100.00] [G loss: 18.803549]\n",
            "********* 1018 [D loss: 0.016311, acc:, 99.22] [G loss: 21.996243]\n",
            "********* 1019 [D loss: 0.243910, acc:, 92.19] [G loss: 10.229149]\n",
            "********* 1020 [D loss: 0.239926, acc:, 90.62] [G loss: 9.884576]\n",
            "********* 1021 [D loss: 0.001732, acc:, 100.00] [G loss: 17.652472]\n",
            "********* 1022 [D loss: 0.022025, acc:, 98.44] [G loss: 20.252975]\n",
            "********* 1023 [D loss: 0.112317, acc:, 97.66] [G loss: 13.233097]\n",
            "********* 1024 [D loss: 0.033461, acc:, 98.44] [G loss: 12.895290]\n",
            "********* 1025 [D loss: 0.051489, acc:, 96.88] [G loss: 15.004778]\n",
            "********* 1026 [D loss: 0.001150, acc:, 100.00] [G loss: 19.911486]\n",
            "********* 1027 [D loss: 0.011190, acc:, 99.22] [G loss: 22.693321]\n",
            "********* 1028 [D loss: 0.006102, acc:, 100.00] [G loss: 23.610558]\n",
            "********* 1029 [D loss: 0.037892, acc:, 98.44] [G loss: 19.416296]\n",
            "********* 1030 [D loss: 0.011743, acc:, 99.22] [G loss: 16.218868]\n",
            "********* 1031 [D loss: 0.004655, acc:, 100.00] [G loss: 13.842010]\n",
            "********* 1032 [D loss: 0.017161, acc:, 99.22] [G loss: 13.077249]\n",
            "********* 1033 [D loss: 0.020685, acc:, 98.44] [G loss: 12.237032]\n",
            "********* 1034 [D loss: 0.051988, acc:, 99.22] [G loss: 12.064018]\n",
            "********* 1035 [D loss: 0.011073, acc:, 99.22] [G loss: 14.595457]\n",
            "********* 1036 [D loss: 0.005728, acc:, 100.00] [G loss: 15.704124]\n",
            "********* 1037 [D loss: 0.008050, acc:, 100.00] [G loss: 15.754539]\n",
            "********* 1038 [D loss: 0.022997, acc:, 99.22] [G loss: 14.677310]\n",
            "********* 1039 [D loss: 0.015564, acc:, 99.22] [G loss: 11.102131]\n",
            "********* 1040 [D loss: 0.021264, acc:, 99.22] [G loss: 9.537691]\n",
            "********* 1041 [D loss: 0.030231, acc:, 98.44] [G loss: 9.443681]\n",
            "********* 1042 [D loss: 0.043280, acc:, 98.44] [G loss: 10.093017]\n",
            "********* 1043 [D loss: 0.007135, acc:, 100.00] [G loss: 11.837935]\n",
            "********* 1044 [D loss: 0.004524, acc:, 100.00] [G loss: 12.366894]\n",
            "********* 1045 [D loss: 0.004201, acc:, 100.00] [G loss: 12.809464]\n",
            "********* 1046 [D loss: 0.051443, acc:, 98.44] [G loss: 10.947519]\n",
            "********* 1047 [D loss: 0.015153, acc:, 100.00] [G loss: 9.299818]\n",
            "********* 1048 [D loss: 0.026196, acc:, 98.44] [G loss: 8.061010]\n",
            "********* 1049 [D loss: 0.015553, acc:, 99.22] [G loss: 8.115669]\n",
            "********* 1050 [D loss: 0.010644, acc:, 100.00] [G loss: 8.648941]\n",
            "********* 1051 [D loss: 0.010689, acc:, 100.00] [G loss: 10.688505]\n",
            "********* 1052 [D loss: 0.012762, acc:, 100.00] [G loss: 11.251956]\n",
            "********* 1053 [D loss: 0.004352, acc:, 100.00] [G loss: 12.311518]\n",
            "********* 1054 [D loss: 0.025035, acc:, 99.22] [G loss: 10.161608]\n",
            "********* 1055 [D loss: 0.029364, acc:, 99.22] [G loss: 8.702271]\n",
            "********* 1056 [D loss: 0.020942, acc:, 99.22] [G loss: 8.497171]\n",
            "********* 1057 [D loss: 0.016566, acc:, 100.00] [G loss: 9.753407]\n",
            "********* 1058 [D loss: 0.033706, acc:, 99.22] [G loss: 8.378355]\n",
            "********* 1059 [D loss: 0.013893, acc:, 99.22] [G loss: 7.660923]\n",
            "********* 1060 [D loss: 0.068694, acc:, 98.44] [G loss: 7.549585]\n",
            "********* 1061 [D loss: 0.004045, acc:, 100.00] [G loss: 8.875361]\n",
            "********* 1062 [D loss: 0.001784, acc:, 100.00] [G loss: 9.594861]\n",
            "********* 1063 [D loss: 0.003968, acc:, 100.00] [G loss: 10.278321]\n",
            "********* 1064 [D loss: 0.012338, acc:, 100.00] [G loss: 9.565361]\n",
            "********* 1065 [D loss: 0.005576, acc:, 100.00] [G loss: 8.919319]\n",
            "********* 1066 [D loss: 0.045153, acc:, 98.44] [G loss: 7.790587]\n",
            "********* 1067 [D loss: 0.009895, acc:, 100.00] [G loss: 7.818634]\n",
            "********* 1068 [D loss: 0.010876, acc:, 100.00] [G loss: 8.598129]\n",
            "********* 1069 [D loss: 0.046920, acc:, 99.22] [G loss: 8.549411]\n",
            "********* 1070 [D loss: 0.033256, acc:, 99.22] [G loss: 8.985508]\n",
            "********* 1071 [D loss: 0.017595, acc:, 99.22] [G loss: 8.441338]\n",
            "********* 1072 [D loss: 0.101653, acc:, 97.66] [G loss: 7.946969]\n",
            "********* 1073 [D loss: 0.004342, acc:, 100.00] [G loss: 8.188943]\n",
            "********* 1074 [D loss: 0.015305, acc:, 100.00] [G loss: 9.187676]\n",
            "********* 1075 [D loss: 0.050114, acc:, 99.22] [G loss: 7.570916]\n",
            "********* 1076 [D loss: 0.012773, acc:, 100.00] [G loss: 7.471633]\n",
            "********* 1077 [D loss: 0.016091, acc:, 100.00] [G loss: 7.419738]\n",
            "********* 1078 [D loss: 0.031299, acc:, 99.22] [G loss: 9.521114]\n",
            "********* 1079 [D loss: 0.029192, acc:, 99.22] [G loss: 11.004445]\n",
            "********* 1080 [D loss: 0.040074, acc:, 99.22] [G loss: 9.332836]\n",
            "********* 1081 [D loss: 0.003988, acc:, 100.00] [G loss: 7.282405]\n",
            "********* 1082 [D loss: 0.009263, acc:, 100.00] [G loss: 7.575322]\n",
            "********* 1083 [D loss: 0.009885, acc:, 100.00] [G loss: 6.907486]\n",
            "********* 1084 [D loss: 0.008069, acc:, 100.00] [G loss: 7.415033]\n",
            "********* 1085 [D loss: 0.014288, acc:, 99.22] [G loss: 8.712265]\n",
            "********* 1086 [D loss: 0.003098, acc:, 100.00] [G loss: 8.848825]\n",
            "********* 1087 [D loss: 0.051623, acc:, 97.66] [G loss: 8.605245]\n",
            "********* 1088 [D loss: 0.011796, acc:, 100.00] [G loss: 9.160627]\n",
            "********* 1089 [D loss: 0.010769, acc:, 100.00] [G loss: 10.077150]\n",
            "********* 1090 [D loss: 0.023289, acc:, 99.22] [G loss: 8.282100]\n",
            "********* 1091 [D loss: 0.018471, acc:, 99.22] [G loss: 7.337096]\n",
            "********* 1092 [D loss: 0.026697, acc:, 99.22] [G loss: 6.831082]\n",
            "********* 1093 [D loss: 0.016268, acc:, 100.00] [G loss: 7.203339]\n",
            "********* 1094 [D loss: 0.015725, acc:, 99.22] [G loss: 8.637878]\n",
            "********* 1095 [D loss: 0.015358, acc:, 99.22] [G loss: 9.034786]\n",
            "********* 1096 [D loss: 0.011159, acc:, 100.00] [G loss: 8.650508]\n",
            "********* 1097 [D loss: 0.030268, acc:, 99.22] [G loss: 7.044651]\n",
            "********* 1098 [D loss: 0.074801, acc:, 97.66] [G loss: 8.185373]\n",
            "********* 1099 [D loss: 0.006110, acc:, 100.00] [G loss: 9.136750]\n",
            "********* 1100 [D loss: 0.005176, acc:, 100.00] [G loss: 10.522463]\n",
            "********* 1101 [D loss: 0.015816, acc:, 99.22] [G loss: 9.873718]\n",
            "********* 1102 [D loss: 0.052460, acc:, 97.66] [G loss: 10.874271]\n",
            "********* 1103 [D loss: 0.007069, acc:, 100.00] [G loss: 13.753597]\n",
            "********* 1104 [D loss: 0.050956, acc:, 98.44] [G loss: 10.592989]\n",
            "********* 1105 [D loss: 0.013967, acc:, 100.00] [G loss: 8.782063]\n",
            "********* 1106 [D loss: 0.015597, acc:, 100.00] [G loss: 8.610477]\n",
            "********* 1107 [D loss: 0.002727, acc:, 100.00] [G loss: 10.497896]\n",
            "********* 1108 [D loss: 0.117805, acc:, 99.22] [G loss: 11.628963]\n",
            "********* 1109 [D loss: 0.014415, acc:, 99.22] [G loss: 9.769558]\n",
            "********* 1110 [D loss: 0.043286, acc:, 96.88] [G loss: 10.949829]\n",
            "********* 1111 [D loss: 0.010757, acc:, 100.00] [G loss: 10.441631]\n",
            "********* 1112 [D loss: 0.009950, acc:, 100.00] [G loss: 9.984890]\n",
            "********* 1113 [D loss: 0.012690, acc:, 99.22] [G loss: 10.603388]\n",
            "********* 1114 [D loss: 0.014465, acc:, 99.22] [G loss: 11.206434]\n",
            "********* 1115 [D loss: 0.071108, acc:, 99.22] [G loss: 10.149394]\n",
            "********* 1116 [D loss: 0.003679, acc:, 100.00] [G loss: 9.462292]\n",
            "********* 1117 [D loss: 0.007462, acc:, 99.22] [G loss: 9.753596]\n",
            "********* 1118 [D loss: 0.035771, acc:, 99.22] [G loss: 9.994513]\n",
            "********* 1119 [D loss: 0.035181, acc:, 98.44] [G loss: 12.288219]\n",
            "********* 1120 [D loss: 0.017258, acc:, 99.22] [G loss: 12.831686]\n",
            "********* 1121 [D loss: 0.065447, acc:, 98.44] [G loss: 10.564504]\n",
            "********* 1122 [D loss: 0.042910, acc:, 98.44] [G loss: 12.660233]\n",
            "********* 1123 [D loss: 0.036417, acc:, 98.44] [G loss: 14.782318]\n",
            "********* 1124 [D loss: 0.008477, acc:, 100.00] [G loss: 13.471549]\n",
            "********* 1125 [D loss: 0.040402, acc:, 97.66] [G loss: 11.291441]\n",
            "********* 1126 [D loss: 0.016686, acc:, 99.22] [G loss: 10.388713]\n",
            "********* 1127 [D loss: 0.021309, acc:, 99.22] [G loss: 10.460221]\n",
            "********* 1128 [D loss: 0.001092, acc:, 100.00] [G loss: 13.650179]\n",
            "********* 1129 [D loss: 0.010244, acc:, 99.22] [G loss: 13.470415]\n",
            "********* 1130 [D loss: 0.014857, acc:, 99.22] [G loss: 12.047355]\n",
            "********* 1131 [D loss: 0.010235, acc:, 100.00] [G loss: 8.707846]\n",
            "********* 1132 [D loss: 0.048075, acc:, 96.88] [G loss: 8.066957]\n",
            "********* 1133 [D loss: 0.003049, acc:, 100.00] [G loss: 11.337387]\n",
            "********* 1134 [D loss: 0.011821, acc:, 99.22] [G loss: 14.377878]\n",
            "********* 1135 [D loss: 0.011166, acc:, 100.00] [G loss: 15.864185]\n",
            "********* 1136 [D loss: 0.020420, acc:, 98.44] [G loss: 13.268445]\n",
            "********* 1137 [D loss: 0.010213, acc:, 100.00] [G loss: 10.183544]\n",
            "********* 1138 [D loss: 0.005419, acc:, 100.00] [G loss: 9.042467]\n",
            "********* 1139 [D loss: 0.022558, acc:, 99.22] [G loss: 8.593605]\n",
            "********* 1140 [D loss: 0.026257, acc:, 99.22] [G loss: 9.680870]\n",
            "********* 1141 [D loss: 0.062595, acc:, 99.22] [G loss: 10.320038]\n",
            "********* 1142 [D loss: 0.028343, acc:, 99.22] [G loss: 9.757120]\n",
            "********* 1143 [D loss: 0.057739, acc:, 99.22] [G loss: 9.861101]\n",
            "********* 1144 [D loss: 0.030695, acc:, 98.44] [G loss: 8.578484]\n",
            "********* 1145 [D loss: 0.005487, acc:, 100.00] [G loss: 10.082348]\n",
            "********* 1146 [D loss: 0.004435, acc:, 100.00] [G loss: 10.720083]\n",
            "********* 1147 [D loss: 0.011825, acc:, 99.22] [G loss: 9.688690]\n",
            "********* 1148 [D loss: 0.027499, acc:, 99.22] [G loss: 9.037575]\n",
            "********* 1149 [D loss: 0.017635, acc:, 100.00] [G loss: 9.849426]\n",
            "********* 1150 [D loss: 0.071985, acc:, 98.44] [G loss: 9.260550]\n",
            "********* 1151 [D loss: 0.028199, acc:, 99.22] [G loss: 9.248340]\n",
            "********* 1152 [D loss: 0.095296, acc:, 98.44] [G loss: 9.182081]\n",
            "********* 1153 [D loss: 0.016369, acc:, 99.22] [G loss: 10.728459]\n",
            "********* 1154 [D loss: 0.069199, acc:, 98.44] [G loss: 8.929695]\n",
            "********* 1155 [D loss: 0.044738, acc:, 99.22] [G loss: 8.396074]\n",
            "********* 1156 [D loss: 0.014329, acc:, 100.00] [G loss: 11.408233]\n",
            "********* 1157 [D loss: 0.000856, acc:, 100.00] [G loss: 15.434174]\n",
            "********* 1158 [D loss: 0.021022, acc:, 99.22] [G loss: 17.671062]\n",
            "********* 1159 [D loss: 0.089840, acc:, 96.09] [G loss: 8.096133]\n",
            "********* 1160 [D loss: 0.144853, acc:, 92.19] [G loss: 10.622830]\n",
            "********* 1161 [D loss: 0.084627, acc:, 96.88] [G loss: 22.125340]\n",
            "********* 1162 [D loss: 0.141452, acc:, 94.53] [G loss: 18.858112]\n",
            "********* 1163 [D loss: 0.044744, acc:, 97.66] [G loss: 14.162348]\n",
            "********* 1164 [D loss: 0.179681, acc:, 90.62] [G loss: 11.440559]\n",
            "********* 1165 [D loss: 0.039158, acc:, 99.22] [G loss: 14.860098]\n",
            "********* 1166 [D loss: 0.017041, acc:, 99.22] [G loss: 18.787357]\n",
            "********* 1167 [D loss: 0.016458, acc:, 99.22] [G loss: 20.027920]\n",
            "********* 1168 [D loss: 0.081197, acc:, 98.44] [G loss: 17.377182]\n",
            "********* 1169 [D loss: 0.116943, acc:, 95.31] [G loss: 15.937601]\n",
            "********* 1170 [D loss: 0.096139, acc:, 98.44] [G loss: 17.429001]\n",
            "********* 1171 [D loss: 0.009306, acc:, 99.22] [G loss: 19.291407]\n",
            "********* 1172 [D loss: 0.099707, acc:, 97.66] [G loss: 17.578197]\n",
            "********* 1173 [D loss: 0.043166, acc:, 99.22] [G loss: 17.975254]\n",
            "********* 1174 [D loss: 0.037227, acc:, 98.44] [G loss: 15.661366]\n",
            "********* 1175 [D loss: 0.005504, acc:, 100.00] [G loss: 16.300488]\n",
            "********* 1176 [D loss: 0.006346, acc:, 100.00] [G loss: 18.189240]\n",
            "********* 1177 [D loss: 0.050212, acc:, 97.66] [G loss: 16.213299]\n",
            "********* 1178 [D loss: 0.013005, acc:, 99.22] [G loss: 14.535585]\n",
            "********* 1179 [D loss: 0.030401, acc:, 98.44] [G loss: 15.543280]\n",
            "********* 1180 [D loss: 0.023032, acc:, 99.22] [G loss: 17.186123]\n",
            "********* 1181 [D loss: 0.004929, acc:, 100.00] [G loss: 14.801126]\n",
            "********* 1182 [D loss: 0.023921, acc:, 99.22] [G loss: 13.695020]\n",
            "********* 1183 [D loss: 0.029713, acc:, 98.44] [G loss: 13.988420]\n",
            "********* 1184 [D loss: 0.061061, acc:, 96.88] [G loss: 11.214121]\n",
            "********* 1185 [D loss: 0.036681, acc:, 99.22] [G loss: 9.330767]\n",
            "********* 1186 [D loss: 0.023155, acc:, 99.22] [G loss: 12.350218]\n",
            "********* 1187 [D loss: 0.033007, acc:, 99.22] [G loss: 14.347612]\n",
            "********* 1188 [D loss: 0.062390, acc:, 97.66] [G loss: 12.657436]\n",
            "********* 1189 [D loss: 0.064667, acc:, 97.66] [G loss: 10.497705]\n",
            "********* 1190 [D loss: 0.007142, acc:, 100.00] [G loss: 11.593192]\n",
            "********* 1191 [D loss: 0.060345, acc:, 98.44] [G loss: 13.423634]\n",
            "********* 1192 [D loss: 0.113424, acc:, 98.44] [G loss: 10.461147]\n",
            "********* 1193 [D loss: 0.045496, acc:, 99.22] [G loss: 10.654570]\n",
            "********* 1194 [D loss: 0.006532, acc:, 100.00] [G loss: 13.168470]\n",
            "********* 1195 [D loss: 0.022251, acc:, 98.44] [G loss: 10.729845]\n",
            "********* 1196 [D loss: 0.043165, acc:, 97.66] [G loss: 6.110676]\n",
            "********* 1197 [D loss: 0.127194, acc:, 92.97] [G loss: 15.830233]\n",
            "********* 1198 [D loss: 0.205860, acc:, 94.53] [G loss: 23.093069]\n",
            "********* 1199 [D loss: 0.084313, acc:, 97.66] [G loss: 18.779419]\n",
            "********* 1200 [D loss: 0.199869, acc:, 92.19] [G loss: 7.447190]\n",
            "********* 1201 [D loss: 0.181322, acc:, 95.31] [G loss: 6.345540]\n",
            "********* 1202 [D loss: 0.113732, acc:, 95.31] [G loss: 12.455458]\n",
            "********* 1203 [D loss: 0.210615, acc:, 94.53] [G loss: 15.537498]\n",
            "********* 1204 [D loss: 0.126402, acc:, 96.88] [G loss: 12.345104]\n",
            "********* 1205 [D loss: 0.164749, acc:, 95.31] [G loss: 11.801159]\n",
            "********* 1206 [D loss: 0.031492, acc:, 98.44] [G loss: 13.770015]\n",
            "********* 1207 [D loss: 0.015245, acc:, 99.22] [G loss: 14.029404]\n",
            "********* 1208 [D loss: 0.024092, acc:, 98.44] [G loss: 12.457721]\n",
            "********* 1209 [D loss: 0.045367, acc:, 96.88] [G loss: 12.171322]\n",
            "********* 1210 [D loss: 0.067723, acc:, 98.44] [G loss: 11.618752]\n",
            "********* 1211 [D loss: 0.020517, acc:, 99.22] [G loss: 12.818932]\n",
            "********* 1212 [D loss: 0.009361, acc:, 99.22] [G loss: 9.913599]\n",
            "********* 1213 [D loss: 0.017871, acc:, 100.00] [G loss: 11.794297]\n",
            "********* 1214 [D loss: 0.019644, acc:, 99.22] [G loss: 12.554882]\n",
            "********* 1215 [D loss: 0.026205, acc:, 99.22] [G loss: 11.805117]\n",
            "********* 1216 [D loss: 0.047712, acc:, 97.66] [G loss: 10.508978]\n",
            "********* 1217 [D loss: 0.047211, acc:, 97.66] [G loss: 10.188219]\n",
            "********* 1218 [D loss: 0.014692, acc:, 100.00] [G loss: 11.396824]\n",
            "********* 1219 [D loss: 0.048029, acc:, 99.22] [G loss: 10.170507]\n",
            "********* 1220 [D loss: 0.085962, acc:, 97.66] [G loss: 8.644243]\n",
            "********* 1221 [D loss: 0.015943, acc:, 100.00] [G loss: 10.651801]\n",
            "********* 1222 [D loss: 0.021343, acc:, 99.22] [G loss: 9.786608]\n",
            "********* 1223 [D loss: 0.066203, acc:, 97.66] [G loss: 9.943563]\n",
            "********* 1224 [D loss: 0.012138, acc:, 100.00] [G loss: 10.133404]\n",
            "********* 1225 [D loss: 0.033662, acc:, 99.22] [G loss: 8.220801]\n",
            "********* 1226 [D loss: 0.137534, acc:, 96.09] [G loss: 10.911335]\n",
            "********* 1227 [D loss: 0.022610, acc:, 99.22] [G loss: 16.300859]\n",
            "********* 1228 [D loss: 0.066776, acc:, 95.31] [G loss: 14.625787]\n",
            "********* 1229 [D loss: 0.038479, acc:, 98.44] [G loss: 10.002497]\n",
            "********* 1230 [D loss: 0.030088, acc:, 98.44] [G loss: 5.995065]\n",
            "********* 1231 [D loss: 0.048261, acc:, 97.66] [G loss: 7.332704]\n",
            "********* 1232 [D loss: 0.023134, acc:, 99.22] [G loss: 11.244020]\n",
            "********* 1233 [D loss: 0.025776, acc:, 99.22] [G loss: 11.903728]\n",
            "********* 1234 [D loss: 0.007017, acc:, 100.00] [G loss: 11.723961]\n",
            "********* 1235 [D loss: 0.037391, acc:, 99.22] [G loss: 12.733268]\n",
            "********* 1236 [D loss: 0.062910, acc:, 99.22] [G loss: 9.199337]\n",
            "********* 1237 [D loss: 0.005890, acc:, 100.00] [G loss: 8.597063]\n",
            "********* 1238 [D loss: 0.013198, acc:, 99.22] [G loss: 8.691433]\n",
            "********* 1239 [D loss: 0.005014, acc:, 100.00] [G loss: 9.405516]\n",
            "********* 1240 [D loss: 0.012999, acc:, 99.22] [G loss: 9.494664]\n",
            "********* 1241 [D loss: 0.039209, acc:, 99.22] [G loss: 8.689016]\n",
            "********* 1242 [D loss: 0.028613, acc:, 98.44] [G loss: 7.994922]\n",
            "********* 1243 [D loss: 0.051521, acc:, 97.66] [G loss: 8.143135]\n",
            "********* 1244 [D loss: 0.017513, acc:, 99.22] [G loss: 10.778423]\n",
            "********* 1245 [D loss: 0.051290, acc:, 97.66] [G loss: 8.225878]\n",
            "********* 1246 [D loss: 0.054521, acc:, 98.44] [G loss: 7.227181]\n",
            "********* 1247 [D loss: 0.013241, acc:, 99.22] [G loss: 9.805819]\n",
            "********* 1248 [D loss: 0.020936, acc:, 99.22] [G loss: 10.484976]\n",
            "********* 1249 [D loss: 0.006660, acc:, 100.00] [G loss: 10.036961]\n",
            "********* 1250 [D loss: 0.008691, acc:, 100.00] [G loss: 10.309528]\n",
            "********* 1251 [D loss: 0.032646, acc:, 97.66] [G loss: 9.387177]\n",
            "********* 1252 [D loss: 0.018722, acc:, 99.22] [G loss: 8.729326]\n",
            "********* 1253 [D loss: 0.012577, acc:, 100.00] [G loss: 8.588053]\n",
            "********* 1254 [D loss: 0.018950, acc:, 99.22] [G loss: 7.637664]\n",
            "********* 1255 [D loss: 0.011079, acc:, 100.00] [G loss: 8.014576]\n",
            "********* 1256 [D loss: 0.039120, acc:, 98.44] [G loss: 9.956043]\n",
            "********* 1257 [D loss: 0.038903, acc:, 97.66] [G loss: 8.514647]\n",
            "********* 1258 [D loss: 0.028115, acc:, 98.44] [G loss: 6.792324]\n",
            "********* 1259 [D loss: 0.020556, acc:, 98.44] [G loss: 6.064174]\n",
            "********* 1260 [D loss: 0.112454, acc:, 94.53] [G loss: 8.402085]\n",
            "********* 1261 [D loss: 0.013313, acc:, 100.00] [G loss: 13.153587]\n",
            "********* 1262 [D loss: 0.571265, acc:, 89.84] [G loss: 6.572710]\n",
            "********* 1263 [D loss: 0.244506, acc:, 87.50] [G loss: 9.708796]\n",
            "********* 1264 [D loss: 0.035137, acc:, 97.66] [G loss: 16.705273]\n",
            "********* 1265 [D loss: 0.466597, acc:, 90.62] [G loss: 5.047405]\n",
            "********* 1266 [D loss: 0.996252, acc:, 67.19] [G loss: 18.201180]\n",
            "********* 1267 [D loss: 0.076238, acc:, 98.44] [G loss: 57.395569]\n",
            "********* 1268 [D loss: 3.487311, acc:, 74.22] [G loss: 32.503426]\n",
            "********* 1269 [D loss: 0.603972, acc:, 88.28] [G loss: 14.742929]\n",
            "********* 1270 [D loss: 0.473163, acc:, 89.84] [G loss: 6.244302]\n",
            "********* 1271 [D loss: 0.602854, acc:, 85.94] [G loss: 10.834226]\n",
            "********* 1272 [D loss: 0.041111, acc:, 96.88] [G loss: 20.152496]\n",
            "********* 1273 [D loss: 0.551849, acc:, 89.06] [G loss: 20.039616]\n",
            "********* 1274 [D loss: 0.140130, acc:, 95.31] [G loss: 17.907904]\n",
            "********* 1275 [D loss: 0.028034, acc:, 98.44] [G loss: 14.015173]\n",
            "********* 1276 [D loss: 0.108294, acc:, 98.44] [G loss: 10.203926]\n",
            "********* 1277 [D loss: 0.165768, acc:, 94.53] [G loss: 8.716042]\n",
            "********* 1278 [D loss: 0.102732, acc:, 95.31] [G loss: 9.380035]\n",
            "********* 1279 [D loss: 0.000551, acc:, 100.00] [G loss: 10.807592]\n",
            "********* 1280 [D loss: 0.044003, acc:, 99.22] [G loss: 11.342354]\n",
            "********* 1281 [D loss: 0.006449, acc:, 100.00] [G loss: 12.870317]\n",
            "********* 1282 [D loss: 0.032473, acc:, 99.22] [G loss: 13.584764]\n",
            "********* 1283 [D loss: 0.030931, acc:, 99.22] [G loss: 11.867277]\n",
            "********* 1284 [D loss: 0.087604, acc:, 97.66] [G loss: 9.401657]\n",
            "********* 1285 [D loss: 0.029110, acc:, 99.22] [G loss: 9.868972]\n",
            "********* 1286 [D loss: 0.032051, acc:, 98.44] [G loss: 8.942835]\n",
            "********* 1287 [D loss: 0.003759, acc:, 100.00] [G loss: 9.259733]\n",
            "********* 1288 [D loss: 0.017165, acc:, 100.00] [G loss: 9.989744]\n",
            "********* 1289 [D loss: 0.009030, acc:, 100.00] [G loss: 9.903090]\n",
            "********* 1290 [D loss: 0.028855, acc:, 99.22] [G loss: 8.562866]\n",
            "********* 1291 [D loss: 0.012036, acc:, 100.00] [G loss: 8.690914]\n",
            "********* 1292 [D loss: 0.010851, acc:, 99.22] [G loss: 9.173904]\n",
            "********* 1293 [D loss: 0.021403, acc:, 98.44] [G loss: 8.304150]\n",
            "********* 1294 [D loss: 0.013492, acc:, 100.00] [G loss: 8.271839]\n",
            "********* 1295 [D loss: 0.024998, acc:, 98.44] [G loss: 8.024973]\n",
            "********* 1296 [D loss: 0.035053, acc:, 97.66] [G loss: 8.062276]\n",
            "********* 1297 [D loss: 0.028725, acc:, 99.22] [G loss: 8.009645]\n",
            "********* 1298 [D loss: 0.030880, acc:, 99.22] [G loss: 8.577511]\n",
            "********* 1299 [D loss: 0.011964, acc:, 100.00] [G loss: 8.993516]\n",
            "********* 1300 [D loss: 0.045317, acc:, 99.22] [G loss: 7.998726]\n",
            "********* 1301 [D loss: 0.022835, acc:, 100.00] [G loss: 7.630879]\n",
            "********* 1302 [D loss: 0.021252, acc:, 100.00] [G loss: 6.654202]\n",
            "********* 1303 [D loss: 0.048181, acc:, 98.44] [G loss: 7.002122]\n",
            "********* 1304 [D loss: 0.011935, acc:, 100.00] [G loss: 6.745144]\n",
            "********* 1305 [D loss: 0.017516, acc:, 100.00] [G loss: 7.806220]\n",
            "********* 1306 [D loss: 0.028936, acc:, 99.22] [G loss: 7.438366]\n",
            "********* 1307 [D loss: 0.062567, acc:, 97.66] [G loss: 6.403126]\n",
            "********* 1308 [D loss: 0.028816, acc:, 100.00] [G loss: 6.393244]\n",
            "********* 1309 [D loss: 0.031772, acc:, 98.44] [G loss: 7.131644]\n",
            "********* 1310 [D loss: 0.042270, acc:, 97.66] [G loss: 7.988116]\n",
            "********* 1311 [D loss: 0.026208, acc:, 99.22] [G loss: 7.621099]\n",
            "********* 1312 [D loss: 0.053519, acc:, 98.44] [G loss: 6.441908]\n",
            "********* 1313 [D loss: 0.074417, acc:, 97.66] [G loss: 5.659970]\n",
            "********* 1314 [D loss: 0.024224, acc:, 100.00] [G loss: 5.940826]\n",
            "********* 1315 [D loss: 0.087862, acc:, 98.44] [G loss: 6.670093]\n",
            "********* 1316 [D loss: 0.054922, acc:, 98.44] [G loss: 7.642364]\n",
            "********* 1317 [D loss: 0.033813, acc:, 98.44] [G loss: 7.658257]\n",
            "********* 1318 [D loss: 0.046388, acc:, 97.66] [G loss: 7.450609]\n",
            "********* 1319 [D loss: 0.053942, acc:, 96.88] [G loss: 5.638981]\n",
            "********* 1320 [D loss: 0.106280, acc:, 95.31] [G loss: 6.965754]\n",
            "********* 1321 [D loss: 0.045619, acc:, 98.44] [G loss: 9.167681]\n",
            "********* 1322 [D loss: 0.032392, acc:, 98.44] [G loss: 11.774494]\n",
            "********* 1323 [D loss: 0.061581, acc:, 97.66] [G loss: 8.680130]\n",
            "********* 1324 [D loss: 0.026090, acc:, 100.00] [G loss: 6.117355]\n",
            "********* 1325 [D loss: 0.050637, acc:, 98.44] [G loss: 5.662499]\n",
            "********* 1326 [D loss: 0.035088, acc:, 99.22] [G loss: 6.765307]\n",
            "********* 1327 [D loss: 0.017091, acc:, 99.22] [G loss: 8.301357]\n",
            "********* 1328 [D loss: 0.021126, acc:, 99.22] [G loss: 9.420041]\n",
            "********* 1329 [D loss: 0.017506, acc:, 99.22] [G loss: 8.821374]\n",
            "********* 1330 [D loss: 0.017267, acc:, 100.00] [G loss: 6.914253]\n",
            "********* 1331 [D loss: 0.035651, acc:, 99.22] [G loss: 6.837563]\n",
            "********* 1332 [D loss: 0.017767, acc:, 99.22] [G loss: 6.934428]\n",
            "********* 1333 [D loss: 0.025931, acc:, 98.44] [G loss: 7.857763]\n",
            "********* 1334 [D loss: 0.003759, acc:, 100.00] [G loss: 8.292572]\n",
            "********* 1335 [D loss: 0.023106, acc:, 99.22] [G loss: 6.937403]\n",
            "********* 1336 [D loss: 0.027497, acc:, 98.44] [G loss: 6.345474]\n",
            "********* 1337 [D loss: 0.032054, acc:, 98.44] [G loss: 5.492270]\n",
            "********* 1338 [D loss: 0.038835, acc:, 98.44] [G loss: 5.973793]\n",
            "********* 1339 [D loss: 0.012961, acc:, 100.00] [G loss: 6.499952]\n",
            "********* 1340 [D loss: 0.011405, acc:, 100.00] [G loss: 7.419870]\n",
            "********* 1341 [D loss: 0.031677, acc:, 99.22] [G loss: 7.364552]\n",
            "********* 1342 [D loss: 0.028803, acc:, 99.22] [G loss: 6.484681]\n",
            "********* 1343 [D loss: 0.025667, acc:, 99.22] [G loss: 7.015220]\n",
            "********* 1344 [D loss: 0.010827, acc:, 100.00] [G loss: 7.828565]\n",
            "********* 1345 [D loss: 0.024321, acc:, 99.22] [G loss: 7.444093]\n",
            "********* 1346 [D loss: 0.018437, acc:, 100.00] [G loss: 6.351930]\n",
            "********* 1347 [D loss: 0.056453, acc:, 97.66] [G loss: 6.180066]\n",
            "********* 1348 [D loss: 0.032319, acc:, 98.44] [G loss: 7.344465]\n",
            "********* 1349 [D loss: 0.013068, acc:, 99.22] [G loss: 8.008677]\n",
            "********* 1350 [D loss: 0.011616, acc:, 100.00] [G loss: 6.809991]\n",
            "********* 1351 [D loss: 0.020551, acc:, 100.00] [G loss: 6.809839]\n",
            "********* 1352 [D loss: 0.079492, acc:, 97.66] [G loss: 6.338170]\n",
            "********* 1353 [D loss: 0.005741, acc:, 100.00] [G loss: 8.848135]\n",
            "********* 1354 [D loss: 0.022676, acc:, 99.22] [G loss: 8.402153]\n",
            "********* 1355 [D loss: 0.058992, acc:, 98.44] [G loss: 5.573480]\n",
            "********* 1356 [D loss: 0.054416, acc:, 99.22] [G loss: 5.661151]\n",
            "********* 1357 [D loss: 0.010652, acc:, 100.00] [G loss: 8.852645]\n",
            "********* 1358 [D loss: 0.009029, acc:, 100.00] [G loss: 10.597858]\n",
            "********* 1359 [D loss: 0.077521, acc:, 96.88] [G loss: 5.255527]\n",
            "********* 1360 [D loss: 0.118235, acc:, 94.53] [G loss: 6.069211]\n",
            "********* 1361 [D loss: 0.000761, acc:, 100.00] [G loss: 11.956326]\n",
            "********* 1362 [D loss: 0.077106, acc:, 96.09] [G loss: 11.731284]\n",
            "********* 1363 [D loss: 0.067987, acc:, 96.88] [G loss: 8.750716]\n",
            "********* 1364 [D loss: 0.054875, acc:, 97.66] [G loss: 5.942565]\n",
            "********* 1365 [D loss: 0.062569, acc:, 96.88] [G loss: 8.334300]\n",
            "********* 1366 [D loss: 0.024722, acc:, 99.22] [G loss: 10.440487]\n",
            "********* 1367 [D loss: 0.061382, acc:, 96.09] [G loss: 10.138575]\n",
            "********* 1368 [D loss: 0.006885, acc:, 100.00] [G loss: 8.301374]\n",
            "********* 1369 [D loss: 0.012566, acc:, 100.00] [G loss: 8.076521]\n",
            "********* 1370 [D loss: 0.027700, acc:, 98.44] [G loss: 8.541416]\n",
            "********* 1371 [D loss: 0.007270, acc:, 100.00] [G loss: 8.667673]\n",
            "********* 1372 [D loss: 0.040025, acc:, 98.44] [G loss: 8.767031]\n",
            "********* 1373 [D loss: 0.038465, acc:, 98.44] [G loss: 8.780506]\n",
            "********* 1374 [D loss: 0.006230, acc:, 100.00] [G loss: 8.517292]\n",
            "********* 1375 [D loss: 0.015081, acc:, 99.22] [G loss: 8.313192]\n",
            "********* 1376 [D loss: 0.010890, acc:, 100.00] [G loss: 8.223371]\n",
            "********* 1377 [D loss: 0.008241, acc:, 99.22] [G loss: 8.110029]\n",
            "********* 1378 [D loss: 0.015823, acc:, 99.22] [G loss: 8.395949]\n",
            "********* 1379 [D loss: 0.017926, acc:, 100.00] [G loss: 9.190336]\n",
            "********* 1380 [D loss: 0.012931, acc:, 99.22] [G loss: 9.627734]\n",
            "********* 1381 [D loss: 0.040228, acc:, 99.22] [G loss: 7.880441]\n",
            "********* 1382 [D loss: 0.025439, acc:, 99.22] [G loss: 6.874090]\n",
            "********* 1383 [D loss: 0.017658, acc:, 100.00] [G loss: 7.228084]\n",
            "********* 1384 [D loss: 0.005556, acc:, 100.00] [G loss: 7.675669]\n",
            "********* 1385 [D loss: 0.017104, acc:, 99.22] [G loss: 7.381556]\n",
            "********* 1386 [D loss: 0.008908, acc:, 100.00] [G loss: 6.866709]\n",
            "********* 1387 [D loss: 0.014497, acc:, 100.00] [G loss: 7.272879]\n",
            "********* 1388 [D loss: 0.046490, acc:, 98.44] [G loss: 6.987382]\n",
            "********* 1389 [D loss: 0.020547, acc:, 99.22] [G loss: 8.812767]\n",
            "********* 1390 [D loss: 0.038501, acc:, 99.22] [G loss: 8.040977]\n",
            "********* 1391 [D loss: 0.080476, acc:, 98.44] [G loss: 5.498444]\n",
            "********* 1392 [D loss: 0.037621, acc:, 99.22] [G loss: 7.329183]\n",
            "********* 1393 [D loss: 0.002940, acc:, 100.00] [G loss: 9.732952]\n",
            "********* 1394 [D loss: 0.050504, acc:, 97.66] [G loss: 8.196963]\n",
            "********* 1395 [D loss: 0.009494, acc:, 100.00] [G loss: 5.973516]\n",
            "********* 1396 [D loss: 0.009109, acc:, 100.00] [G loss: 5.275877]\n",
            "********* 1397 [D loss: 0.011700, acc:, 100.00] [G loss: 6.259171]\n",
            "********* 1398 [D loss: 0.026314, acc:, 99.22] [G loss: 8.348792]\n",
            "********* 1399 [D loss: 0.010296, acc:, 100.00] [G loss: 8.914097]\n",
            "********* 1400 [D loss: 0.010612, acc:, 100.00] [G loss: 8.537456]\n",
            "********* 1401 [D loss: 0.010557, acc:, 99.22] [G loss: 6.918294]\n",
            "********* 1402 [D loss: 0.010626, acc:, 100.00] [G loss: 7.117332]\n",
            "********* 1403 [D loss: 0.022363, acc:, 99.22] [G loss: 5.596041]\n",
            "********* 1404 [D loss: 0.028054, acc:, 98.44] [G loss: 6.453976]\n",
            "********* 1405 [D loss: 0.007141, acc:, 100.00] [G loss: 8.200024]\n",
            "********* 1406 [D loss: 0.009157, acc:, 100.00] [G loss: 8.066625]\n",
            "********* 1407 [D loss: 0.008661, acc:, 100.00] [G loss: 8.375813]\n",
            "********* 1408 [D loss: 0.027701, acc:, 98.44] [G loss: 5.858669]\n",
            "********* 1409 [D loss: 0.056050, acc:, 97.66] [G loss: 4.829014]\n",
            "********* 1410 [D loss: 0.015627, acc:, 100.00] [G loss: 7.753945]\n",
            "********* 1411 [D loss: 0.002493, acc:, 100.00] [G loss: 10.424811]\n",
            "********* 1412 [D loss: 0.086681, acc:, 96.88] [G loss: 5.726336]\n",
            "********* 1413 [D loss: 0.101078, acc:, 96.09] [G loss: 7.701643]\n",
            "********* 1414 [D loss: 0.002909, acc:, 100.00] [G loss: 17.688538]\n",
            "********* 1415 [D loss: 0.209636, acc:, 94.53] [G loss: 8.198593]\n",
            "********* 1416 [D loss: 0.093318, acc:, 96.88] [G loss: 5.880862]\n",
            "********* 1417 [D loss: 0.089485, acc:, 96.09] [G loss: 9.970364]\n",
            "********* 1418 [D loss: 0.003825, acc:, 100.00] [G loss: 17.611042]\n",
            "********* 1419 [D loss: 0.187375, acc:, 93.75] [G loss: 12.738512]\n",
            "********* 1420 [D loss: 0.023831, acc:, 99.22] [G loss: 7.594099]\n",
            "********* 1421 [D loss: 0.114875, acc:, 93.75] [G loss: 8.254021]\n",
            "********* 1422 [D loss: 0.001323, acc:, 100.00] [G loss: 14.896293]\n",
            "********* 1423 [D loss: 0.058009, acc:, 98.44] [G loss: 18.363281]\n",
            "********* 1424 [D loss: 0.148532, acc:, 94.53] [G loss: 15.589734]\n",
            "********* 1425 [D loss: 0.032921, acc:, 98.44] [G loss: 11.030663]\n",
            "********* 1426 [D loss: 0.093277, acc:, 98.44] [G loss: 7.339674]\n",
            "********* 1427 [D loss: 0.030487, acc:, 99.22] [G loss: 6.833138]\n",
            "********* 1428 [D loss: 0.023353, acc:, 99.22] [G loss: 8.547348]\n",
            "********* 1429 [D loss: 0.026613, acc:, 99.22] [G loss: 10.191968]\n",
            "********* 1430 [D loss: 0.034907, acc:, 99.22] [G loss: 10.827305]\n",
            "********* 1431 [D loss: 0.002886, acc:, 100.00] [G loss: 12.509945]\n",
            "********* 1432 [D loss: 0.040733, acc:, 98.44] [G loss: 11.617617]\n",
            "********* 1433 [D loss: 0.010082, acc:, 100.00] [G loss: 11.080354]\n",
            "********* 1434 [D loss: 0.024965, acc:, 99.22] [G loss: 11.324138]\n",
            "********* 1435 [D loss: 0.024836, acc:, 98.44] [G loss: 9.896836]\n",
            "********* 1436 [D loss: 0.014626, acc:, 100.00] [G loss: 8.725374]\n",
            "********* 1437 [D loss: 0.006095, acc:, 100.00] [G loss: 9.507423]\n",
            "********* 1438 [D loss: 0.016379, acc:, 100.00] [G loss: 10.298096]\n",
            "********* 1439 [D loss: 0.031369, acc:, 98.44] [G loss: 8.823185]\n",
            "********* 1440 [D loss: 0.018932, acc:, 99.22] [G loss: 7.921937]\n",
            "********* 1441 [D loss: 0.019071, acc:, 99.22] [G loss: 8.715779]\n",
            "********* 1442 [D loss: 0.022451, acc:, 99.22] [G loss: 9.527539]\n",
            "********* 1443 [D loss: 0.006267, acc:, 100.00] [G loss: 10.552753]\n",
            "********* 1444 [D loss: 0.011641, acc:, 100.00] [G loss: 9.910841]\n",
            "********* 1445 [D loss: 0.016404, acc:, 100.00] [G loss: 9.285749]\n",
            "********* 1446 [D loss: 0.025661, acc:, 99.22] [G loss: 8.185638]\n",
            "********* 1447 [D loss: 0.021387, acc:, 100.00] [G loss: 8.400595]\n",
            "********* 1448 [D loss: 0.007067, acc:, 100.00] [G loss: 9.706037]\n",
            "********* 1449 [D loss: 0.039525, acc:, 97.66] [G loss: 8.854313]\n",
            "********* 1450 [D loss: 0.011093, acc:, 100.00] [G loss: 7.905243]\n",
            "********* 1451 [D loss: 0.005354, acc:, 100.00] [G loss: 8.252291]\n",
            "********* 1452 [D loss: 0.039583, acc:, 99.22] [G loss: 7.686198]\n",
            "********* 1453 [D loss: 0.039117, acc:, 98.44] [G loss: 7.389856]\n",
            "********* 1454 [D loss: 0.026613, acc:, 99.22] [G loss: 9.235262]\n",
            "********* 1455 [D loss: 0.011938, acc:, 100.00] [G loss: 11.488285]\n",
            "********* 1456 [D loss: 0.114795, acc:, 98.44] [G loss: 7.752145]\n",
            "********* 1457 [D loss: 0.026511, acc:, 100.00] [G loss: 6.378536]\n",
            "********* 1458 [D loss: 0.020007, acc:, 99.22] [G loss: 7.646353]\n",
            "********* 1459 [D loss: 0.030912, acc:, 99.22] [G loss: 10.571653]\n",
            "********* 1460 [D loss: 0.009830, acc:, 99.22] [G loss: 14.383587]\n",
            "********* 1461 [D loss: 0.123544, acc:, 95.31] [G loss: 8.581635]\n",
            "********* 1462 [D loss: 0.027585, acc:, 98.44] [G loss: 5.984744]\n",
            "********* 1463 [D loss: 0.052000, acc:, 98.44] [G loss: 8.797024]\n",
            "********* 1464 [D loss: 0.041851, acc:, 98.44] [G loss: 11.545567]\n",
            "********* 1465 [D loss: 0.017762, acc:, 99.22] [G loss: 11.250795]\n",
            "********* 1466 [D loss: 0.061203, acc:, 96.88] [G loss: 6.548761]\n",
            "********* 1467 [D loss: 0.038647, acc:, 98.44] [G loss: 5.615520]\n",
            "********* 1468 [D loss: 0.024212, acc:, 99.22] [G loss: 8.714241]\n",
            "********* 1469 [D loss: 0.000769, acc:, 100.00] [G loss: 12.942822]\n",
            "********* 1470 [D loss: 0.050808, acc:, 98.44] [G loss: 12.669524]\n",
            "********* 1471 [D loss: 0.044495, acc:, 97.66] [G loss: 9.014524]\n",
            "********* 1472 [D loss: 0.032111, acc:, 99.22] [G loss: 7.075559]\n",
            "********* 1473 [D loss: 0.012695, acc:, 99.22] [G loss: 9.903439]\n",
            "********* 1474 [D loss: 0.016595, acc:, 98.44] [G loss: 10.526308]\n",
            "********* 1475 [D loss: 0.001130, acc:, 100.00] [G loss: 11.975225]\n",
            "********* 1476 [D loss: 0.014145, acc:, 99.22] [G loss: 12.769590]\n",
            "********* 1477 [D loss: 0.022158, acc:, 99.22] [G loss: 10.851772]\n",
            "********* 1478 [D loss: 0.007006, acc:, 100.00] [G loss: 7.646907]\n",
            "********* 1479 [D loss: 0.017090, acc:, 99.22] [G loss: 6.634398]\n",
            "********* 1480 [D loss: 0.026364, acc:, 100.00] [G loss: 7.084070]\n",
            "********* 1481 [D loss: 0.003948, acc:, 100.00] [G loss: 10.016882]\n",
            "********* 1482 [D loss: 0.009368, acc:, 100.00] [G loss: 11.774042]\n",
            "********* 1483 [D loss: 0.012509, acc:, 99.22] [G loss: 10.528784]\n",
            "********* 1484 [D loss: 0.019014, acc:, 99.22] [G loss: 8.962817]\n",
            "********* 1485 [D loss: 0.067615, acc:, 98.44] [G loss: 9.571622]\n",
            "********* 1486 [D loss: 0.011517, acc:, 100.00] [G loss: 12.984585]\n",
            "********* 1487 [D loss: 0.046087, acc:, 96.88] [G loss: 11.284288]\n",
            "********* 1488 [D loss: 0.054097, acc:, 98.44] [G loss: 5.877043]\n",
            "********* 1489 [D loss: 0.125078, acc:, 96.09] [G loss: 7.465774]\n",
            "********* 1490 [D loss: 0.000604, acc:, 100.00] [G loss: 14.896838]\n",
            "********* 1491 [D loss: 0.053793, acc:, 99.22] [G loss: 16.937847]\n",
            "********* 1492 [D loss: 0.193630, acc:, 92.19] [G loss: 8.363673]\n",
            "********* 1493 [D loss: 0.285287, acc:, 91.41] [G loss: 6.208095]\n",
            "********* 1494 [D loss: 0.119931, acc:, 95.31] [G loss: 12.040867]\n",
            "********* 1495 [D loss: 0.061575, acc:, 98.44] [G loss: 18.580181]\n",
            "********* 1496 [D loss: 0.204989, acc:, 92.19] [G loss: 13.410528]\n",
            "********* 1497 [D loss: 0.570677, acc:, 90.62] [G loss: 11.460402]\n",
            "********* 1498 [D loss: 0.289688, acc:, 92.97] [G loss: 14.572830]\n",
            "********* 1499 [D loss: 0.007556, acc:, 100.00] [G loss: 20.962444]\n",
            "********* 1500 [D loss: 0.045254, acc:, 96.88] [G loss: 23.787384]\n",
            "********* 1501 [D loss: 0.638471, acc:, 86.72] [G loss: 10.595160]\n",
            "********* 1502 [D loss: 0.090534, acc:, 96.09] [G loss: 8.030468]\n",
            "********* 1503 [D loss: 0.024151, acc:, 99.22] [G loss: 11.076214]\n",
            "********* 1504 [D loss: 0.061578, acc:, 98.44] [G loss: 12.433455]\n",
            "********* 1505 [D loss: 0.249558, acc:, 93.75] [G loss: 13.668612]\n",
            "********* 1506 [D loss: 0.063713, acc:, 98.44] [G loss: 13.916853]\n",
            "********* 1507 [D loss: 0.045638, acc:, 99.22] [G loss: 13.125291]\n",
            "********* 1508 [D loss: 0.015705, acc:, 99.22] [G loss: 12.258451]\n",
            "********* 1509 [D loss: 0.053721, acc:, 98.44] [G loss: 11.763628]\n",
            "********* 1510 [D loss: 0.040373, acc:, 97.66] [G loss: 12.530328]\n",
            "********* 1511 [D loss: 0.035990, acc:, 98.44] [G loss: 12.793893]\n",
            "********* 1512 [D loss: 0.013523, acc:, 100.00] [G loss: 12.755271]\n",
            "********* 1513 [D loss: 0.029122, acc:, 98.44] [G loss: 11.926779]\n",
            "********* 1514 [D loss: 0.047513, acc:, 99.22] [G loss: 11.434826]\n",
            "********* 1515 [D loss: 0.017840, acc:, 99.22] [G loss: 11.249409]\n",
            "********* 1516 [D loss: 0.005855, acc:, 100.00] [G loss: 9.981651]\n",
            "********* 1517 [D loss: 0.061370, acc:, 97.66] [G loss: 8.768024]\n",
            "********* 1518 [D loss: 0.020139, acc:, 99.22] [G loss: 10.148682]\n",
            "********* 1519 [D loss: 0.046381, acc:, 99.22] [G loss: 10.680712]\n",
            "********* 1520 [D loss: 0.012213, acc:, 100.00] [G loss: 9.948015]\n",
            "********* 1521 [D loss: 0.091571, acc:, 97.66] [G loss: 7.469309]\n",
            "********* 1522 [D loss: 0.028246, acc:, 98.44] [G loss: 6.147062]\n",
            "********* 1523 [D loss: 0.039005, acc:, 98.44] [G loss: 7.670908]\n",
            "********* 1524 [D loss: 0.010088, acc:, 100.00] [G loss: 9.995501]\n",
            "********* 1525 [D loss: 0.014370, acc:, 99.22] [G loss: 11.766088]\n",
            "********* 1526 [D loss: 0.038833, acc:, 99.22] [G loss: 9.469522]\n",
            "********* 1527 [D loss: 0.070156, acc:, 98.44] [G loss: 6.706873]\n",
            "********* 1528 [D loss: 0.101716, acc:, 94.53] [G loss: 6.692889]\n",
            "********* 1529 [D loss: 0.006871, acc:, 100.00] [G loss: 9.742321]\n",
            "********* 1530 [D loss: 0.046212, acc:, 98.44] [G loss: 9.949570]\n",
            "********* 1531 [D loss: 0.020314, acc:, 99.22] [G loss: 7.207091]\n",
            "********* 1532 [D loss: 0.024835, acc:, 100.00] [G loss: 6.617415]\n",
            "********* 1533 [D loss: 0.063148, acc:, 98.44] [G loss: 7.726849]\n",
            "********* 1534 [D loss: 0.015328, acc:, 99.22] [G loss: 10.940323]\n",
            "********* 1535 [D loss: 0.073452, acc:, 96.88] [G loss: 11.136745]\n",
            "********* 1536 [D loss: 0.064245, acc:, 96.09] [G loss: 6.447246]\n",
            "********* 1537 [D loss: 0.128437, acc:, 92.97] [G loss: 8.095612]\n",
            "********* 1538 [D loss: 0.006224, acc:, 100.00] [G loss: 13.040739]\n",
            "********* 1539 [D loss: 0.051362, acc:, 99.22] [G loss: 13.678991]\n",
            "********* 1540 [D loss: 0.180516, acc:, 92.97] [G loss: 5.157006]\n",
            "********* 1541 [D loss: 0.452610, acc:, 78.12] [G loss: 8.790444]\n",
            "********* 1542 [D loss: 0.045807, acc:, 99.22] [G loss: 24.593037]\n",
            "********* 1543 [D loss: 1.164510, acc:, 85.16] [G loss: 12.257257]\n",
            "********* 1544 [D loss: 0.125836, acc:, 96.88] [G loss: 5.359334]\n",
            "********* 1545 [D loss: 0.165775, acc:, 93.75] [G loss: 5.521601]\n",
            "********* 1546 [D loss: 0.207393, acc:, 96.09] [G loss: 9.760785]\n",
            "********* 1547 [D loss: 0.104054, acc:, 96.09] [G loss: 11.659225]\n",
            "********* 1548 [D loss: 0.041695, acc:, 99.22] [G loss: 13.553429]\n",
            "********* 1549 [D loss: 0.087174, acc:, 97.66] [G loss: 11.804462]\n",
            "********* 1550 [D loss: 0.028696, acc:, 99.22] [G loss: 10.129309]\n",
            "********* 1551 [D loss: 0.007288, acc:, 100.00] [G loss: 9.263177]\n",
            "********* 1552 [D loss: 0.015505, acc:, 100.00] [G loss: 7.642693]\n",
            "********* 1553 [D loss: 0.036248, acc:, 99.22] [G loss: 5.885251]\n",
            "********* 1554 [D loss: 0.019532, acc:, 100.00] [G loss: 7.448806]\n",
            "********* 1555 [D loss: 0.010393, acc:, 100.00] [G loss: 7.800734]\n",
            "********* 1556 [D loss: 0.016048, acc:, 99.22] [G loss: 8.942234]\n",
            "********* 1557 [D loss: 0.035590, acc:, 99.22] [G loss: 9.476327]\n",
            "********* 1558 [D loss: 0.088735, acc:, 99.22] [G loss: 8.843803]\n",
            "********* 1559 [D loss: 0.019653, acc:, 99.22] [G loss: 7.053673]\n",
            "********* 1560 [D loss: 0.038715, acc:, 100.00] [G loss: 7.042189]\n",
            "********* 1561 [D loss: 0.016833, acc:, 99.22] [G loss: 7.906884]\n",
            "********* 1562 [D loss: 0.012280, acc:, 100.00] [G loss: 8.495806]\n",
            "********* 1563 [D loss: 0.026900, acc:, 98.44] [G loss: 7.505171]\n",
            "********* 1564 [D loss: 0.033202, acc:, 99.22] [G loss: 6.790393]\n",
            "********* 1565 [D loss: 0.022314, acc:, 99.22] [G loss: 6.876674]\n",
            "********* 1566 [D loss: 0.023407, acc:, 99.22] [G loss: 7.247485]\n",
            "********* 1567 [D loss: 0.006214, acc:, 100.00] [G loss: 7.494928]\n",
            "********* 1568 [D loss: 0.009774, acc:, 100.00] [G loss: 8.029102]\n",
            "********* 1569 [D loss: 0.018379, acc:, 100.00] [G loss: 7.359784]\n",
            "********* 1570 [D loss: 0.024835, acc:, 98.44] [G loss: 6.507732]\n",
            "********* 1571 [D loss: 0.039057, acc:, 98.44] [G loss: 5.602335]\n",
            "********* 1572 [D loss: 0.015394, acc:, 100.00] [G loss: 5.911444]\n",
            "********* 1573 [D loss: 0.017202, acc:, 100.00] [G loss: 6.252646]\n",
            "********* 1574 [D loss: 0.025537, acc:, 99.22] [G loss: 7.191276]\n",
            "********* 1575 [D loss: 0.014946, acc:, 100.00] [G loss: 8.433426]\n",
            "********* 1576 [D loss: 0.079897, acc:, 96.88] [G loss: 6.590468]\n",
            "********* 1577 [D loss: 0.033911, acc:, 99.22] [G loss: 5.050009]\n",
            "********* 1578 [D loss: 0.060924, acc:, 97.66] [G loss: 5.596438]\n",
            "********* 1579 [D loss: 0.016252, acc:, 100.00] [G loss: 7.421666]\n",
            "********* 1580 [D loss: 0.022489, acc:, 98.44] [G loss: 7.883604]\n",
            "********* 1581 [D loss: 0.035157, acc:, 99.22] [G loss: 7.113744]\n",
            "********* 1582 [D loss: 0.021839, acc:, 100.00] [G loss: 6.662389]\n",
            "********* 1583 [D loss: 0.042022, acc:, 97.66] [G loss: 6.039645]\n",
            "********* 1584 [D loss: 0.012735, acc:, 100.00] [G loss: 6.711825]\n",
            "********* 1585 [D loss: 0.012600, acc:, 100.00] [G loss: 7.601695]\n",
            "********* 1586 [D loss: 0.080711, acc:, 98.44] [G loss: 6.517639]\n",
            "********* 1587 [D loss: 0.049886, acc:, 99.22] [G loss: 5.766293]\n",
            "********* 1588 [D loss: 0.010797, acc:, 100.00] [G loss: 6.465686]\n",
            "********* 1589 [D loss: 0.019482, acc:, 100.00] [G loss: 6.801768]\n",
            "********* 1590 [D loss: 0.019131, acc:, 100.00] [G loss: 6.576910]\n",
            "********* 1591 [D loss: 0.021623, acc:, 99.22] [G loss: 6.725688]\n",
            "********* 1592 [D loss: 0.021903, acc:, 99.22] [G loss: 6.552824]\n",
            "********* 1593 [D loss: 0.023499, acc:, 99.22] [G loss: 6.694268]\n",
            "********* 1594 [D loss: 0.030954, acc:, 99.22] [G loss: 6.764049]\n",
            "********* 1595 [D loss: 0.018862, acc:, 99.22] [G loss: 7.766326]\n",
            "********* 1596 [D loss: 0.015241, acc:, 100.00] [G loss: 7.813755]\n",
            "********* 1597 [D loss: 0.046613, acc:, 99.22] [G loss: 6.638763]\n",
            "********* 1598 [D loss: 0.029107, acc:, 99.22] [G loss: 6.049449]\n",
            "********* 1599 [D loss: 0.087932, acc:, 96.88] [G loss: 5.295660]\n",
            "********* 1600 [D loss: 0.010461, acc:, 100.00] [G loss: 7.514832]\n",
            "********* 1601 [D loss: 0.024448, acc:, 99.22] [G loss: 8.636105]\n",
            "********* 1602 [D loss: 0.016725, acc:, 100.00] [G loss: 8.766165]\n",
            "********* 1603 [D loss: 0.023782, acc:, 100.00] [G loss: 6.889181]\n",
            "********* 1604 [D loss: 0.042566, acc:, 97.66] [G loss: 6.715819]\n",
            "********* 1605 [D loss: 0.059872, acc:, 98.44] [G loss: 7.386511]\n",
            "********* 1606 [D loss: 0.016056, acc:, 100.00] [G loss: 8.423908]\n",
            "********* 1607 [D loss: 0.017461, acc:, 99.22] [G loss: 7.262490]\n",
            "********* 1608 [D loss: 0.023671, acc:, 98.44] [G loss: 7.252446]\n",
            "********* 1609 [D loss: 0.072729, acc:, 99.22] [G loss: 6.433381]\n",
            "********* 1610 [D loss: 0.029529, acc:, 99.22] [G loss: 7.440893]\n",
            "********* 1611 [D loss: 0.004083, acc:, 100.00] [G loss: 9.903390]\n",
            "********* 1612 [D loss: 0.046471, acc:, 99.22] [G loss: 8.232544]\n",
            "********* 1613 [D loss: 0.078481, acc:, 96.09] [G loss: 5.540686]\n",
            "********* 1614 [D loss: 0.031162, acc:, 98.44] [G loss: 5.944629]\n",
            "********* 1615 [D loss: 0.004613, acc:, 100.00] [G loss: 8.174713]\n",
            "********* 1616 [D loss: 0.005428, acc:, 100.00] [G loss: 10.450867]\n",
            "********* 1617 [D loss: 0.058968, acc:, 98.44] [G loss: 8.467323]\n",
            "********* 1618 [D loss: 0.052885, acc:, 97.66] [G loss: 6.369794]\n",
            "********* 1619 [D loss: 0.038074, acc:, 100.00] [G loss: 6.956673]\n",
            "********* 1620 [D loss: 0.008665, acc:, 100.00] [G loss: 9.145073]\n",
            "********* 1621 [D loss: 0.142493, acc:, 96.88] [G loss: 8.120382]\n",
            "********* 1622 [D loss: 0.014308, acc:, 99.22] [G loss: 7.118766]\n",
            "********* 1623 [D loss: 0.010721, acc:, 100.00] [G loss: 6.666407]\n",
            "********* 1624 [D loss: 0.010362, acc:, 99.22] [G loss: 7.361244]\n",
            "********* 1625 [D loss: 0.084428, acc:, 96.88] [G loss: 9.417847]\n",
            "********* 1626 [D loss: 0.008025, acc:, 100.00] [G loss: 12.148219]\n",
            "********* 1627 [D loss: 0.059692, acc:, 96.09] [G loss: 8.812654]\n",
            "********* 1628 [D loss: 0.090728, acc:, 96.09] [G loss: 8.935832]\n",
            "********* 1629 [D loss: 0.028176, acc:, 99.22] [G loss: 10.374292]\n",
            "********* 1630 [D loss: 0.058424, acc:, 97.66] [G loss: 10.655476]\n",
            "********* 1631 [D loss: 0.015288, acc:, 99.22] [G loss: 11.363927]\n",
            "********* 1632 [D loss: 0.006942, acc:, 100.00] [G loss: 10.437984]\n",
            "********* 1633 [D loss: 0.033080, acc:, 98.44] [G loss: 8.316620]\n",
            "********* 1634 [D loss: 0.046226, acc:, 98.44] [G loss: 8.915909]\n",
            "********* 1635 [D loss: 0.009428, acc:, 100.00] [G loss: 11.100835]\n",
            "********* 1636 [D loss: 0.021145, acc:, 100.00] [G loss: 10.711685]\n",
            "********* 1637 [D loss: 0.033424, acc:, 98.44] [G loss: 7.832700]\n",
            "********* 1638 [D loss: 0.009628, acc:, 100.00] [G loss: 6.558638]\n",
            "********* 1639 [D loss: 0.026132, acc:, 99.22] [G loss: 7.205918]\n",
            "********* 1640 [D loss: 0.008603, acc:, 100.00] [G loss: 9.481026]\n",
            "********* 1641 [D loss: 0.043589, acc:, 98.44] [G loss: 8.414861]\n",
            "********* 1642 [D loss: 0.017857, acc:, 99.22] [G loss: 7.377606]\n",
            "********* 1643 [D loss: 0.012129, acc:, 100.00] [G loss: 8.117672]\n",
            "********* 1644 [D loss: 0.026920, acc:, 99.22] [G loss: 8.776389]\n",
            "********* 1645 [D loss: 0.006363, acc:, 100.00] [G loss: 10.018070]\n",
            "********* 1646 [D loss: 0.002318, acc:, 100.00] [G loss: 11.080997]\n",
            "********* 1647 [D loss: 0.017555, acc:, 99.22] [G loss: 9.943260]\n",
            "********* 1648 [D loss: 0.013937, acc:, 100.00] [G loss: 8.985911]\n",
            "********* 1649 [D loss: 0.038944, acc:, 98.44] [G loss: 9.960426]\n",
            "********* 1650 [D loss: 0.010615, acc:, 100.00] [G loss: 11.252602]\n",
            "********* 1651 [D loss: 0.019451, acc:, 99.22] [G loss: 10.629187]\n",
            "********* 1652 [D loss: 0.009524, acc:, 100.00] [G loss: 10.274250]\n",
            "********* 1653 [D loss: 0.040999, acc:, 98.44] [G loss: 7.903506]\n",
            "********* 1654 [D loss: 0.012872, acc:, 100.00] [G loss: 9.382301]\n",
            "********* 1655 [D loss: 0.007206, acc:, 100.00] [G loss: 9.057793]\n",
            "********* 1656 [D loss: 0.013218, acc:, 99.22] [G loss: 7.832433]\n",
            "********* 1657 [D loss: 0.052392, acc:, 98.44] [G loss: 6.473486]\n",
            "********* 1658 [D loss: 0.035654, acc:, 98.44] [G loss: 8.658276]\n",
            "********* 1659 [D loss: 0.005470, acc:, 100.00] [G loss: 9.620764]\n",
            "********* 1660 [D loss: 0.032321, acc:, 99.22] [G loss: 9.309217]\n",
            "********* 1661 [D loss: 0.010902, acc:, 100.00] [G loss: 8.925273]\n",
            "********* 1662 [D loss: 0.010484, acc:, 100.00] [G loss: 9.191358]\n",
            "********* 1663 [D loss: 0.024056, acc:, 99.22] [G loss: 7.864642]\n",
            "********* 1664 [D loss: 0.013123, acc:, 100.00] [G loss: 7.325871]\n",
            "********* 1665 [D loss: 0.008951, acc:, 100.00] [G loss: 7.208685]\n",
            "********* 1666 [D loss: 0.013815, acc:, 100.00] [G loss: 8.111717]\n",
            "********* 1667 [D loss: 0.113866, acc:, 94.53] [G loss: 6.127311]\n",
            "********* 1668 [D loss: 0.018623, acc:, 100.00] [G loss: 8.284999]\n",
            "********* 1669 [D loss: 0.038876, acc:, 98.44] [G loss: 8.290161]\n",
            "********* 1670 [D loss: 0.040193, acc:, 99.22] [G loss: 6.499632]\n",
            "********* 1671 [D loss: 0.021290, acc:, 100.00] [G loss: 7.264387]\n",
            "********* 1672 [D loss: 0.009664, acc:, 100.00] [G loss: 8.837714]\n",
            "********* 1673 [D loss: 0.014957, acc:, 100.00] [G loss: 11.231998]\n",
            "********* 1674 [D loss: 0.098932, acc:, 97.66] [G loss: 6.906339]\n",
            "********* 1675 [D loss: 0.026320, acc:, 100.00] [G loss: 6.003399]\n",
            "********* 1676 [D loss: 0.010819, acc:, 100.00] [G loss: 8.096758]\n",
            "********* 1677 [D loss: 0.007357, acc:, 100.00] [G loss: 8.584254]\n",
            "********* 1678 [D loss: 0.005713, acc:, 100.00] [G loss: 10.613429]\n",
            "********* 1679 [D loss: 0.015248, acc:, 99.22] [G loss: 9.608414]\n",
            "********* 1680 [D loss: 0.018297, acc:, 99.22] [G loss: 8.461192]\n",
            "********* 1681 [D loss: 0.004744, acc:, 100.00] [G loss: 6.992785]\n",
            "********* 1682 [D loss: 0.017580, acc:, 99.22] [G loss: 7.355355]\n",
            "********* 1683 [D loss: 0.089250, acc:, 98.44] [G loss: 7.799947]\n",
            "********* 1684 [D loss: 0.006266, acc:, 100.00] [G loss: 8.833157]\n",
            "********* 1685 [D loss: 0.005756, acc:, 100.00] [G loss: 10.388088]\n",
            "********* 1686 [D loss: 0.036447, acc:, 98.44] [G loss: 7.920970]\n",
            "********* 1687 [D loss: 0.052871, acc:, 99.22] [G loss: 5.852403]\n",
            "********* 1688 [D loss: 0.039614, acc:, 99.22] [G loss: 8.432864]\n",
            "********* 1689 [D loss: 0.006610, acc:, 100.00] [G loss: 11.569757]\n",
            "********* 1690 [D loss: 0.040582, acc:, 98.44] [G loss: 11.142390]\n",
            "********* 1691 [D loss: 0.020301, acc:, 99.22] [G loss: 8.684330]\n",
            "********* 1692 [D loss: 0.042842, acc:, 99.22] [G loss: 6.951569]\n",
            "********* 1693 [D loss: 0.015684, acc:, 100.00] [G loss: 8.315463]\n",
            "********* 1694 [D loss: 0.008528, acc:, 100.00] [G loss: 10.495996]\n",
            "********* 1695 [D loss: 0.033994, acc:, 99.22] [G loss: 9.082403]\n",
            "********* 1696 [D loss: 0.021153, acc:, 98.44] [G loss: 8.828500]\n",
            "********* 1697 [D loss: 0.007802, acc:, 100.00] [G loss: 8.114697]\n",
            "********* 1698 [D loss: 0.006355, acc:, 100.00] [G loss: 9.401123]\n",
            "********* 1699 [D loss: 0.007346, acc:, 100.00] [G loss: 9.523184]\n",
            "********* 1700 [D loss: 0.086857, acc:, 96.88] [G loss: 5.499687]\n",
            "********* 1701 [D loss: 0.070501, acc:, 96.09] [G loss: 7.894519]\n",
            "********* 1702 [D loss: 0.001994, acc:, 100.00] [G loss: 14.206538]\n",
            "********* 1703 [D loss: 0.048684, acc:, 98.44] [G loss: 13.586143]\n",
            "********* 1704 [D loss: 0.075451, acc:, 97.66] [G loss: 4.856813]\n",
            "********* 1705 [D loss: 0.312996, acc:, 84.38] [G loss: 16.264460]\n",
            "********* 1706 [D loss: 0.080105, acc:, 97.66] [G loss: 37.301254]\n",
            "********* 1707 [D loss: 0.948637, acc:, 82.81] [G loss: 14.594884]\n",
            "********* 1708 [D loss: 0.289744, acc:, 91.41] [G loss: 6.662138]\n",
            "********* 1709 [D loss: 0.350533, acc:, 89.84] [G loss: 10.247241]\n",
            "********* 1710 [D loss: 0.031670, acc:, 99.22] [G loss: 17.739882]\n",
            "********* 1711 [D loss: 0.089194, acc:, 98.44] [G loss: 22.021931]\n",
            "********* 1712 [D loss: 0.119170, acc:, 97.66] [G loss: 21.358341]\n",
            "********* 1713 [D loss: 0.083527, acc:, 96.88] [G loss: 15.811913]\n",
            "********* 1714 [D loss: 0.034748, acc:, 99.22] [G loss: 10.996725]\n",
            "********* 1715 [D loss: 0.032261, acc:, 99.22] [G loss: 10.112565]\n",
            "********* 1716 [D loss: 0.046162, acc:, 97.66] [G loss: 9.860331]\n",
            "********* 1717 [D loss: 0.008562, acc:, 100.00] [G loss: 11.662736]\n",
            "********* 1718 [D loss: 0.006536, acc:, 100.00] [G loss: 13.326191]\n",
            "********* 1719 [D loss: 0.150324, acc:, 98.44] [G loss: 14.684063]\n",
            "********* 1720 [D loss: 0.033771, acc:, 99.22] [G loss: 15.343102]\n",
            "********* 1721 [D loss: 0.006724, acc:, 100.00] [G loss: 14.563013]\n",
            "********* 1722 [D loss: 0.024968, acc:, 99.22] [G loss: 12.467119]\n",
            "********* 1723 [D loss: 0.004180, acc:, 100.00] [G loss: 12.472733]\n",
            "********* 1724 [D loss: 0.009393, acc:, 99.22] [G loss: 10.237640]\n",
            "********* 1725 [D loss: 0.017567, acc:, 99.22] [G loss: 8.213932]\n",
            "********* 1726 [D loss: 0.028013, acc:, 99.22] [G loss: 7.568974]\n",
            "********* 1727 [D loss: 0.010748, acc:, 100.00] [G loss: 10.179192]\n",
            "********* 1728 [D loss: 0.034060, acc:, 98.44] [G loss: 9.737214]\n",
            "********* 1729 [D loss: 0.021991, acc:, 99.22] [G loss: 9.260843]\n",
            "********* 1730 [D loss: 0.010022, acc:, 100.00] [G loss: 9.113597]\n",
            "********* 1731 [D loss: 0.038306, acc:, 97.66] [G loss: 8.657323]\n",
            "********* 1732 [D loss: 0.012101, acc:, 99.22] [G loss: 9.228256]\n",
            "********* 1733 [D loss: 0.043956, acc:, 97.66] [G loss: 7.743462]\n",
            "********* 1734 [D loss: 0.025465, acc:, 99.22] [G loss: 6.656434]\n",
            "********* 1735 [D loss: 0.029435, acc:, 99.22] [G loss: 6.777652]\n",
            "********* 1736 [D loss: 0.004273, acc:, 100.00] [G loss: 8.467812]\n",
            "********* 1737 [D loss: 0.006529, acc:, 100.00] [G loss: 9.690561]\n",
            "********* 1738 [D loss: 0.010019, acc:, 100.00] [G loss: 8.901838]\n",
            "********* 1739 [D loss: 0.007036, acc:, 100.00] [G loss: 8.501224]\n",
            "********* 1740 [D loss: 0.028394, acc:, 98.44] [G loss: 8.290627]\n",
            "********* 1741 [D loss: 0.017386, acc:, 99.22] [G loss: 7.468492]\n",
            "********* 1742 [D loss: 0.021184, acc:, 100.00] [G loss: 6.820982]\n",
            "********* 1743 [D loss: 0.009097, acc:, 100.00] [G loss: 7.496735]\n",
            "********* 1744 [D loss: 0.029461, acc:, 100.00] [G loss: 7.757797]\n",
            "********* 1745 [D loss: 0.036641, acc:, 98.44] [G loss: 6.874257]\n",
            "********* 1746 [D loss: 0.012370, acc:, 100.00] [G loss: 7.076119]\n",
            "********* 1747 [D loss: 0.012079, acc:, 100.00] [G loss: 7.998945]\n",
            "********* 1748 [D loss: 0.031897, acc:, 98.44] [G loss: 7.426498]\n",
            "********* 1749 [D loss: 0.082360, acc:, 96.88] [G loss: 6.195662]\n",
            "********* 1750 [D loss: 0.015265, acc:, 100.00] [G loss: 7.541045]\n",
            "********* 1751 [D loss: 0.077300, acc:, 97.66] [G loss: 6.181405]\n",
            "********* 1752 [D loss: 0.033757, acc:, 100.00] [G loss: 7.033137]\n",
            "********* 1753 [D loss: 0.013601, acc:, 100.00] [G loss: 9.770749]\n",
            "********* 1754 [D loss: 0.039571, acc:, 98.44] [G loss: 9.173577]\n",
            "********* 1755 [D loss: 0.050988, acc:, 97.66] [G loss: 6.239464]\n",
            "********* 1756 [D loss: 0.043688, acc:, 98.44] [G loss: 5.904732]\n",
            "********* 1757 [D loss: 0.026487, acc:, 99.22] [G loss: 8.271625]\n",
            "********* 1758 [D loss: 0.038128, acc:, 99.22] [G loss: 10.180839]\n",
            "********* 1759 [D loss: 0.033969, acc:, 99.22] [G loss: 8.247894]\n",
            "********* 1760 [D loss: 0.062001, acc:, 99.22] [G loss: 6.992214]\n",
            "********* 1761 [D loss: 0.016388, acc:, 100.00] [G loss: 7.918905]\n",
            "********* 1762 [D loss: 0.052654, acc:, 98.44] [G loss: 9.544863]\n",
            "********* 1763 [D loss: 0.008023, acc:, 100.00] [G loss: 12.640780]\n",
            "********* 1764 [D loss: 0.056772, acc:, 98.44] [G loss: 11.282616]\n",
            "********* 1765 [D loss: 0.013423, acc:, 99.22] [G loss: 8.834377]\n",
            "********* 1766 [D loss: 0.021284, acc:, 99.22] [G loss: 7.561366]\n",
            "********* 1767 [D loss: 0.015083, acc:, 100.00] [G loss: 7.591592]\n",
            "********* 1768 [D loss: 0.008665, acc:, 99.22] [G loss: 10.245508]\n",
            "********* 1769 [D loss: 0.032105, acc:, 97.66] [G loss: 10.481545]\n",
            "********* 1770 [D loss: 0.025032, acc:, 99.22] [G loss: 8.388645]\n",
            "********* 1771 [D loss: 0.033277, acc:, 98.44] [G loss: 8.453997]\n",
            "********* 1772 [D loss: 0.042468, acc:, 99.22] [G loss: 9.170900]\n",
            "********* 1773 [D loss: 0.070653, acc:, 98.44] [G loss: 8.205628]\n",
            "********* 1774 [D loss: 0.037278, acc:, 99.22] [G loss: 7.640697]\n",
            "********* 1775 [D loss: 0.005901, acc:, 100.00] [G loss: 9.090126]\n",
            "********* 1776 [D loss: 0.027605, acc:, 99.22] [G loss: 11.275488]\n",
            "********* 1777 [D loss: 0.038402, acc:, 97.66] [G loss: 10.489797]\n",
            "********* 1778 [D loss: 0.019153, acc:, 100.00] [G loss: 8.164946]\n",
            "********* 1779 [D loss: 0.139203, acc:, 93.75] [G loss: 8.526065]\n",
            "********* 1780 [D loss: 0.007498, acc:, 100.00] [G loss: 14.340803]\n",
            "********* 1781 [D loss: 0.084946, acc:, 96.09] [G loss: 9.092407]\n",
            "********* 1782 [D loss: 0.128869, acc:, 94.53] [G loss: 6.231767]\n",
            "********* 1783 [D loss: 0.004187, acc:, 100.00] [G loss: 8.795984]\n",
            "********* 1784 [D loss: 0.027320, acc:, 98.44] [G loss: 11.405519]\n",
            "********* 1785 [D loss: 0.007342, acc:, 100.00] [G loss: 13.493318]\n",
            "********* 1786 [D loss: 0.088823, acc:, 97.66] [G loss: 7.972808]\n",
            "********* 1787 [D loss: 0.026741, acc:, 100.00] [G loss: 5.596971]\n",
            "********* 1788 [D loss: 0.033491, acc:, 100.00] [G loss: 7.555058]\n",
            "********* 1789 [D loss: 0.044976, acc:, 99.22] [G loss: 11.938761]\n",
            "********* 1790 [D loss: 0.009491, acc:, 100.00] [G loss: 13.783773]\n",
            "********* 1791 [D loss: 0.093608, acc:, 95.31] [G loss: 7.989621]\n",
            "********* 1792 [D loss: 0.032499, acc:, 99.22] [G loss: 5.988881]\n",
            "********* 1793 [D loss: 0.051941, acc:, 99.22] [G loss: 9.126719]\n",
            "********* 1794 [D loss: 0.012124, acc:, 100.00] [G loss: 16.309591]\n",
            "********* 1795 [D loss: 0.118306, acc:, 94.53] [G loss: 10.285629]\n",
            "********* 1796 [D loss: 0.027461, acc:, 99.22] [G loss: 7.782444]\n",
            "********* 1797 [D loss: 0.077646, acc:, 96.09] [G loss: 10.241126]\n",
            "********* 1798 [D loss: 0.014196, acc:, 100.00] [G loss: 15.959262]\n",
            "********* 1799 [D loss: 0.022330, acc:, 99.22] [G loss: 18.892891]\n",
            "********* 1800 [D loss: 0.136396, acc:, 96.09] [G loss: 11.920599]\n",
            "********* 1801 [D loss: 0.030301, acc:, 100.00] [G loss: 7.597070]\n",
            "********* 1802 [D loss: 0.053701, acc:, 97.66] [G loss: 9.052235]\n",
            "********* 1803 [D loss: 0.001143, acc:, 100.00] [G loss: 14.170031]\n",
            "********* 1804 [D loss: 0.002159, acc:, 100.00] [G loss: 18.325138]\n",
            "********* 1805 [D loss: 0.066853, acc:, 97.66] [G loss: 19.230650]\n",
            "********* 1806 [D loss: 0.221757, acc:, 96.09] [G loss: 11.525448]\n",
            "********* 1807 [D loss: 0.017689, acc:, 99.22] [G loss: 7.288025]\n",
            "********* 1808 [D loss: 0.059340, acc:, 96.88] [G loss: 6.458618]\n",
            "********* 1809 [D loss: 0.002835, acc:, 100.00] [G loss: 11.174809]\n",
            "********* 1810 [D loss: 0.001559, acc:, 100.00] [G loss: 14.600880]\n",
            "********* 1811 [D loss: 0.004376, acc:, 100.00] [G loss: 16.608934]\n",
            "********* 1812 [D loss: 0.061026, acc:, 99.22] [G loss: 13.091946]\n",
            "********* 1813 [D loss: 0.007336, acc:, 100.00] [G loss: 10.405715]\n",
            "********* 1814 [D loss: 0.004419, acc:, 100.00] [G loss: 9.116066]\n",
            "********* 1815 [D loss: 0.026825, acc:, 99.22] [G loss: 9.314404]\n",
            "********* 1816 [D loss: 0.004347, acc:, 100.00] [G loss: 10.797092]\n",
            "********* 1817 [D loss: 0.015310, acc:, 99.22] [G loss: 11.913818]\n",
            "********* 1818 [D loss: 0.002922, acc:, 100.00] [G loss: 11.589097]\n",
            "********* 1819 [D loss: 0.001936, acc:, 100.00] [G loss: 11.015484]\n",
            "********* 1820 [D loss: 0.031745, acc:, 98.44] [G loss: 7.151192]\n",
            "********* 1821 [D loss: 0.077708, acc:, 98.44] [G loss: 8.413199]\n",
            "********* 1822 [D loss: 0.009699, acc:, 100.00] [G loss: 11.640967]\n",
            "********* 1823 [D loss: 0.004678, acc:, 100.00] [G loss: 13.741587]\n",
            "********* 1824 [D loss: 0.072945, acc:, 96.88] [G loss: 9.447001]\n",
            "********* 1825 [D loss: 0.098257, acc:, 94.53] [G loss: 8.934755]\n",
            "********* 1826 [D loss: 0.013463, acc:, 99.22] [G loss: 10.502981]\n",
            "********* 1827 [D loss: 0.008034, acc:, 100.00] [G loss: 11.380898]\n",
            "********* 1828 [D loss: 0.129789, acc:, 95.31] [G loss: 8.243809]\n",
            "********* 1829 [D loss: 0.014599, acc:, 100.00] [G loss: 7.532843]\n",
            "********* 1830 [D loss: 0.022846, acc:, 99.22] [G loss: 5.997799]\n",
            "********* 1831 [D loss: 0.010735, acc:, 100.00] [G loss: 7.405166]\n",
            "********* 1832 [D loss: 0.048218, acc:, 99.22] [G loss: 10.052856]\n",
            "********* 1833 [D loss: 0.002369, acc:, 100.00] [G loss: 11.871438]\n",
            "********* 1834 [D loss: 0.066614, acc:, 97.66] [G loss: 8.845161]\n",
            "********* 1835 [D loss: 0.012881, acc:, 100.00] [G loss: 7.360469]\n",
            "********* 1836 [D loss: 0.062489, acc:, 99.22] [G loss: 9.864881]\n",
            "********* 1837 [D loss: 0.004845, acc:, 100.00] [G loss: 12.821443]\n",
            "********* 1838 [D loss: 0.053832, acc:, 96.88] [G loss: 11.212529]\n",
            "********* 1839 [D loss: 0.038298, acc:, 98.44] [G loss: 8.608476]\n",
            "********* 1840 [D loss: 0.015297, acc:, 99.22] [G loss: 8.392332]\n",
            "********* 1841 [D loss: 0.006797, acc:, 100.00] [G loss: 9.742350]\n",
            "********* 1842 [D loss: 0.122283, acc:, 96.09] [G loss: 11.066391]\n",
            "********* 1843 [D loss: 0.006187, acc:, 100.00] [G loss: 15.510958]\n",
            "********* 1844 [D loss: 0.046748, acc:, 96.88] [G loss: 13.291056]\n",
            "********* 1845 [D loss: 0.187529, acc:, 92.19] [G loss: 7.560279]\n",
            "********* 1846 [D loss: 0.099291, acc:, 96.09] [G loss: 8.809685]\n",
            "********* 1847 [D loss: 0.054519, acc:, 97.66] [G loss: 12.333239]\n",
            "********* 1848 [D loss: 0.030683, acc:, 99.22] [G loss: 14.980444]\n",
            "********* 1849 [D loss: 0.078080, acc:, 97.66] [G loss: 10.800610]\n",
            "********* 1850 [D loss: 0.039172, acc:, 98.44] [G loss: 8.882698]\n",
            "********* 1851 [D loss: 0.043272, acc:, 97.66] [G loss: 8.956522]\n",
            "********* 1852 [D loss: 0.011740, acc:, 99.22] [G loss: 11.158730]\n",
            "********* 1853 [D loss: 0.005774, acc:, 100.00] [G loss: 12.962277]\n",
            "********* 1854 [D loss: 0.061880, acc:, 96.88] [G loss: 9.241428]\n",
            "********* 1855 [D loss: 0.010857, acc:, 100.00] [G loss: 6.001540]\n",
            "********* 1856 [D loss: 0.077712, acc:, 96.88] [G loss: 8.813711]\n",
            "********* 1857 [D loss: 0.001119, acc:, 100.00] [G loss: 14.522892]\n",
            "********* 1858 [D loss: 0.036705, acc:, 99.22] [G loss: 15.912103]\n",
            "********* 1859 [D loss: 0.160350, acc:, 96.88] [G loss: 9.526274]\n",
            "********* 1860 [D loss: 0.035696, acc:, 99.22] [G loss: 7.380451]\n",
            "********* 1861 [D loss: 0.019586, acc:, 100.00] [G loss: 9.123672]\n",
            "********* 1862 [D loss: 0.002317, acc:, 100.00] [G loss: 12.704629]\n",
            "********* 1863 [D loss: 0.001626, acc:, 100.00] [G loss: 15.261035]\n",
            "********* 1864 [D loss: 0.013221, acc:, 99.22] [G loss: 16.908804]\n",
            "********* 1865 [D loss: 0.022688, acc:, 99.22] [G loss: 13.614589]\n",
            "********* 1866 [D loss: 0.018734, acc:, 99.22] [G loss: 9.130487]\n",
            "********* 1867 [D loss: 0.026829, acc:, 99.22] [G loss: 7.869716]\n",
            "********* 1868 [D loss: 0.019421, acc:, 99.22] [G loss: 9.099506]\n",
            "********* 1869 [D loss: 0.020795, acc:, 99.22] [G loss: 9.091021]\n",
            "********* 1870 [D loss: 0.011710, acc:, 99.22] [G loss: 9.336067]\n",
            "********* 1871 [D loss: 0.002941, acc:, 100.00] [G loss: 10.313896]\n",
            "********* 1872 [D loss: 0.015140, acc:, 99.22] [G loss: 10.681357]\n",
            "********* 1873 [D loss: 0.047419, acc:, 98.44] [G loss: 7.419744]\n",
            "********* 1874 [D loss: 0.081578, acc:, 97.66] [G loss: 7.595998]\n",
            "********* 1875 [D loss: 0.035798, acc:, 98.44] [G loss: 8.749277]\n",
            "********* 1876 [D loss: 0.009532, acc:, 100.00] [G loss: 11.094522]\n",
            "********* 1877 [D loss: 0.019404, acc:, 99.22] [G loss: 11.675323]\n",
            "********* 1878 [D loss: 0.049856, acc:, 98.44] [G loss: 8.419217]\n",
            "********* 1879 [D loss: 0.015427, acc:, 100.00] [G loss: 6.301229]\n",
            "********* 1880 [D loss: 0.016631, acc:, 100.00] [G loss: 7.546794]\n",
            "********* 1881 [D loss: 0.019910, acc:, 99.22] [G loss: 8.016123]\n",
            "********* 1882 [D loss: 0.009906, acc:, 100.00] [G loss: 10.253839]\n",
            "********* 1883 [D loss: 0.026593, acc:, 99.22] [G loss: 8.617820]\n",
            "********* 1884 [D loss: 0.004831, acc:, 100.00] [G loss: 8.465284]\n",
            "********* 1885 [D loss: 0.027738, acc:, 98.44] [G loss: 7.967589]\n",
            "********* 1886 [D loss: 0.007347, acc:, 100.00] [G loss: 9.549609]\n",
            "********* 1887 [D loss: 0.018034, acc:, 99.22] [G loss: 8.228613]\n",
            "********* 1888 [D loss: 0.076115, acc:, 97.66] [G loss: 6.498561]\n",
            "********* 1889 [D loss: 0.044194, acc:, 98.44] [G loss: 10.777373]\n",
            "********* 1890 [D loss: 0.028560, acc:, 98.44] [G loss: 13.135265]\n",
            "********* 1891 [D loss: 0.038635, acc:, 98.44] [G loss: 12.452373]\n",
            "********* 1892 [D loss: 0.042795, acc:, 97.66] [G loss: 9.029086]\n",
            "********* 1893 [D loss: 0.035595, acc:, 99.22] [G loss: 9.202208]\n",
            "********* 1894 [D loss: 0.007711, acc:, 100.00] [G loss: 9.985085]\n",
            "********* 1895 [D loss: 0.006131, acc:, 100.00] [G loss: 10.777747]\n",
            "********* 1896 [D loss: 0.004219, acc:, 100.00] [G loss: 11.039255]\n",
            "********* 1897 [D loss: 0.019545, acc:, 99.22] [G loss: 12.259625]\n",
            "********* 1898 [D loss: 0.023696, acc:, 99.22] [G loss: 11.085417]\n",
            "********* 1899 [D loss: 0.014133, acc:, 100.00] [G loss: 9.615366]\n",
            "********* 1900 [D loss: 0.022023, acc:, 99.22] [G loss: 7.186456]\n",
            "********* 1901 [D loss: 0.010106, acc:, 100.00] [G loss: 7.590587]\n",
            "********* 1902 [D loss: 0.007282, acc:, 100.00] [G loss: 8.199825]\n",
            "********* 1903 [D loss: 0.007586, acc:, 100.00] [G loss: 10.631048]\n",
            "********* 1904 [D loss: 0.043828, acc:, 97.66] [G loss: 8.116409]\n",
            "********* 1905 [D loss: 0.050165, acc:, 97.66] [G loss: 7.178876]\n",
            "********* 1906 [D loss: 0.016127, acc:, 100.00] [G loss: 9.537345]\n",
            "********* 1907 [D loss: 0.018280, acc:, 99.22] [G loss: 10.178762]\n",
            "********* 1908 [D loss: 0.012569, acc:, 100.00] [G loss: 10.762566]\n",
            "********* 1909 [D loss: 0.024472, acc:, 99.22] [G loss: 7.528540]\n",
            "********* 1910 [D loss: 0.011906, acc:, 100.00] [G loss: 6.846995]\n",
            "********* 1911 [D loss: 0.023730, acc:, 99.22] [G loss: 7.224584]\n",
            "********* 1912 [D loss: 0.012104, acc:, 99.22] [G loss: 7.950511]\n",
            "********* 1913 [D loss: 0.023507, acc:, 99.22] [G loss: 7.391754]\n",
            "********* 1914 [D loss: 0.023119, acc:, 98.44] [G loss: 7.898563]\n",
            "********* 1915 [D loss: 0.005284, acc:, 100.00] [G loss: 8.075615]\n",
            "********* 1916 [D loss: 0.008913, acc:, 100.00] [G loss: 10.273754]\n",
            "********* 1917 [D loss: 0.008016, acc:, 100.00] [G loss: 11.560765]\n",
            "********* 1918 [D loss: 0.016687, acc:, 99.22] [G loss: 9.771865]\n",
            "********* 1919 [D loss: 0.033664, acc:, 98.44] [G loss: 7.413824]\n",
            "********* 1920 [D loss: 0.065373, acc:, 96.09] [G loss: 8.281331]\n",
            "********* 1921 [D loss: 0.004631, acc:, 100.00] [G loss: 12.184467]\n",
            "********* 1922 [D loss: 0.022661, acc:, 99.22] [G loss: 12.085938]\n",
            "********* 1923 [D loss: 0.123572, acc:, 96.88] [G loss: 8.530416]\n",
            "********* 1924 [D loss: 0.068130, acc:, 96.88] [G loss: 9.702538]\n",
            "********* 1925 [D loss: 0.038595, acc:, 99.22] [G loss: 16.775955]\n",
            "********* 1926 [D loss: 0.167767, acc:, 96.09] [G loss: 13.386602]\n",
            "********* 1927 [D loss: 0.087670, acc:, 97.66] [G loss: 6.894378]\n",
            "********* 1928 [D loss: 0.169308, acc:, 96.09] [G loss: 11.219318]\n",
            "********* 1929 [D loss: 0.003502, acc:, 100.00] [G loss: 23.366968]\n",
            "********* 1930 [D loss: 0.282612, acc:, 92.97] [G loss: 14.779793]\n",
            "********* 1931 [D loss: 0.020241, acc:, 99.22] [G loss: 8.642313]\n",
            "********* 1932 [D loss: 0.056825, acc:, 97.66] [G loss: 6.139158]\n",
            "********* 1933 [D loss: 0.096197, acc:, 96.88] [G loss: 9.404975]\n",
            "********* 1934 [D loss: 0.000924, acc:, 100.00] [G loss: 14.066536]\n",
            "********* 1935 [D loss: 0.000635, acc:, 100.00] [G loss: 18.399309]\n",
            "********* 1936 [D loss: 0.077053, acc:, 97.66] [G loss: 19.719788]\n",
            "********* 1937 [D loss: 0.062472, acc:, 96.88] [G loss: 13.130072]\n",
            "********* 1938 [D loss: 0.007637, acc:, 100.00] [G loss: 9.755039]\n",
            "********* 1939 [D loss: 0.010542, acc:, 100.00] [G loss: 7.009933]\n",
            "********* 1940 [D loss: 0.053281, acc:, 97.66] [G loss: 8.384542]\n",
            "********* 1941 [D loss: 0.002559, acc:, 100.00] [G loss: 11.046472]\n",
            "********* 1942 [D loss: 0.007191, acc:, 100.00] [G loss: 15.043904]\n",
            "********* 1943 [D loss: 0.014246, acc:, 99.22] [G loss: 14.538635]\n",
            "********* 1944 [D loss: 0.121119, acc:, 98.44] [G loss: 13.543395]\n",
            "********* 1945 [D loss: 0.092944, acc:, 96.88] [G loss: 12.710566]\n",
            "********* 1946 [D loss: 0.086272, acc:, 97.66] [G loss: 11.165133]\n",
            "********* 1947 [D loss: 0.002653, acc:, 100.00] [G loss: 11.876970]\n",
            "********* 1948 [D loss: 0.092134, acc:, 98.44] [G loss: 12.416331]\n",
            "********* 1949 [D loss: 0.045536, acc:, 99.22] [G loss: 12.333526]\n",
            "********* 1950 [D loss: 0.002641, acc:, 100.00] [G loss: 9.422901]\n",
            "********* 1951 [D loss: 0.031600, acc:, 98.44] [G loss: 11.118572]\n",
            "********* 1952 [D loss: 0.011076, acc:, 100.00] [G loss: 8.297552]\n",
            "********* 1953 [D loss: 0.005854, acc:, 100.00] [G loss: 10.049959]\n",
            "********* 1954 [D loss: 0.005870, acc:, 100.00] [G loss: 10.567736]\n",
            "********* 1955 [D loss: 0.001971, acc:, 100.00] [G loss: 10.445171]\n",
            "********* 1956 [D loss: 0.009792, acc:, 100.00] [G loss: 11.022162]\n",
            "********* 1957 [D loss: 0.007249, acc:, 100.00] [G loss: 11.733015]\n",
            "********* 1958 [D loss: 0.021801, acc:, 99.22] [G loss: 9.565659]\n",
            "********* 1959 [D loss: 0.039615, acc:, 98.44] [G loss: 8.274742]\n",
            "********* 1960 [D loss: 0.020209, acc:, 100.00] [G loss: 7.789059]\n",
            "********* 1961 [D loss: 0.010457, acc:, 100.00] [G loss: 7.628561]\n",
            "********* 1962 [D loss: 0.014179, acc:, 100.00] [G loss: 9.156637]\n",
            "********* 1963 [D loss: 0.018625, acc:, 99.22] [G loss: 10.247150]\n",
            "********* 1964 [D loss: 0.014602, acc:, 100.00] [G loss: 8.764317]\n",
            "********* 1965 [D loss: 0.040468, acc:, 99.22] [G loss: 7.436130]\n",
            "********* 1966 [D loss: 0.056983, acc:, 98.44] [G loss: 7.533269]\n",
            "********* 1967 [D loss: 0.007385, acc:, 100.00] [G loss: 10.274881]\n",
            "********* 1968 [D loss: 0.020174, acc:, 98.44] [G loss: 10.712185]\n",
            "********* 1969 [D loss: 0.036881, acc:, 98.44] [G loss: 8.870674]\n",
            "********* 1970 [D loss: 0.010781, acc:, 99.22] [G loss: 8.351179]\n",
            "********* 1971 [D loss: 0.050622, acc:, 98.44] [G loss: 7.049122]\n",
            "********* 1972 [D loss: 0.006491, acc:, 100.00] [G loss: 9.387608]\n",
            "********* 1973 [D loss: 0.003879, acc:, 100.00] [G loss: 11.762706]\n",
            "********* 1974 [D loss: 0.020347, acc:, 100.00] [G loss: 11.698236]\n",
            "********* 1975 [D loss: 0.005329, acc:, 100.00] [G loss: 9.771388]\n",
            "********* 1976 [D loss: 0.032620, acc:, 98.44] [G loss: 9.139040]\n",
            "********* 1977 [D loss: 0.012150, acc:, 100.00] [G loss: 7.814327]\n",
            "********* 1978 [D loss: 0.008840, acc:, 100.00] [G loss: 8.027819]\n",
            "********* 1979 [D loss: 0.004163, acc:, 100.00] [G loss: 8.883955]\n",
            "********* 1980 [D loss: 0.017373, acc:, 99.22] [G loss: 8.730268]\n",
            "********* 1981 [D loss: 0.021966, acc:, 99.22] [G loss: 9.403635]\n",
            "********* 1982 [D loss: 0.020222, acc:, 100.00] [G loss: 8.053879]\n",
            "********* 1983 [D loss: 0.007099, acc:, 100.00] [G loss: 8.717471]\n",
            "********* 1984 [D loss: 0.006724, acc:, 100.00] [G loss: 8.414375]\n",
            "********* 1985 [D loss: 0.068219, acc:, 96.09] [G loss: 6.454885]\n",
            "********* 1986 [D loss: 0.037257, acc:, 98.44] [G loss: 8.075114]\n",
            "********* 1987 [D loss: 0.021332, acc:, 99.22] [G loss: 11.583862]\n",
            "********* 1988 [D loss: 0.057786, acc:, 97.66] [G loss: 7.935376]\n",
            "********* 1989 [D loss: 0.053629, acc:, 96.88] [G loss: 7.641037]\n",
            "********* 1990 [D loss: 0.020735, acc:, 99.22] [G loss: 9.319893]\n",
            "********* 1991 [D loss: 0.010905, acc:, 99.22] [G loss: 10.451769]\n",
            "********* 1992 [D loss: 0.013559, acc:, 99.22] [G loss: 10.780805]\n",
            "********* 1993 [D loss: 0.004018, acc:, 100.00] [G loss: 10.502089]\n",
            "********* 1994 [D loss: 0.018656, acc:, 99.22] [G loss: 8.697223]\n",
            "********* 1995 [D loss: 0.013231, acc:, 99.22] [G loss: 7.644819]\n",
            "********* 1996 [D loss: 0.045734, acc:, 99.22] [G loss: 9.734493]\n",
            "********* 1997 [D loss: 0.002025, acc:, 100.00] [G loss: 10.747734]\n",
            "********* 1998 [D loss: 0.009069, acc:, 100.00] [G loss: 11.213472]\n",
            "********* 1999 [D loss: 0.036997, acc:, 99.22] [G loss: 11.043772]\n",
            "********* 2000 [D loss: 0.082359, acc:, 98.44] [G loss: 8.607172]\n",
            "********* 2001 [D loss: 0.016160, acc:, 99.22] [G loss: 7.830582]\n",
            "********* 2002 [D loss: 0.169004, acc:, 94.53] [G loss: 10.844375]\n",
            "********* 2003 [D loss: 0.011738, acc:, 99.22] [G loss: 16.946297]\n",
            "********* 2004 [D loss: 0.091509, acc:, 96.88] [G loss: 14.188478]\n",
            "********* 2005 [D loss: 0.241442, acc:, 92.97] [G loss: 8.992174]\n",
            "********* 2006 [D loss: 0.020621, acc:, 100.00] [G loss: 13.109886]\n",
            "********* 2007 [D loss: 0.184083, acc:, 92.97] [G loss: 16.708275]\n",
            "********* 2008 [D loss: 0.025336, acc:, 99.22] [G loss: 20.617628]\n",
            "********* 2009 [D loss: 0.100703, acc:, 97.66] [G loss: 15.439476]\n",
            "********* 2010 [D loss: 0.076046, acc:, 98.44] [G loss: 12.845831]\n",
            "********* 2011 [D loss: 0.029984, acc:, 99.22] [G loss: 12.198986]\n",
            "********* 2012 [D loss: 0.006215, acc:, 100.00] [G loss: 15.720407]\n",
            "********* 2013 [D loss: 0.022307, acc:, 98.44] [G loss: 16.011349]\n",
            "********* 2014 [D loss: 0.043808, acc:, 98.44] [G loss: 14.162140]\n",
            "********* 2015 [D loss: 0.003620, acc:, 100.00] [G loss: 12.044713]\n",
            "********* 2016 [D loss: 0.018823, acc:, 99.22] [G loss: 12.592615]\n",
            "********* 2017 [D loss: 0.023486, acc:, 97.66] [G loss: 11.365093]\n",
            "********* 2018 [D loss: 0.065186, acc:, 97.66] [G loss: 12.540316]\n",
            "********* 2019 [D loss: 0.020820, acc:, 99.22] [G loss: 15.425121]\n",
            "********* 2020 [D loss: 0.098429, acc:, 98.44] [G loss: 12.888000]\n",
            "********* 2021 [D loss: 0.050786, acc:, 97.66] [G loss: 8.613972]\n",
            "********* 2022 [D loss: 0.079899, acc:, 95.31] [G loss: 13.297850]\n",
            "********* 2023 [D loss: 0.027748, acc:, 99.22] [G loss: 20.963158]\n",
            "********* 2024 [D loss: 0.255111, acc:, 94.53] [G loss: 10.043280]\n",
            "********* 2025 [D loss: 0.069517, acc:, 96.88] [G loss: 8.119995]\n",
            "********* 2026 [D loss: 0.004855, acc:, 100.00] [G loss: 9.527626]\n",
            "********* 2027 [D loss: 0.008538, acc:, 99.22] [G loss: 10.990540]\n",
            "********* 2028 [D loss: 0.001353, acc:, 100.00] [G loss: 12.452934]\n",
            "********* 2029 [D loss: 0.009072, acc:, 100.00] [G loss: 12.679409]\n",
            "********* 2030 [D loss: 0.005450, acc:, 100.00] [G loss: 13.411805]\n",
            "********* 2031 [D loss: 0.008648, acc:, 100.00] [G loss: 12.709133]\n",
            "********* 2032 [D loss: 0.039999, acc:, 99.22] [G loss: 12.200502]\n",
            "********* 2033 [D loss: 0.009922, acc:, 100.00] [G loss: 11.131834]\n",
            "********* 2034 [D loss: 0.078540, acc:, 95.31] [G loss: 10.732637]\n",
            "********* 2035 [D loss: 0.015094, acc:, 99.22] [G loss: 11.254320]\n",
            "********* 2036 [D loss: 0.007240, acc:, 100.00] [G loss: 11.554022]\n",
            "********* 2037 [D loss: 0.056358, acc:, 98.44] [G loss: 9.543475]\n",
            "********* 2038 [D loss: 0.016937, acc:, 100.00] [G loss: 9.114023]\n",
            "********* 2039 [D loss: 0.015626, acc:, 99.22] [G loss: 11.341373]\n",
            "********* 2040 [D loss: 0.003988, acc:, 100.00] [G loss: 11.749319]\n",
            "********* 2041 [D loss: 0.094046, acc:, 95.31] [G loss: 8.753811]\n",
            "********* 2042 [D loss: 0.028600, acc:, 99.22] [G loss: 10.265637]\n",
            "********* 2043 [D loss: 0.235443, acc:, 92.97] [G loss: 11.422437]\n",
            "********* 2044 [D loss: 0.022604, acc:, 99.22] [G loss: 14.709198]\n",
            "********* 2045 [D loss: 0.005874, acc:, 100.00] [G loss: 18.606678]\n",
            "********* 2046 [D loss: 0.319009, acc:, 93.75] [G loss: 6.121303]\n",
            "********* 2047 [D loss: 0.561064, acc:, 79.69] [G loss: 13.432872]\n",
            "********* 2048 [D loss: 0.017770, acc:, 99.22] [G loss: 41.677425]\n",
            "********* 2049 [D loss: 1.662921, acc:, 81.25] [G loss: 15.912396]\n",
            "********* 2050 [D loss: 0.299597, acc:, 89.06] [G loss: 5.473785]\n",
            "********* 2051 [D loss: 0.364011, acc:, 85.16] [G loss: 8.487181]\n",
            "********* 2052 [D loss: 0.010696, acc:, 100.00] [G loss: 18.861841]\n",
            "********* 2053 [D loss: 0.274057, acc:, 94.53] [G loss: 22.114796]\n",
            "********* 2054 [D loss: 0.217240, acc:, 92.97] [G loss: 19.484800]\n",
            "********* 2055 [D loss: 0.020106, acc:, 98.44] [G loss: 15.162995]\n",
            "********* 2056 [D loss: 0.014945, acc:, 99.22] [G loss: 12.984962]\n",
            "********* 2057 [D loss: 0.016669, acc:, 99.22] [G loss: 10.776434]\n",
            "********* 2058 [D loss: 0.007026, acc:, 100.00] [G loss: 9.815243]\n",
            "********* 2059 [D loss: 0.016521, acc:, 99.22] [G loss: 10.480650]\n",
            "********* 2060 [D loss: 0.002755, acc:, 100.00] [G loss: 11.169352]\n",
            "********* 2061 [D loss: 0.016789, acc:, 98.44] [G loss: 11.452854]\n",
            "********* 2062 [D loss: 0.005996, acc:, 100.00] [G loss: 12.682746]\n",
            "********* 2063 [D loss: 0.002780, acc:, 100.00] [G loss: 13.485840]\n",
            "********* 2064 [D loss: 0.006910, acc:, 100.00] [G loss: 13.443074]\n",
            "********* 2065 [D loss: 0.003082, acc:, 100.00] [G loss: 14.104474]\n",
            "********* 2066 [D loss: 0.004794, acc:, 100.00] [G loss: 13.140517]\n",
            "********* 2067 [D loss: 0.073436, acc:, 98.44] [G loss: 11.324154]\n",
            "********* 2068 [D loss: 0.033537, acc:, 99.22] [G loss: 8.772434]\n",
            "********* 2069 [D loss: 0.019466, acc:, 100.00] [G loss: 7.750179]\n",
            "********* 2070 [D loss: 0.022679, acc:, 99.22] [G loss: 9.366266]\n",
            "********* 2071 [D loss: 0.025266, acc:, 99.22] [G loss: 9.042515]\n",
            "********* 2072 [D loss: 0.004272, acc:, 100.00] [G loss: 10.522618]\n",
            "********* 2073 [D loss: 0.004382, acc:, 100.00] [G loss: 10.004551]\n",
            "********* 2074 [D loss: 0.037725, acc:, 97.66] [G loss: 8.196499]\n",
            "********* 2075 [D loss: 0.006381, acc:, 100.00] [G loss: 7.103862]\n",
            "********* 2076 [D loss: 0.014443, acc:, 100.00] [G loss: 7.578787]\n",
            "********* 2077 [D loss: 0.018408, acc:, 99.22] [G loss: 8.241009]\n",
            "********* 2078 [D loss: 0.008650, acc:, 100.00] [G loss: 9.811200]\n",
            "********* 2079 [D loss: 0.021761, acc:, 100.00] [G loss: 9.381936]\n",
            "********* 2080 [D loss: 0.027660, acc:, 100.00] [G loss: 7.795720]\n",
            "********* 2081 [D loss: 0.036504, acc:, 98.44] [G loss: 5.967817]\n",
            "********* 2082 [D loss: 0.063236, acc:, 96.09] [G loss: 5.882070]\n",
            "********* 2083 [D loss: 0.016531, acc:, 100.00] [G loss: 8.030408]\n",
            "********* 2084 [D loss: 0.037005, acc:, 98.44] [G loss: 9.239778]\n",
            "********* 2085 [D loss: 0.028297, acc:, 99.22] [G loss: 7.757788]\n",
            "********* 2086 [D loss: 0.025320, acc:, 99.22] [G loss: 7.954279]\n",
            "********* 2087 [D loss: 0.022735, acc:, 99.22] [G loss: 7.448982]\n",
            "********* 2088 [D loss: 0.028061, acc:, 100.00] [G loss: 7.983991]\n",
            "********* 2089 [D loss: 0.017966, acc:, 99.22] [G loss: 9.019226]\n",
            "********* 2090 [D loss: 0.045801, acc:, 98.44] [G loss: 7.511215]\n",
            "********* 2091 [D loss: 0.037209, acc:, 99.22] [G loss: 6.763458]\n",
            "********* 2092 [D loss: 0.041087, acc:, 98.44] [G loss: 6.786980]\n",
            "********* 2093 [D loss: 0.022038, acc:, 99.22] [G loss: 8.281162]\n",
            "********* 2094 [D loss: 0.017981, acc:, 100.00] [G loss: 8.566792]\n",
            "********* 2095 [D loss: 0.052950, acc:, 98.44] [G loss: 7.262724]\n",
            "********* 2096 [D loss: 0.022688, acc:, 99.22] [G loss: 6.991896]\n",
            "********* 2097 [D loss: 0.082607, acc:, 96.88] [G loss: 7.125706]\n",
            "********* 2098 [D loss: 0.014179, acc:, 99.22] [G loss: 7.009411]\n",
            "********* 2099 [D loss: 0.037076, acc:, 98.44] [G loss: 8.684127]\n",
            "********* 2100 [D loss: 0.124688, acc:, 96.88] [G loss: 7.900735]\n",
            "********* 2101 [D loss: 0.023342, acc:, 100.00] [G loss: 7.668567]\n",
            "********* 2102 [D loss: 0.123288, acc:, 96.88] [G loss: 7.141156]\n",
            "********* 2103 [D loss: 0.033522, acc:, 99.22] [G loss: 7.890903]\n",
            "********* 2104 [D loss: 0.035110, acc:, 99.22] [G loss: 7.840794]\n",
            "********* 2105 [D loss: 0.023852, acc:, 100.00] [G loss: 8.053568]\n",
            "********* 2106 [D loss: 0.006558, acc:, 100.00] [G loss: 9.992027]\n",
            "********* 2107 [D loss: 0.012849, acc:, 100.00] [G loss: 10.341555]\n",
            "********* 2108 [D loss: 0.066474, acc:, 96.88] [G loss: 7.257236]\n",
            "********* 2109 [D loss: 0.111338, acc:, 96.88] [G loss: 7.468406]\n",
            "********* 2110 [D loss: 0.029880, acc:, 99.22] [G loss: 7.363400]\n",
            "********* 2111 [D loss: 0.052668, acc:, 99.22] [G loss: 8.856956]\n",
            "********* 2112 [D loss: 0.031106, acc:, 97.66] [G loss: 10.839417]\n",
            "********* 2113 [D loss: 0.116497, acc:, 96.09] [G loss: 9.254786]\n",
            "********* 2114 [D loss: 0.012286, acc:, 100.00] [G loss: 8.321267]\n",
            "********* 2115 [D loss: 0.088956, acc:, 95.31] [G loss: 10.724280]\n",
            "********* 2116 [D loss: 0.036212, acc:, 98.44] [G loss: 14.238143]\n",
            "********* 2117 [D loss: 0.222091, acc:, 91.41] [G loss: 8.414016]\n",
            "********* 2118 [D loss: 0.183900, acc:, 92.97] [G loss: 4.939736]\n",
            "********* 2119 [D loss: 0.067567, acc:, 96.88] [G loss: 6.943934]\n",
            "********* 2120 [D loss: 0.010735, acc:, 100.00] [G loss: 9.538781]\n",
            "********* 2121 [D loss: 0.018456, acc:, 99.22] [G loss: 11.967611]\n",
            "********* 2122 [D loss: 0.032739, acc:, 98.44] [G loss: 11.649209]\n",
            "********* 2123 [D loss: 0.016160, acc:, 100.00] [G loss: 10.304043]\n",
            "********* 2124 [D loss: 0.032719, acc:, 99.22] [G loss: 7.227577]\n",
            "********* 2125 [D loss: 0.073886, acc:, 96.88] [G loss: 7.798973]\n",
            "********* 2126 [D loss: 0.017975, acc:, 99.22] [G loss: 10.135966]\n",
            "********* 2127 [D loss: 0.022963, acc:, 99.22] [G loss: 9.645317]\n",
            "********* 2128 [D loss: 0.033298, acc:, 98.44] [G loss: 7.848157]\n",
            "********* 2129 [D loss: 0.029218, acc:, 99.22] [G loss: 7.061068]\n",
            "********* 2130 [D loss: 0.037314, acc:, 98.44] [G loss: 7.000398]\n",
            "********* 2131 [D loss: 0.010261, acc:, 100.00] [G loss: 7.688473]\n",
            "********* 2132 [D loss: 0.004157, acc:, 100.00] [G loss: 9.389948]\n",
            "********* 2133 [D loss: 0.015701, acc:, 100.00] [G loss: 9.959846]\n",
            "********* 2134 [D loss: 0.020354, acc:, 99.22] [G loss: 10.315706]\n",
            "********* 2135 [D loss: 0.045864, acc:, 99.22] [G loss: 8.205045]\n",
            "********* 2136 [D loss: 0.022788, acc:, 99.22] [G loss: 8.317985]\n",
            "********* 2137 [D loss: 0.009202, acc:, 99.22] [G loss: 9.006008]\n",
            "********* 2138 [D loss: 0.032883, acc:, 98.44] [G loss: 8.335145]\n",
            "********* 2139 [D loss: 0.030908, acc:, 98.44] [G loss: 6.907368]\n",
            "********* 2140 [D loss: 0.004601, acc:, 100.00] [G loss: 7.758020]\n",
            "********* 2141 [D loss: 0.075211, acc:, 97.66] [G loss: 5.557985]\n",
            "********* 2142 [D loss: 0.080992, acc:, 97.66] [G loss: 9.165372]\n",
            "********* 2143 [D loss: 0.041859, acc:, 98.44] [G loss: 13.850429]\n",
            "********* 2144 [D loss: 0.148741, acc:, 94.53] [G loss: 6.868819]\n",
            "********* 2145 [D loss: 0.066259, acc:, 96.88] [G loss: 4.906652]\n",
            "********* 2146 [D loss: 0.015037, acc:, 99.22] [G loss: 7.678095]\n",
            "********* 2147 [D loss: 0.004111, acc:, 100.00] [G loss: 11.307082]\n",
            "********* 2148 [D loss: 0.011132, acc:, 100.00] [G loss: 13.123742]\n",
            "********* 2149 [D loss: 0.025393, acc:, 100.00] [G loss: 10.950579]\n",
            "********* 2150 [D loss: 0.076241, acc:, 99.22] [G loss: 8.298263]\n",
            "********* 2151 [D loss: 0.031166, acc:, 98.44] [G loss: 6.821352]\n",
            "********* 2152 [D loss: 0.016446, acc:, 99.22] [G loss: 7.997552]\n",
            "********* 2153 [D loss: 0.034819, acc:, 98.44] [G loss: 10.474794]\n",
            "********* 2154 [D loss: 0.041240, acc:, 97.66] [G loss: 11.500834]\n",
            "********* 2155 [D loss: 0.008818, acc:, 100.00] [G loss: 12.351254]\n",
            "********* 2156 [D loss: 0.020179, acc:, 100.00] [G loss: 10.093350]\n",
            "********* 2157 [D loss: 0.081351, acc:, 96.09] [G loss: 6.525827]\n",
            "********* 2158 [D loss: 0.039953, acc:, 98.44] [G loss: 7.126967]\n",
            "********* 2159 [D loss: 0.024719, acc:, 99.22] [G loss: 8.555578]\n",
            "********* 2160 [D loss: 0.134433, acc:, 97.66] [G loss: 10.738434]\n",
            "********* 2161 [D loss: 0.093605, acc:, 96.88] [G loss: 12.612524]\n",
            "********* 2162 [D loss: 0.060675, acc:, 98.44] [G loss: 12.448909]\n",
            "********* 2163 [D loss: 0.090580, acc:, 97.66] [G loss: 7.005818]\n",
            "********* 2164 [D loss: 0.126174, acc:, 95.31] [G loss: 10.255899]\n",
            "********* 2165 [D loss: 0.018842, acc:, 99.22] [G loss: 17.893744]\n",
            "********* 2166 [D loss: 0.169366, acc:, 92.19] [G loss: 13.149636]\n",
            "********* 2167 [D loss: 0.090220, acc:, 97.66] [G loss: 7.115816]\n",
            "********* 2168 [D loss: 0.152204, acc:, 92.97] [G loss: 6.159155]\n",
            "********* 2169 [D loss: 0.009092, acc:, 100.00] [G loss: 9.862687]\n",
            "********* 2170 [D loss: 0.001516, acc:, 100.00] [G loss: 14.078327]\n",
            "********* 2171 [D loss: 0.034583, acc:, 98.44] [G loss: 14.585865]\n",
            "********* 2172 [D loss: 0.181455, acc:, 92.19] [G loss: 9.496191]\n",
            "********* 2173 [D loss: 0.091636, acc:, 97.66] [G loss: 8.084198]\n",
            "********* 2174 [D loss: 0.013189, acc:, 99.22] [G loss: 8.742865]\n",
            "********* 2175 [D loss: 0.125172, acc:, 95.31] [G loss: 11.482834]\n",
            "********* 2176 [D loss: 0.001902, acc:, 100.00] [G loss: 15.718328]\n",
            "********* 2177 [D loss: 0.068446, acc:, 97.66] [G loss: 14.800105]\n",
            "********* 2178 [D loss: 0.008680, acc:, 100.00] [G loss: 14.350639]\n",
            "********* 2179 [D loss: 0.014889, acc:, 99.22] [G loss: 13.147213]\n",
            "********* 2180 [D loss: 0.006666, acc:, 100.00] [G loss: 11.043067]\n",
            "********* 2181 [D loss: 0.024606, acc:, 99.22] [G loss: 9.197241]\n",
            "********* 2182 [D loss: 0.003624, acc:, 100.00] [G loss: 11.244979]\n",
            "********* 2183 [D loss: 0.002583, acc:, 100.00] [G loss: 11.797119]\n",
            "********* 2184 [D loss: 0.020502, acc:, 99.22] [G loss: 10.041918]\n",
            "********* 2185 [D loss: 0.019410, acc:, 100.00] [G loss: 9.052458]\n",
            "********* 2186 [D loss: 0.003936, acc:, 100.00] [G loss: 9.238960]\n",
            "********* 2187 [D loss: 0.007042, acc:, 100.00] [G loss: 9.109214]\n",
            "********* 2188 [D loss: 0.002280, acc:, 100.00] [G loss: 9.601868]\n",
            "********* 2189 [D loss: 0.005425, acc:, 100.00] [G loss: 9.070375]\n",
            "********* 2190 [D loss: 0.002728, acc:, 100.00] [G loss: 8.833323]\n",
            "********* 2191 [D loss: 0.002053, acc:, 100.00] [G loss: 9.292747]\n",
            "********* 2192 [D loss: 0.036096, acc:, 98.44] [G loss: 7.898839]\n",
            "********* 2193 [D loss: 0.006010, acc:, 100.00] [G loss: 9.391379]\n",
            "********* 2194 [D loss: 0.038010, acc:, 99.22] [G loss: 8.983046]\n",
            "********* 2195 [D loss: 0.029603, acc:, 98.44] [G loss: 8.360317]\n",
            "********* 2196 [D loss: 0.024122, acc:, 99.22] [G loss: 7.664946]\n",
            "********* 2197 [D loss: 0.010345, acc:, 100.00] [G loss: 8.074074]\n",
            "********* 2198 [D loss: 0.004975, acc:, 100.00] [G loss: 8.762257]\n",
            "********* 2199 [D loss: 0.073172, acc:, 96.88] [G loss: 6.417087]\n",
            "********* 2200 [D loss: 0.052175, acc:, 98.44] [G loss: 7.764129]\n",
            "********* 2201 [D loss: 0.017813, acc:, 99.22] [G loss: 9.933153]\n",
            "********* 2202 [D loss: 0.017609, acc:, 99.22] [G loss: 11.484442]\n",
            "********* 2203 [D loss: 0.135410, acc:, 95.31] [G loss: 5.657396]\n",
            "********* 2204 [D loss: 0.090761, acc:, 96.09] [G loss: 7.270834]\n",
            "********* 2205 [D loss: 0.011835, acc:, 99.22] [G loss: 11.916141]\n",
            "********* 2206 [D loss: 0.030662, acc:, 98.44] [G loss: 14.211487]\n",
            "********* 2207 [D loss: 0.040605, acc:, 99.22] [G loss: 13.504971]\n",
            "********* 2208 [D loss: 0.064595, acc:, 98.44] [G loss: 9.057091]\n",
            "********* 2209 [D loss: 0.039985, acc:, 97.66] [G loss: 5.616650]\n",
            "********* 2210 [D loss: 0.088168, acc:, 96.09] [G loss: 6.811379]\n",
            "********* 2211 [D loss: 0.010286, acc:, 99.22] [G loss: 11.466377]\n",
            "********* 2212 [D loss: 0.036781, acc:, 98.44] [G loss: 12.187925]\n",
            "********* 2213 [D loss: 0.028761, acc:, 98.44] [G loss: 11.301554]\n",
            "********* 2214 [D loss: 0.028932, acc:, 99.22] [G loss: 7.615219]\n",
            "********* 2215 [D loss: 0.010099, acc:, 100.00] [G loss: 5.845948]\n",
            "********* 2216 [D loss: 0.011537, acc:, 100.00] [G loss: 4.914637]\n",
            "********* 2217 [D loss: 0.014302, acc:, 100.00] [G loss: 6.827185]\n",
            "********* 2218 [D loss: 0.004384, acc:, 100.00] [G loss: 9.312168]\n",
            "********* 2219 [D loss: 0.001263, acc:, 100.00] [G loss: 11.363309]\n",
            "********* 2220 [D loss: 0.022091, acc:, 98.44] [G loss: 9.764218]\n",
            "********* 2221 [D loss: 0.004285, acc:, 100.00] [G loss: 9.450795]\n",
            "********* 2222 [D loss: 0.006792, acc:, 100.00] [G loss: 8.733025]\n",
            "********* 2223 [D loss: 0.056317, acc:, 98.44] [G loss: 6.725836]\n",
            "********* 2224 [D loss: 0.022754, acc:, 99.22] [G loss: 6.115399]\n",
            "********* 2225 [D loss: 0.016365, acc:, 100.00] [G loss: 7.664573]\n",
            "********* 2226 [D loss: 0.010748, acc:, 100.00] [G loss: 9.361979]\n",
            "********* 2227 [D loss: 0.010187, acc:, 99.22] [G loss: 10.945099]\n",
            "********* 2228 [D loss: 0.117295, acc:, 96.88] [G loss: 8.023378]\n",
            "********* 2229 [D loss: 0.055735, acc:, 98.44] [G loss: 7.509155]\n",
            "********* 2230 [D loss: 0.021155, acc:, 99.22] [G loss: 7.835011]\n",
            "********* 2231 [D loss: 0.012823, acc:, 100.00] [G loss: 9.477346]\n",
            "********* 2232 [D loss: 0.002104, acc:, 100.00] [G loss: 11.561926]\n",
            "********* 2233 [D loss: 0.047151, acc:, 99.22] [G loss: 9.966192]\n",
            "********* 2234 [D loss: 0.053093, acc:, 96.88] [G loss: 6.018206]\n",
            "********* 2235 [D loss: 0.037457, acc:, 99.22] [G loss: 6.010648]\n",
            "********* 2236 [D loss: 0.003631, acc:, 100.00] [G loss: 8.574066]\n",
            "********* 2237 [D loss: 0.005011, acc:, 100.00] [G loss: 11.713692]\n",
            "********* 2238 [D loss: 0.027905, acc:, 99.22] [G loss: 9.945076]\n",
            "********* 2239 [D loss: 0.004273, acc:, 100.00] [G loss: 8.628043]\n",
            "********* 2240 [D loss: 0.024640, acc:, 99.22] [G loss: 6.354324]\n",
            "********* 2241 [D loss: 0.036564, acc:, 99.22] [G loss: 6.085480]\n",
            "********* 2242 [D loss: 0.010018, acc:, 100.00] [G loss: 7.939648]\n",
            "********* 2243 [D loss: 0.028281, acc:, 99.22] [G loss: 8.894977]\n",
            "********* 2244 [D loss: 0.012161, acc:, 100.00] [G loss: 8.453365]\n",
            "********* 2245 [D loss: 0.008246, acc:, 100.00] [G loss: 7.991080]\n",
            "********* 2246 [D loss: 0.040105, acc:, 97.66] [G loss: 8.547729]\n",
            "********* 2247 [D loss: 0.013586, acc:, 100.00] [G loss: 7.615035]\n",
            "********* 2248 [D loss: 0.027618, acc:, 99.22] [G loss: 6.963480]\n",
            "********* 2249 [D loss: 0.045044, acc:, 98.44] [G loss: 8.650772]\n",
            "********* 2250 [D loss: 0.062414, acc:, 96.88] [G loss: 11.221767]\n",
            "********* 2251 [D loss: 0.015837, acc:, 99.22] [G loss: 13.200709]\n",
            "********* 2252 [D loss: 0.044444, acc:, 97.66] [G loss: 10.657183]\n",
            "********* 2253 [D loss: 0.007933, acc:, 100.00] [G loss: 7.949017]\n",
            "********* 2254 [D loss: 0.045202, acc:, 97.66] [G loss: 7.441233]\n",
            "********* 2255 [D loss: 0.082488, acc:, 96.88] [G loss: 7.284691]\n",
            "********* 2256 [D loss: 0.012511, acc:, 100.00] [G loss: 9.154051]\n",
            "********* 2257 [D loss: 0.020638, acc:, 99.22] [G loss: 9.633138]\n",
            "********* 2258 [D loss: 0.012870, acc:, 99.22] [G loss: 8.813713]\n",
            "********* 2259 [D loss: 0.022911, acc:, 99.22] [G loss: 7.964750]\n",
            "********* 2260 [D loss: 0.038535, acc:, 97.66] [G loss: 10.108181]\n",
            "********* 2261 [D loss: 0.002719, acc:, 100.00] [G loss: 12.478035]\n",
            "********* 2262 [D loss: 0.087932, acc:, 96.88] [G loss: 7.429187]\n",
            "********* 2263 [D loss: 0.089810, acc:, 94.53] [G loss: 9.163179]\n",
            "********* 2264 [D loss: 0.020608, acc:, 99.22] [G loss: 17.265806]\n",
            "********* 2265 [D loss: 0.068520, acc:, 98.44] [G loss: 16.459368]\n",
            "********* 2266 [D loss: 0.055565, acc:, 97.66] [G loss: 13.533501]\n",
            "********* 2267 [D loss: 0.052687, acc:, 98.44] [G loss: 8.735241]\n",
            "********* 2268 [D loss: 0.099511, acc:, 97.66] [G loss: 7.616090]\n",
            "********* 2269 [D loss: 0.032327, acc:, 98.44] [G loss: 10.882730]\n",
            "********* 2270 [D loss: 0.002114, acc:, 100.00] [G loss: 14.100641]\n",
            "********* 2271 [D loss: 0.051466, acc:, 98.44] [G loss: 14.426739]\n",
            "********* 2272 [D loss: 0.091511, acc:, 98.44] [G loss: 14.181068]\n",
            "********* 2273 [D loss: 0.030714, acc:, 99.22] [G loss: 11.136559]\n",
            "********* 2274 [D loss: 0.005230, acc:, 100.00] [G loss: 8.432665]\n",
            "********* 2275 [D loss: 0.011672, acc:, 100.00] [G loss: 7.790564]\n",
            "********* 2276 [D loss: 0.082823, acc:, 95.31] [G loss: 9.619837]\n",
            "********* 2277 [D loss: 0.002282, acc:, 100.00] [G loss: 14.148123]\n",
            "********* 2278 [D loss: 0.007498, acc:, 100.00] [G loss: 17.963308]\n",
            "********* 2279 [D loss: 0.161459, acc:, 96.09] [G loss: 13.273655]\n",
            "********* 2280 [D loss: 0.040849, acc:, 98.44] [G loss: 9.002445]\n",
            "********* 2281 [D loss: 0.034963, acc:, 99.22] [G loss: 6.651605]\n",
            "********* 2282 [D loss: 0.070346, acc:, 97.66] [G loss: 7.329263]\n",
            "********* 2283 [D loss: 0.014509, acc:, 99.22] [G loss: 11.261029]\n",
            "********* 2284 [D loss: 0.071210, acc:, 99.22] [G loss: 13.555243]\n",
            "********* 2285 [D loss: 0.014996, acc:, 99.22] [G loss: 12.642598]\n",
            "********* 2286 [D loss: 0.015961, acc:, 99.22] [G loss: 10.329741]\n",
            "********* 2287 [D loss: 0.008360, acc:, 100.00] [G loss: 8.278033]\n",
            "********* 2288 [D loss: 0.021440, acc:, 100.00] [G loss: 7.647415]\n",
            "********* 2289 [D loss: 0.030546, acc:, 98.44] [G loss: 9.452610]\n",
            "********* 2290 [D loss: 0.003548, acc:, 100.00] [G loss: 13.004662]\n",
            "********* 2291 [D loss: 0.042830, acc:, 99.22] [G loss: 12.314293]\n",
            "********* 2292 [D loss: 0.011978, acc:, 100.00] [G loss: 11.989172]\n",
            "********* 2293 [D loss: 0.013638, acc:, 99.22] [G loss: 8.385403]\n",
            "********* 2294 [D loss: 0.016136, acc:, 99.22] [G loss: 7.675208]\n",
            "********* 2295 [D loss: 0.008969, acc:, 100.00] [G loss: 6.615546]\n",
            "********* 2296 [D loss: 0.126611, acc:, 97.66] [G loss: 8.271542]\n",
            "********* 2297 [D loss: 0.010414, acc:, 100.00] [G loss: 10.359322]\n",
            "********* 2298 [D loss: 0.064163, acc:, 98.44] [G loss: 12.289169]\n",
            "********* 2299 [D loss: 0.028737, acc:, 99.22] [G loss: 12.471611]\n",
            "********* 2300 [D loss: 0.047716, acc:, 97.66] [G loss: 7.662901]\n",
            "********* 2301 [D loss: 0.068539, acc:, 98.44] [G loss: 5.964086]\n",
            "********* 2302 [D loss: 0.010067, acc:, 99.22] [G loss: 8.598347]\n",
            "********* 2303 [D loss: 0.012871, acc:, 100.00] [G loss: 8.954019]\n",
            "********* 2304 [D loss: 0.032593, acc:, 99.22] [G loss: 7.041754]\n",
            "********* 2305 [D loss: 0.020045, acc:, 100.00] [G loss: 6.290029]\n",
            "********* 2306 [D loss: 0.012894, acc:, 100.00] [G loss: 6.553678]\n",
            "********* 2307 [D loss: 0.032102, acc:, 98.44] [G loss: 8.186258]\n",
            "********* 2308 [D loss: 0.065137, acc:, 97.66] [G loss: 9.703300]\n",
            "********* 2309 [D loss: 0.013235, acc:, 99.22] [G loss: 9.823220]\n",
            "********* 2310 [D loss: 0.029087, acc:, 99.22] [G loss: 8.079662]\n",
            "********* 2311 [D loss: 0.016729, acc:, 99.22] [G loss: 6.771142]\n",
            "********* 2312 [D loss: 0.029815, acc:, 99.22] [G loss: 8.721284]\n",
            "********* 2313 [D loss: 0.008255, acc:, 100.00] [G loss: 9.481326]\n",
            "********* 2314 [D loss: 0.042407, acc:, 98.44] [G loss: 10.511269]\n",
            "********* 2315 [D loss: 0.013649, acc:, 99.22] [G loss: 8.900361]\n",
            "********* 2316 [D loss: 0.007123, acc:, 100.00] [G loss: 9.268158]\n",
            "********* 2317 [D loss: 0.047708, acc:, 98.44] [G loss: 7.442298]\n",
            "********* 2318 [D loss: 0.022244, acc:, 99.22] [G loss: 7.163911]\n",
            "********* 2319 [D loss: 0.007616, acc:, 100.00] [G loss: 10.056149]\n",
            "********* 2320 [D loss: 0.030823, acc:, 98.44] [G loss: 9.259790]\n",
            "********* 2321 [D loss: 0.027106, acc:, 98.44] [G loss: 8.314556]\n",
            "********* 2322 [D loss: 0.010084, acc:, 100.00] [G loss: 6.068907]\n",
            "********* 2323 [D loss: 0.076865, acc:, 96.88] [G loss: 8.583427]\n",
            "********* 2324 [D loss: 0.006318, acc:, 100.00] [G loss: 13.000893]\n",
            "********* 2325 [D loss: 0.221117, acc:, 93.75] [G loss: 9.206425]\n",
            "********* 2326 [D loss: 0.012651, acc:, 100.00] [G loss: 7.487179]\n",
            "********* 2327 [D loss: 0.013879, acc:, 100.00] [G loss: 6.728799]\n",
            "********* 2328 [D loss: 0.012937, acc:, 100.00] [G loss: 7.702775]\n",
            "********* 2329 [D loss: 0.038684, acc:, 97.66] [G loss: 8.468341]\n",
            "********* 2330 [D loss: 0.024109, acc:, 98.44] [G loss: 8.711893]\n",
            "********* 2331 [D loss: 0.010902, acc:, 99.22] [G loss: 8.463072]\n",
            "********* 2332 [D loss: 0.005534, acc:, 100.00] [G loss: 8.121943]\n",
            "********* 2333 [D loss: 0.005038, acc:, 100.00] [G loss: 8.784848]\n",
            "********* 2334 [D loss: 0.014811, acc:, 100.00] [G loss: 9.705296]\n",
            "********* 2335 [D loss: 0.013643, acc:, 100.00] [G loss: 11.537586]\n",
            "********* 2336 [D loss: 0.007776, acc:, 100.00] [G loss: 12.018642]\n",
            "********* 2337 [D loss: 0.021755, acc:, 99.22] [G loss: 10.888597]\n",
            "********* 2338 [D loss: 0.028847, acc:, 99.22] [G loss: 8.464561]\n",
            "********* 2339 [D loss: 0.008143, acc:, 100.00] [G loss: 8.910875]\n",
            "********* 2340 [D loss: 0.015316, acc:, 100.00] [G loss: 10.054964]\n",
            "********* 2341 [D loss: 0.029312, acc:, 99.22] [G loss: 9.264043]\n",
            "********* 2342 [D loss: 0.036641, acc:, 98.44] [G loss: 8.711418]\n",
            "********* 2343 [D loss: 0.028752, acc:, 98.44] [G loss: 8.646955]\n",
            "********* 2344 [D loss: 0.007046, acc:, 100.00] [G loss: 9.611513]\n",
            "********* 2345 [D loss: 0.035527, acc:, 99.22] [G loss: 8.953487]\n",
            "********* 2346 [D loss: 0.022033, acc:, 100.00] [G loss: 8.158426]\n",
            "********* 2347 [D loss: 0.035553, acc:, 99.22] [G loss: 9.004565]\n",
            "********* 2348 [D loss: 0.044998, acc:, 98.44] [G loss: 7.725208]\n",
            "********* 2349 [D loss: 0.012298, acc:, 100.00] [G loss: 8.497970]\n",
            "********* 2350 [D loss: 0.003956, acc:, 100.00] [G loss: 8.864020]\n",
            "********* 2351 [D loss: 0.051098, acc:, 97.66] [G loss: 8.886432]\n",
            "********* 2352 [D loss: 0.018533, acc:, 100.00] [G loss: 8.055658]\n",
            "********* 2353 [D loss: 0.015311, acc:, 99.22] [G loss: 9.222394]\n",
            "********* 2354 [D loss: 0.004876, acc:, 100.00] [G loss: 9.710981]\n",
            "********* 2355 [D loss: 0.025375, acc:, 99.22] [G loss: 8.833606]\n",
            "********* 2356 [D loss: 0.018895, acc:, 100.00] [G loss: 9.152134]\n",
            "********* 2357 [D loss: 0.048001, acc:, 98.44] [G loss: 7.117641]\n",
            "********* 2358 [D loss: 0.052214, acc:, 98.44] [G loss: 10.316732]\n",
            "********* 2359 [D loss: 0.019318, acc:, 98.44] [G loss: 10.073055]\n",
            "********* 2360 [D loss: 0.017964, acc:, 99.22] [G loss: 9.130434]\n",
            "********* 2361 [D loss: 0.019195, acc:, 100.00] [G loss: 8.974693]\n",
            "********* 2362 [D loss: 0.034998, acc:, 98.44] [G loss: 7.418624]\n",
            "********* 2363 [D loss: 0.060681, acc:, 97.66] [G loss: 10.977186]\n",
            "********* 2364 [D loss: 0.063598, acc:, 97.66] [G loss: 11.043522]\n",
            "********* 2365 [D loss: 0.149248, acc:, 92.19] [G loss: 5.032691]\n",
            "********* 2366 [D loss: 0.123270, acc:, 94.53] [G loss: 10.497857]\n",
            "********* 2367 [D loss: 0.023077, acc:, 98.44] [G loss: 19.846420]\n",
            "********* 2368 [D loss: 0.059179, acc:, 97.66] [G loss: 19.413649]\n",
            "********* 2369 [D loss: 0.422080, acc:, 91.41] [G loss: 1.281714]\n",
            "********* 2370 [D loss: 2.839280, acc:, 56.25] [G loss: 9.262589]\n",
            "********* 2371 [D loss: 0.005286, acc:, 100.00] [G loss: 42.461582]\n",
            "********* 2372 [D loss: 2.807188, acc:, 71.09] [G loss: 21.451115]\n",
            "********* 2373 [D loss: 1.135094, acc:, 82.81] [G loss: 7.610670]\n",
            "********* 2374 [D loss: 1.655712, acc:, 71.88] [G loss: 3.554690]\n",
            "********* 2375 [D loss: 0.314454, acc:, 85.94] [G loss: 7.941688]\n",
            "********* 2376 [D loss: 0.048837, acc:, 97.66] [G loss: 16.900450]\n",
            "********* 2377 [D loss: 0.196831, acc:, 93.75] [G loss: 21.886288]\n",
            "********* 2378 [D loss: 0.846237, acc:, 81.25] [G loss: 16.477283]\n",
            "********* 2379 [D loss: 0.361977, acc:, 87.50] [G loss: 10.553856]\n",
            "********* 2380 [D loss: 0.108341, acc:, 95.31] [G loss: 5.511205]\n",
            "********* 2381 [D loss: 0.137649, acc:, 92.97] [G loss: 6.657440]\n",
            "********* 2382 [D loss: 0.217948, acc:, 92.97] [G loss: 7.946302]\n",
            "********* 2383 [D loss: 0.104073, acc:, 96.88] [G loss: 9.709906]\n",
            "********* 2384 [D loss: 0.003980, acc:, 100.00] [G loss: 13.283257]\n",
            "********* 2385 [D loss: 0.040813, acc:, 98.44] [G loss: 13.233961]\n",
            "********* 2386 [D loss: 0.070782, acc:, 97.66] [G loss: 13.902670]\n",
            "********* 2387 [D loss: 0.072080, acc:, 96.88] [G loss: 12.357141]\n",
            "********* 2388 [D loss: 0.012473, acc:, 100.00] [G loss: 11.985167]\n",
            "********* 2389 [D loss: 0.022127, acc:, 100.00] [G loss: 10.687835]\n",
            "********* 2390 [D loss: 0.043230, acc:, 99.22] [G loss: 9.805578]\n",
            "********* 2391 [D loss: 0.023740, acc:, 99.22] [G loss: 10.178597]\n",
            "********* 2392 [D loss: 0.005588, acc:, 100.00] [G loss: 9.899093]\n",
            "********* 2393 [D loss: 0.010271, acc:, 100.00] [G loss: 10.187922]\n",
            "********* 2394 [D loss: 0.013543, acc:, 100.00] [G loss: 8.929340]\n",
            "********* 2395 [D loss: 0.007878, acc:, 100.00] [G loss: 8.257395]\n",
            "********* 2396 [D loss: 0.013407, acc:, 100.00] [G loss: 8.731894]\n",
            "********* 2397 [D loss: 0.019517, acc:, 99.22] [G loss: 8.760713]\n",
            "********* 2398 [D loss: 0.010476, acc:, 100.00] [G loss: 8.648502]\n",
            "********* 2399 [D loss: 0.006223, acc:, 100.00] [G loss: 7.587556]\n",
            "********* 2400 [D loss: 0.014060, acc:, 100.00] [G loss: 7.076706]\n",
            "********* 2401 [D loss: 0.016770, acc:, 100.00] [G loss: 7.459734]\n",
            "********* 2402 [D loss: 0.013817, acc:, 100.00] [G loss: 7.639894]\n",
            "********* 2403 [D loss: 0.014084, acc:, 100.00] [G loss: 8.240717]\n",
            "********* 2404 [D loss: 0.015971, acc:, 99.22] [G loss: 7.352563]\n",
            "********* 2405 [D loss: 0.009293, acc:, 100.00] [G loss: 6.584524]\n",
            "********* 2406 [D loss: 0.031357, acc:, 99.22] [G loss: 7.563237]\n",
            "********* 2407 [D loss: 0.015942, acc:, 100.00] [G loss: 6.555531]\n",
            "********* 2408 [D loss: 0.015352, acc:, 100.00] [G loss: 6.572427]\n",
            "********* 2409 [D loss: 0.029955, acc:, 98.44] [G loss: 6.030448]\n",
            "********* 2410 [D loss: 0.009241, acc:, 100.00] [G loss: 6.558470]\n",
            "********* 2411 [D loss: 0.014064, acc:, 100.00] [G loss: 7.308890]\n",
            "********* 2412 [D loss: 0.017346, acc:, 99.22] [G loss: 6.661480]\n",
            "********* 2413 [D loss: 0.017430, acc:, 100.00] [G loss: 6.782320]\n",
            "********* 2414 [D loss: 0.020281, acc:, 100.00] [G loss: 6.969224]\n",
            "********* 2415 [D loss: 0.020070, acc:, 99.22] [G loss: 7.911808]\n",
            "********* 2416 [D loss: 0.021794, acc:, 99.22] [G loss: 7.810718]\n",
            "********* 2417 [D loss: 0.065350, acc:, 97.66] [G loss: 7.042104]\n",
            "********* 2418 [D loss: 0.080925, acc:, 97.66] [G loss: 5.881996]\n",
            "********* 2419 [D loss: 0.057846, acc:, 99.22] [G loss: 5.740405]\n",
            "********* 2420 [D loss: 0.064808, acc:, 98.44] [G loss: 5.811391]\n",
            "********* 2421 [D loss: 0.021821, acc:, 100.00] [G loss: 6.495854]\n",
            "********* 2422 [D loss: 0.024368, acc:, 99.22] [G loss: 6.477696]\n",
            "********* 2423 [D loss: 0.033666, acc:, 98.44] [G loss: 7.323686]\n",
            "********* 2424 [D loss: 0.020548, acc:, 100.00] [G loss: 6.438455]\n",
            "********* 2425 [D loss: 0.048649, acc:, 98.44] [G loss: 6.963271]\n",
            "********* 2426 [D loss: 0.020871, acc:, 100.00] [G loss: 5.492183]\n",
            "********* 2427 [D loss: 0.020051, acc:, 100.00] [G loss: 6.333077]\n",
            "********* 2428 [D loss: 0.012920, acc:, 100.00] [G loss: 8.114340]\n",
            "********* 2429 [D loss: 0.032384, acc:, 98.44] [G loss: 6.933028]\n",
            "********* 2430 [D loss: 0.016156, acc:, 100.00] [G loss: 6.046257]\n",
            "********* 2431 [D loss: 0.023836, acc:, 100.00] [G loss: 6.568903]\n",
            "********* 2432 [D loss: 0.023306, acc:, 100.00] [G loss: 5.707856]\n",
            "********* 2433 [D loss: 0.014575, acc:, 100.00] [G loss: 5.957886]\n",
            "********* 2434 [D loss: 0.006844, acc:, 100.00] [G loss: 6.597512]\n",
            "********* 2435 [D loss: 0.044988, acc:, 98.44] [G loss: 6.510209]\n",
            "********* 2436 [D loss: 0.041535, acc:, 98.44] [G loss: 6.472281]\n",
            "********* 2437 [D loss: 0.029347, acc:, 99.22] [G loss: 5.457000]\n",
            "********* 2438 [D loss: 0.022733, acc:, 100.00] [G loss: 5.571715]\n",
            "********* 2439 [D loss: 0.023395, acc:, 100.00] [G loss: 6.672172]\n",
            "********* 2440 [D loss: 0.045455, acc:, 98.44] [G loss: 6.252665]\n",
            "********* 2441 [D loss: 0.035281, acc:, 99.22] [G loss: 5.945782]\n",
            "********* 2442 [D loss: 0.041259, acc:, 99.22] [G loss: 5.248253]\n",
            "********* 2443 [D loss: 0.030519, acc:, 99.22] [G loss: 5.879003]\n",
            "********* 2444 [D loss: 0.016801, acc:, 100.00] [G loss: 6.951077]\n",
            "********* 2445 [D loss: 0.036971, acc:, 98.44] [G loss: 5.915352]\n",
            "********* 2446 [D loss: 0.073661, acc:, 97.66] [G loss: 4.887129]\n",
            "********* 2447 [D loss: 0.048899, acc:, 99.22] [G loss: 5.424314]\n",
            "********* 2448 [D loss: 0.024097, acc:, 100.00] [G loss: 7.435200]\n",
            "********* 2449 [D loss: 0.036369, acc:, 98.44] [G loss: 7.622491]\n",
            "********* 2450 [D loss: 0.053330, acc:, 98.44] [G loss: 5.743397]\n",
            "********* 2451 [D loss: 0.042384, acc:, 98.44] [G loss: 4.225589]\n",
            "********* 2452 [D loss: 0.047078, acc:, 99.22] [G loss: 4.597367]\n",
            "********* 2453 [D loss: 0.024206, acc:, 100.00] [G loss: 7.341784]\n",
            "********* 2454 [D loss: 0.051050, acc:, 98.44] [G loss: 8.023226]\n",
            "********* 2455 [D loss: 0.047564, acc:, 98.44] [G loss: 6.192315]\n",
            "********* 2456 [D loss: 0.028564, acc:, 99.22] [G loss: 5.670212]\n",
            "********* 2457 [D loss: 0.015019, acc:, 100.00] [G loss: 6.087732]\n",
            "********* 2458 [D loss: 0.012558, acc:, 100.00] [G loss: 6.969603]\n",
            "********* 2459 [D loss: 0.046203, acc:, 97.66] [G loss: 6.299600]\n",
            "********* 2460 [D loss: 0.018858, acc:, 99.22] [G loss: 6.438697]\n",
            "********* 2461 [D loss: 0.027356, acc:, 99.22] [G loss: 6.580136]\n",
            "********* 2462 [D loss: 0.046779, acc:, 98.44] [G loss: 6.705852]\n",
            "********* 2463 [D loss: 0.032755, acc:, 99.22] [G loss: 6.357378]\n",
            "********* 2464 [D loss: 0.024496, acc:, 100.00] [G loss: 5.963168]\n",
            "********* 2465 [D loss: 0.020360, acc:, 99.22] [G loss: 6.132062]\n",
            "********* 2466 [D loss: 0.017281, acc:, 99.22] [G loss: 5.834418]\n",
            "********* 2467 [D loss: 0.068064, acc:, 97.66] [G loss: 6.473286]\n",
            "********* 2468 [D loss: 0.027082, acc:, 99.22] [G loss: 6.410523]\n",
            "********* 2469 [D loss: 0.013737, acc:, 100.00] [G loss: 8.103424]\n",
            "********* 2470 [D loss: 0.030474, acc:, 99.22] [G loss: 7.540155]\n",
            "********* 2471 [D loss: 0.017860, acc:, 100.00] [G loss: 6.097675]\n",
            "********* 2472 [D loss: 0.027960, acc:, 99.22] [G loss: 7.032899]\n",
            "********* 2473 [D loss: 0.019704, acc:, 100.00] [G loss: 6.982173]\n",
            "********* 2474 [D loss: 0.029127, acc:, 99.22] [G loss: 5.850188]\n",
            "********* 2475 [D loss: 0.079908, acc:, 96.88] [G loss: 5.607564]\n",
            "********* 2476 [D loss: 0.016222, acc:, 100.00] [G loss: 6.356904]\n",
            "********* 2477 [D loss: 0.034067, acc:, 98.44] [G loss: 8.045574]\n",
            "********* 2478 [D loss: 0.080752, acc:, 96.09] [G loss: 4.974730]\n",
            "********* 2479 [D loss: 0.051291, acc:, 99.22] [G loss: 6.572260]\n",
            "********* 2480 [D loss: 0.021704, acc:, 98.44] [G loss: 9.878736]\n",
            "********* 2481 [D loss: 0.040500, acc:, 98.44] [G loss: 8.827016]\n",
            "********* 2482 [D loss: 0.041126, acc:, 98.44] [G loss: 7.228726]\n",
            "********* 2483 [D loss: 0.041303, acc:, 99.22] [G loss: 6.368854]\n",
            "********* 2484 [D loss: 0.011700, acc:, 100.00] [G loss: 6.579633]\n",
            "********* 2485 [D loss: 0.028314, acc:, 99.22] [G loss: 7.809058]\n",
            "********* 2486 [D loss: 0.035064, acc:, 99.22] [G loss: 7.552885]\n",
            "********* 2487 [D loss: 0.047541, acc:, 97.66] [G loss: 8.520880]\n",
            "********* 2488 [D loss: 0.012469, acc:, 99.22] [G loss: 9.101832]\n",
            "********* 2489 [D loss: 0.052510, acc:, 98.44] [G loss: 6.740808]\n",
            "********* 2490 [D loss: 0.080921, acc:, 96.88] [G loss: 6.464215]\n",
            "********* 2491 [D loss: 0.006256, acc:, 100.00] [G loss: 9.623264]\n",
            "********* 2492 [D loss: 0.106122, acc:, 93.75] [G loss: 5.362063]\n",
            "********* 2493 [D loss: 0.247681, acc:, 92.19] [G loss: 7.227904]\n",
            "********* 2494 [D loss: 0.004191, acc:, 100.00] [G loss: 13.805822]\n",
            "********* 2495 [D loss: 0.078967, acc:, 96.88] [G loss: 13.620236]\n",
            "********* 2496 [D loss: 0.106127, acc:, 96.88] [G loss: 9.134101]\n",
            "********* 2497 [D loss: 0.022436, acc:, 100.00] [G loss: 6.520551]\n",
            "********* 2498 [D loss: 0.163726, acc:, 94.53] [G loss: 8.262622]\n",
            "********* 2499 [D loss: 0.003612, acc:, 100.00] [G loss: 13.284733]\n",
            "********* 2500 [D loss: 0.074886, acc:, 97.66] [G loss: 15.149822]\n",
            "********* 2501 [D loss: 0.129693, acc:, 96.88] [G loss: 12.153590]\n",
            "********* 2502 [D loss: 0.009946, acc:, 100.00] [G loss: 9.860358]\n",
            "********* 2503 [D loss: 0.023158, acc:, 99.22] [G loss: 6.670549]\n",
            "********* 2504 [D loss: 0.035868, acc:, 98.44] [G loss: 7.018368]\n",
            "********* 2505 [D loss: 0.032803, acc:, 98.44] [G loss: 7.167751]\n",
            "********* 2506 [D loss: 0.008856, acc:, 100.00] [G loss: 9.635492]\n",
            "********* 2507 [D loss: 0.030748, acc:, 97.66] [G loss: 9.551826]\n",
            "********* 2508 [D loss: 0.023849, acc:, 99.22] [G loss: 9.651398]\n",
            "********* 2509 [D loss: 0.014955, acc:, 99.22] [G loss: 9.513119]\n",
            "********* 2510 [D loss: 0.041118, acc:, 97.66] [G loss: 8.079054]\n",
            "********* 2511 [D loss: 0.021568, acc:, 100.00] [G loss: 8.255546]\n",
            "********* 2512 [D loss: 0.010793, acc:, 100.00] [G loss: 9.323560]\n",
            "********* 2513 [D loss: 0.076003, acc:, 98.44] [G loss: 10.161033]\n",
            "********* 2514 [D loss: 0.031074, acc:, 99.22] [G loss: 10.059064]\n",
            "********* 2515 [D loss: 0.065236, acc:, 97.66] [G loss: 6.217247]\n",
            "********* 2516 [D loss: 0.018017, acc:, 99.22] [G loss: 6.773907]\n",
            "********* 2517 [D loss: 0.019658, acc:, 99.22] [G loss: 7.535929]\n",
            "********* 2518 [D loss: 0.004959, acc:, 100.00] [G loss: 9.749296]\n",
            "********* 2519 [D loss: 0.008963, acc:, 100.00] [G loss: 10.449268]\n",
            "********* 2520 [D loss: 0.017688, acc:, 99.22] [G loss: 9.461132]\n",
            "********* 2521 [D loss: 0.019303, acc:, 99.22] [G loss: 8.112118]\n",
            "********* 2522 [D loss: 0.095639, acc:, 99.22] [G loss: 7.072437]\n",
            "********* 2523 [D loss: 0.073146, acc:, 97.66] [G loss: 5.963733]\n",
            "********* 2524 [D loss: 0.031778, acc:, 99.22] [G loss: 6.334549]\n",
            "********* 2525 [D loss: 0.007430, acc:, 100.00] [G loss: 8.443958]\n",
            "********* 2526 [D loss: 0.032535, acc:, 99.22] [G loss: 8.420671]\n",
            "********* 2527 [D loss: 0.022289, acc:, 99.22] [G loss: 8.447922]\n",
            "********* 2528 [D loss: 0.021780, acc:, 99.22] [G loss: 8.273272]\n",
            "********* 2529 [D loss: 0.004681, acc:, 100.00] [G loss: 9.867286]\n",
            "********* 2530 [D loss: 0.023881, acc:, 99.22] [G loss: 7.785376]\n",
            "********* 2531 [D loss: 0.033499, acc:, 99.22] [G loss: 8.021942]\n",
            "********* 2532 [D loss: 0.027840, acc:, 99.22] [G loss: 8.141867]\n",
            "********* 2533 [D loss: 0.024425, acc:, 99.22] [G loss: 9.142311]\n",
            "********* 2534 [D loss: 0.022558, acc:, 99.22] [G loss: 9.982937]\n",
            "********* 2535 [D loss: 0.031039, acc:, 99.22] [G loss: 8.438650]\n",
            "********* 2536 [D loss: 0.017831, acc:, 99.22] [G loss: 7.254424]\n",
            "********* 2537 [D loss: 0.029023, acc:, 99.22] [G loss: 8.671329]\n",
            "********* 2538 [D loss: 0.012290, acc:, 100.00] [G loss: 9.683445]\n",
            "********* 2539 [D loss: 0.025291, acc:, 98.44] [G loss: 9.887926]\n",
            "********* 2540 [D loss: 0.027823, acc:, 98.44] [G loss: 6.985492]\n",
            "********* 2541 [D loss: 0.028318, acc:, 99.22] [G loss: 7.144376]\n",
            "********* 2542 [D loss: 0.024415, acc:, 99.22] [G loss: 9.198068]\n",
            "********* 2543 [D loss: 0.017614, acc:, 99.22] [G loss: 11.762310]\n",
            "********* 2544 [D loss: 0.023405, acc:, 99.22] [G loss: 10.673656]\n",
            "********* 2545 [D loss: 0.027871, acc:, 99.22] [G loss: 8.193111]\n",
            "********* 2546 [D loss: 0.011784, acc:, 100.00] [G loss: 8.042766]\n",
            "********* 2547 [D loss: 0.011940, acc:, 100.00] [G loss: 8.403666]\n",
            "********* 2548 [D loss: 0.019688, acc:, 99.22] [G loss: 9.804838]\n",
            "********* 2549 [D loss: 0.017123, acc:, 100.00] [G loss: 9.403624]\n",
            "********* 2550 [D loss: 0.016795, acc:, 99.22] [G loss: 8.504652]\n",
            "********* 2551 [D loss: 0.078159, acc:, 97.66] [G loss: 6.870971]\n",
            "********* 2552 [D loss: 0.047554, acc:, 98.44] [G loss: 7.661139]\n",
            "********* 2553 [D loss: 0.003713, acc:, 100.00] [G loss: 11.411423]\n",
            "********* 2554 [D loss: 0.101344, acc:, 96.88] [G loss: 8.377096]\n",
            "********* 2555 [D loss: 0.040946, acc:, 100.00] [G loss: 9.785157]\n",
            "********* 2556 [D loss: 0.040974, acc:, 99.22] [G loss: 13.695303]\n",
            "********* 2557 [D loss: 0.109304, acc:, 94.53] [G loss: 9.257522]\n",
            "********* 2558 [D loss: 0.264872, acc:, 89.06] [G loss: 9.998318]\n",
            "********* 2559 [D loss: 0.004291, acc:, 100.00] [G loss: 18.641396]\n",
            "********* 2560 [D loss: 0.329283, acc:, 91.41] [G loss: 12.063007]\n",
            "********* 2561 [D loss: 0.014705, acc:, 100.00] [G loss: 8.411316]\n",
            "********* 2562 [D loss: 0.051554, acc:, 98.44] [G loss: 8.290655]\n",
            "********* 2563 [D loss: 0.021025, acc:, 100.00] [G loss: 11.361554]\n",
            "********* 2564 [D loss: 0.118618, acc:, 98.44] [G loss: 10.505541]\n",
            "********* 2565 [D loss: 0.020451, acc:, 99.22] [G loss: 10.017384]\n",
            "********* 2566 [D loss: 0.024055, acc:, 98.44] [G loss: 10.522074]\n",
            "********* 2567 [D loss: 0.019810, acc:, 99.22] [G loss: 8.236202]\n",
            "********* 2568 [D loss: 0.018902, acc:, 100.00] [G loss: 8.261116]\n",
            "********* 2569 [D loss: 0.041298, acc:, 99.22] [G loss: 10.437948]\n",
            "********* 2570 [D loss: 0.006009, acc:, 100.00] [G loss: 12.713802]\n",
            "********* 2571 [D loss: 0.028344, acc:, 98.44] [G loss: 11.860621]\n",
            "********* 2572 [D loss: 0.007085, acc:, 100.00] [G loss: 9.756809]\n",
            "********* 2573 [D loss: 0.011885, acc:, 100.00] [G loss: 9.805004]\n",
            "********* 2574 [D loss: 0.011368, acc:, 100.00] [G loss: 9.888105]\n",
            "********* 2575 [D loss: 0.004687, acc:, 100.00] [G loss: 10.834701]\n",
            "********* 2576 [D loss: 0.006747, acc:, 100.00] [G loss: 11.417894]\n",
            "********* 2577 [D loss: 0.061687, acc:, 96.88] [G loss: 6.672980]\n",
            "********* 2578 [D loss: 0.085371, acc:, 96.88] [G loss: 9.267847]\n",
            "********* 2579 [D loss: 0.019650, acc:, 98.44] [G loss: 14.078572]\n",
            "********* 2580 [D loss: 0.135369, acc:, 96.09] [G loss: 10.208573]\n",
            "********* 2581 [D loss: 0.034345, acc:, 98.44] [G loss: 7.901224]\n",
            "********* 2582 [D loss: 0.083611, acc:, 96.88] [G loss: 7.410526]\n",
            "********* 2583 [D loss: 0.006488, acc:, 100.00] [G loss: 9.958349]\n",
            "********* 2584 [D loss: 0.010614, acc:, 99.22] [G loss: 11.505321]\n",
            "********* 2585 [D loss: 0.105546, acc:, 97.66] [G loss: 8.301081]\n",
            "********* 2586 [D loss: 0.065770, acc:, 96.88] [G loss: 6.238008]\n",
            "********* 2587 [D loss: 0.007570, acc:, 100.00] [G loss: 8.552750]\n",
            "********* 2588 [D loss: 0.017661, acc:, 99.22] [G loss: 9.941964]\n",
            "********* 2589 [D loss: 0.016551, acc:, 100.00] [G loss: 8.790121]\n",
            "********* 2590 [D loss: 0.013405, acc:, 100.00] [G loss: 7.017299]\n",
            "********* 2591 [D loss: 0.040526, acc:, 99.22] [G loss: 6.955959]\n",
            "********* 2592 [D loss: 0.042561, acc:, 97.66] [G loss: 8.461640]\n",
            "********* 2593 [D loss: 0.048871, acc:, 99.22] [G loss: 8.380316]\n",
            "********* 2594 [D loss: 0.016615, acc:, 100.00] [G loss: 6.685018]\n",
            "********* 2595 [D loss: 0.023543, acc:, 100.00] [G loss: 8.620217]\n",
            "********* 2596 [D loss: 0.061719, acc:, 97.66] [G loss: 7.783694]\n",
            "********* 2597 [D loss: 0.061071, acc:, 97.66] [G loss: 6.732024]\n",
            "********* 2598 [D loss: 0.077211, acc:, 97.66] [G loss: 8.540432]\n",
            "********* 2599 [D loss: 0.036587, acc:, 99.22] [G loss: 11.186986]\n",
            "********* 2600 [D loss: 0.120913, acc:, 96.09] [G loss: 4.915083]\n",
            "********* 2601 [D loss: 0.507215, acc:, 79.69] [G loss: 14.956539]\n",
            "********* 2602 [D loss: 0.144147, acc:, 95.31] [G loss: 33.727524]\n",
            "********* 2603 [D loss: 1.385464, acc:, 77.34] [G loss: 11.597008]\n",
            "********* 2604 [D loss: 0.632238, acc:, 78.12] [G loss: 7.368396]\n",
            "********* 2605 [D loss: 0.490525, acc:, 91.41] [G loss: 10.631046]\n",
            "********* 2606 [D loss: 0.190662, acc:, 93.75] [G loss: 14.254984]\n",
            "********* 2607 [D loss: 0.143998, acc:, 96.88] [G loss: 17.505291]\n",
            "********* 2608 [D loss: 0.326013, acc:, 92.97] [G loss: 15.209728]\n",
            "********* 2609 [D loss: 0.155292, acc:, 96.88] [G loss: 13.021889]\n",
            "********* 2610 [D loss: 0.032164, acc:, 98.44] [G loss: 10.223679]\n",
            "********* 2611 [D loss: 0.020942, acc:, 99.22] [G loss: 8.388208]\n",
            "********* 2612 [D loss: 0.073311, acc:, 95.31] [G loss: 8.072011]\n",
            "********* 2613 [D loss: 0.052555, acc:, 96.88] [G loss: 8.503845]\n",
            "********* 2614 [D loss: 0.004189, acc:, 100.00] [G loss: 9.842968]\n",
            "********* 2615 [D loss: 0.003172, acc:, 100.00] [G loss: 11.045141]\n",
            "********* 2616 [D loss: 0.013928, acc:, 99.22] [G loss: 10.332383]\n",
            "********* 2617 [D loss: 0.004592, acc:, 100.00] [G loss: 10.308260]\n",
            "********* 2618 [D loss: 0.085074, acc:, 96.88] [G loss: 10.774445]\n",
            "********* 2619 [D loss: 0.071056, acc:, 98.44] [G loss: 10.381915]\n",
            "********* 2620 [D loss: 0.021823, acc:, 99.22] [G loss: 9.657533]\n",
            "********* 2621 [D loss: 0.018647, acc:, 99.22] [G loss: 9.483986]\n",
            "********* 2622 [D loss: 0.014930, acc:, 99.22] [G loss: 9.491842]\n",
            "********* 2623 [D loss: 0.077303, acc:, 98.44] [G loss: 8.744896]\n",
            "********* 2624 [D loss: 0.001568, acc:, 100.00] [G loss: 8.676826]\n",
            "********* 2625 [D loss: 0.016572, acc:, 99.22] [G loss: 8.041546]\n",
            "********* 2626 [D loss: 0.022994, acc:, 98.44] [G loss: 8.109670]\n",
            "********* 2627 [D loss: 0.018636, acc:, 99.22] [G loss: 8.409137]\n",
            "********* 2628 [D loss: 0.006221, acc:, 100.00] [G loss: 9.089690]\n",
            "********* 2629 [D loss: 0.012078, acc:, 100.00] [G loss: 9.270119]\n",
            "********* 2630 [D loss: 0.010613, acc:, 100.00] [G loss: 8.048006]\n",
            "********* 2631 [D loss: 0.017709, acc:, 100.00] [G loss: 7.570706]\n",
            "********* 2632 [D loss: 0.026997, acc:, 99.22] [G loss: 6.976131]\n",
            "********* 2633 [D loss: 0.058785, acc:, 97.66] [G loss: 8.369546]\n",
            "********* 2634 [D loss: 0.114022, acc:, 97.66] [G loss: 8.224141]\n",
            "********* 2635 [D loss: 0.092753, acc:, 97.66] [G loss: 6.385116]\n",
            "********* 2636 [D loss: 0.025442, acc:, 100.00] [G loss: 6.519902]\n",
            "********* 2637 [D loss: 0.020221, acc:, 99.22] [G loss: 7.031750]\n",
            "********* 2638 [D loss: 0.016791, acc:, 100.00] [G loss: 7.042351]\n",
            "********* 2639 [D loss: 0.042272, acc:, 98.44] [G loss: 5.913404]\n",
            "********* 2640 [D loss: 0.041990, acc:, 99.22] [G loss: 6.667482]\n",
            "********* 2641 [D loss: 0.020697, acc:, 100.00] [G loss: 6.668848]\n",
            "********* 2642 [D loss: 0.050805, acc:, 97.66] [G loss: 6.119878]\n",
            "********* 2643 [D loss: 0.039825, acc:, 98.44] [G loss: 6.601089]\n",
            "********* 2644 [D loss: 0.015414, acc:, 100.00] [G loss: 7.720903]\n",
            "********* 2645 [D loss: 0.063928, acc:, 98.44] [G loss: 6.886952]\n",
            "********* 2646 [D loss: 0.053588, acc:, 97.66] [G loss: 6.167878]\n",
            "********* 2647 [D loss: 0.038792, acc:, 99.22] [G loss: 5.522325]\n",
            "********* 2648 [D loss: 0.013527, acc:, 100.00] [G loss: 7.142405]\n",
            "********* 2649 [D loss: 0.015111, acc:, 99.22] [G loss: 7.690250]\n",
            "********* 2650 [D loss: 0.054852, acc:, 97.66] [G loss: 6.297113]\n",
            "********* 2651 [D loss: 0.052724, acc:, 99.22] [G loss: 5.945307]\n",
            "********* 2652 [D loss: 0.019674, acc:, 100.00] [G loss: 6.068897]\n",
            "********* 2653 [D loss: 0.020597, acc:, 100.00] [G loss: 7.156701]\n",
            "********* 2654 [D loss: 0.015080, acc:, 100.00] [G loss: 7.961761]\n",
            "********* 2655 [D loss: 0.045738, acc:, 98.44] [G loss: 8.547518]\n",
            "********* 2656 [D loss: 0.026800, acc:, 100.00] [G loss: 7.369142]\n",
            "********* 2657 [D loss: 0.029498, acc:, 99.22] [G loss: 7.012566]\n",
            "********* 2658 [D loss: 0.046077, acc:, 99.22] [G loss: 6.200525]\n",
            "********* 2659 [D loss: 0.023073, acc:, 99.22] [G loss: 7.440053]\n",
            "********* 2660 [D loss: 0.016303, acc:, 100.00] [G loss: 8.459497]\n",
            "********* 2661 [D loss: 0.092998, acc:, 99.22] [G loss: 7.743775]\n",
            "********* 2662 [D loss: 0.090861, acc:, 96.09] [G loss: 5.646488]\n",
            "********* 2663 [D loss: 0.069770, acc:, 97.66] [G loss: 7.058055]\n",
            "********* 2664 [D loss: 0.017609, acc:, 99.22] [G loss: 11.065523]\n",
            "********* 2665 [D loss: 0.069242, acc:, 96.09] [G loss: 10.779591]\n",
            "********* 2666 [D loss: 0.111730, acc:, 97.66] [G loss: 6.992245]\n",
            "********* 2667 [D loss: 0.086743, acc:, 96.88] [G loss: 5.314202]\n",
            "********* 2668 [D loss: 0.021439, acc:, 100.00] [G loss: 7.903005]\n",
            "********* 2669 [D loss: 0.003981, acc:, 100.00] [G loss: 10.781428]\n",
            "********* 2670 [D loss: 0.068664, acc:, 96.09] [G loss: 10.003965]\n",
            "********* 2671 [D loss: 0.032816, acc:, 98.44] [G loss: 8.459660]\n",
            "********* 2672 [D loss: 0.005753, acc:, 100.00] [G loss: 7.930589]\n",
            "********* 2673 [D loss: 0.028342, acc:, 98.44] [G loss: 6.986861]\n",
            "********* 2674 [D loss: 0.033118, acc:, 98.44] [G loss: 8.289023]\n",
            "********* 2675 [D loss: 0.010848, acc:, 100.00] [G loss: 7.556424]\n",
            "********* 2676 [D loss: 0.062849, acc:, 98.44] [G loss: 7.756029]\n",
            "********* 2677 [D loss: 0.012007, acc:, 100.00] [G loss: 8.067085]\n",
            "********* 2678 [D loss: 0.037833, acc:, 97.66] [G loss: 6.280072]\n",
            "********* 2679 [D loss: 0.008605, acc:, 100.00] [G loss: 7.084507]\n",
            "********* 2680 [D loss: 0.017218, acc:, 99.22] [G loss: 7.848957]\n",
            "********* 2681 [D loss: 0.046384, acc:, 99.22] [G loss: 8.117915]\n",
            "********* 2682 [D loss: 0.014784, acc:, 100.00] [G loss: 8.692665]\n",
            "********* 2683 [D loss: 0.033429, acc:, 99.22] [G loss: 7.593544]\n",
            "********* 2684 [D loss: 0.022952, acc:, 100.00] [G loss: 6.657141]\n",
            "********* 2685 [D loss: 0.072840, acc:, 96.88] [G loss: 8.040407]\n",
            "********* 2686 [D loss: 0.024505, acc:, 99.22] [G loss: 9.221744]\n",
            "********* 2687 [D loss: 0.013700, acc:, 100.00] [G loss: 9.136404]\n",
            "********* 2688 [D loss: 0.031359, acc:, 98.44] [G loss: 8.026718]\n",
            "********* 2689 [D loss: 0.052933, acc:, 97.66] [G loss: 5.735550]\n",
            "********* 2690 [D loss: 0.093460, acc:, 96.09] [G loss: 7.997076]\n",
            "********* 2691 [D loss: 0.030752, acc:, 99.22] [G loss: 11.594826]\n",
            "********* 2692 [D loss: 0.121139, acc:, 93.75] [G loss: 7.138790]\n",
            "********* 2693 [D loss: 0.072431, acc:, 96.88] [G loss: 4.830144]\n",
            "********* 2694 [D loss: 0.134766, acc:, 93.75] [G loss: 7.840897]\n",
            "********* 2695 [D loss: 0.007516, acc:, 100.00] [G loss: 13.644534]\n",
            "********* 2696 [D loss: 0.147113, acc:, 92.19] [G loss: 10.213747]\n",
            "********* 2697 [D loss: 0.052681, acc:, 98.44] [G loss: 5.850158]\n",
            "********* 2698 [D loss: 0.121904, acc:, 94.53] [G loss: 5.644473]\n",
            "********* 2699 [D loss: 0.046614, acc:, 98.44] [G loss: 10.854916]\n",
            "********* 2700 [D loss: 0.061717, acc:, 97.66] [G loss: 12.136276]\n",
            "********* 2701 [D loss: 0.071259, acc:, 96.09] [G loss: 10.014185]\n",
            "********* 2702 [D loss: 0.006995, acc:, 100.00] [G loss: 8.431799]\n",
            "********* 2703 [D loss: 0.020251, acc:, 98.44] [G loss: 6.971544]\n",
            "********* 2704 [D loss: 0.028719, acc:, 99.22] [G loss: 7.629117]\n",
            "********* 2705 [D loss: 0.023184, acc:, 99.22] [G loss: 7.938413]\n",
            "********* 2706 [D loss: 0.003616, acc:, 100.00] [G loss: 10.021280]\n",
            "********* 2707 [D loss: 0.008327, acc:, 100.00] [G loss: 10.752686]\n",
            "********* 2708 [D loss: 0.021767, acc:, 99.22] [G loss: 10.436024]\n",
            "********* 2709 [D loss: 0.017351, acc:, 99.22] [G loss: 10.287623]\n",
            "********* 2710 [D loss: 0.009121, acc:, 100.00] [G loss: 9.894760]\n",
            "********* 2711 [D loss: 0.015957, acc:, 100.00] [G loss: 8.453201]\n",
            "********* 2712 [D loss: 0.020889, acc:, 99.22] [G loss: 7.541770]\n",
            "********* 2713 [D loss: 0.022669, acc:, 99.22] [G loss: 6.120552]\n",
            "********* 2714 [D loss: 0.063969, acc:, 98.44] [G loss: 6.078259]\n",
            "********* 2715 [D loss: 0.006633, acc:, 100.00] [G loss: 8.035818]\n",
            "********* 2716 [D loss: 0.010138, acc:, 100.00] [G loss: 9.935183]\n",
            "********* 2717 [D loss: 0.158722, acc:, 97.66] [G loss: 7.689992]\n",
            "********* 2718 [D loss: 0.048743, acc:, 98.44] [G loss: 8.001987]\n",
            "********* 2719 [D loss: 0.040648, acc:, 99.22] [G loss: 9.064260]\n",
            "********* 2720 [D loss: 0.017126, acc:, 100.00] [G loss: 9.494093]\n",
            "********* 2721 [D loss: 0.079434, acc:, 95.31] [G loss: 7.232749]\n",
            "********* 2722 [D loss: 0.039919, acc:, 98.44] [G loss: 6.261425]\n",
            "********* 2723 [D loss: 0.039343, acc:, 98.44] [G loss: 6.715876]\n",
            "********* 2724 [D loss: 0.033643, acc:, 98.44] [G loss: 7.609935]\n",
            "********* 2725 [D loss: 0.012349, acc:, 100.00] [G loss: 9.367031]\n",
            "********* 2726 [D loss: 0.108135, acc:, 96.09] [G loss: 6.115267]\n",
            "********* 2727 [D loss: 0.057741, acc:, 99.22] [G loss: 8.176340]\n",
            "********* 2728 [D loss: 0.013265, acc:, 100.00] [G loss: 10.642726]\n",
            "********* 2729 [D loss: 0.192138, acc:, 97.66] [G loss: 11.331055]\n",
            "********* 2730 [D loss: 0.027299, acc:, 99.22] [G loss: 8.306488]\n",
            "********* 2731 [D loss: 0.099758, acc:, 94.53] [G loss: 7.052366]\n",
            "********* 2732 [D loss: 0.037182, acc:, 99.22] [G loss: 9.192774]\n",
            "********* 2733 [D loss: 0.010776, acc:, 100.00] [G loss: 10.088501]\n",
            "********* 2734 [D loss: 0.069787, acc:, 96.88] [G loss: 6.989551]\n",
            "********* 2735 [D loss: 0.071922, acc:, 97.66] [G loss: 6.718422]\n",
            "********* 2736 [D loss: 0.052055, acc:, 97.66] [G loss: 7.317283]\n",
            "********* 2737 [D loss: 0.019668, acc:, 100.00] [G loss: 9.393129]\n",
            "********* 2738 [D loss: 0.053041, acc:, 97.66] [G loss: 10.061709]\n",
            "********* 2739 [D loss: 0.070145, acc:, 98.44] [G loss: 9.031887]\n",
            "********* 2740 [D loss: 0.026414, acc:, 99.22] [G loss: 7.991513]\n",
            "********* 2741 [D loss: 0.017768, acc:, 99.22] [G loss: 8.937380]\n",
            "********* 2742 [D loss: 0.025221, acc:, 99.22] [G loss: 9.847450]\n",
            "********* 2743 [D loss: 0.082798, acc:, 97.66] [G loss: 8.132105]\n",
            "********* 2744 [D loss: 0.020714, acc:, 99.22] [G loss: 8.528096]\n",
            "********* 2745 [D loss: 0.047912, acc:, 96.88] [G loss: 8.974709]\n",
            "********* 2746 [D loss: 0.020352, acc:, 99.22] [G loss: 11.653733]\n",
            "********* 2747 [D loss: 0.068423, acc:, 96.88] [G loss: 10.279682]\n",
            "********* 2748 [D loss: 0.027080, acc:, 98.44] [G loss: 6.648299]\n",
            "********* 2749 [D loss: 0.056471, acc:, 96.09] [G loss: 6.643135]\n",
            "********* 2750 [D loss: 0.017589, acc:, 100.00] [G loss: 8.458101]\n",
            "********* 2751 [D loss: 0.005286, acc:, 100.00] [G loss: 9.786915]\n",
            "********* 2752 [D loss: 0.100962, acc:, 96.09] [G loss: 8.381449]\n",
            "********* 2753 [D loss: 0.019782, acc:, 99.22] [G loss: 7.595196]\n",
            "********* 2754 [D loss: 0.012649, acc:, 100.00] [G loss: 8.248650]\n",
            "********* 2755 [D loss: 0.004216, acc:, 100.00] [G loss: 8.642751]\n",
            "********* 2756 [D loss: 0.049010, acc:, 98.44] [G loss: 8.288978]\n",
            "********* 2757 [D loss: 0.007792, acc:, 100.00] [G loss: 7.863568]\n",
            "********* 2758 [D loss: 0.020339, acc:, 99.22] [G loss: 7.226306]\n",
            "********* 2759 [D loss: 0.013212, acc:, 99.22] [G loss: 7.679059]\n",
            "********* 2760 [D loss: 0.026235, acc:, 99.22] [G loss: 7.791288]\n",
            "********* 2761 [D loss: 0.016903, acc:, 99.22] [G loss: 9.003662]\n",
            "********* 2762 [D loss: 0.009164, acc:, 100.00] [G loss: 9.918406]\n",
            "********* 2763 [D loss: 0.045231, acc:, 97.66] [G loss: 7.045837]\n",
            "********* 2764 [D loss: 0.046036, acc:, 97.66] [G loss: 5.686507]\n",
            "********* 2765 [D loss: 0.018940, acc:, 99.22] [G loss: 7.928539]\n",
            "********* 2766 [D loss: 0.009213, acc:, 100.00] [G loss: 10.251480]\n",
            "********* 2767 [D loss: 0.013797, acc:, 100.00] [G loss: 10.797880]\n",
            "********* 2768 [D loss: 0.022653, acc:, 98.44] [G loss: 9.199186]\n",
            "********* 2769 [D loss: 0.012320, acc:, 100.00] [G loss: 7.576656]\n",
            "********* 2770 [D loss: 0.044065, acc:, 96.88] [G loss: 7.366602]\n",
            "********* 2771 [D loss: 0.009687, acc:, 100.00] [G loss: 9.877879]\n",
            "********* 2772 [D loss: 0.031033, acc:, 99.22] [G loss: 9.415582]\n",
            "********* 2773 [D loss: 0.013791, acc:, 99.22] [G loss: 8.353684]\n",
            "********* 2774 [D loss: 0.009263, acc:, 100.00] [G loss: 6.902936]\n",
            "********* 2775 [D loss: 0.011870, acc:, 100.00] [G loss: 6.046748]\n",
            "********* 2776 [D loss: 0.019235, acc:, 99.22] [G loss: 7.310232]\n",
            "********* 2777 [D loss: 0.018522, acc:, 99.22] [G loss: 8.310911]\n",
            "********* 2778 [D loss: 0.010621, acc:, 100.00] [G loss: 9.278986]\n",
            "********* 2779 [D loss: 0.012403, acc:, 100.00] [G loss: 9.130033]\n",
            "********* 2780 [D loss: 0.009854, acc:, 100.00] [G loss: 7.900787]\n",
            "********* 2781 [D loss: 0.019473, acc:, 99.22] [G loss: 6.519224]\n",
            "********* 2782 [D loss: 0.051420, acc:, 97.66] [G loss: 7.019944]\n",
            "********* 2783 [D loss: 0.018813, acc:, 100.00] [G loss: 8.830935]\n",
            "********* 2784 [D loss: 0.084236, acc:, 96.88] [G loss: 7.122841]\n",
            "********* 2785 [D loss: 0.016830, acc:, 100.00] [G loss: 7.683589]\n",
            "********* 2786 [D loss: 0.047940, acc:, 98.44] [G loss: 6.161726]\n",
            "********* 2787 [D loss: 0.020647, acc:, 100.00] [G loss: 6.871789]\n",
            "********* 2788 [D loss: 0.008255, acc:, 100.00] [G loss: 8.601782]\n",
            "********* 2789 [D loss: 0.010001, acc:, 100.00] [G loss: 8.847527]\n",
            "********* 2790 [D loss: 0.020364, acc:, 99.22] [G loss: 8.425985]\n",
            "********* 2791 [D loss: 0.014889, acc:, 100.00] [G loss: 6.715838]\n",
            "********* 2792 [D loss: 0.025904, acc:, 99.22] [G loss: 6.141986]\n",
            "********* 2793 [D loss: 0.011877, acc:, 100.00] [G loss: 7.529947]\n",
            "********* 2794 [D loss: 0.003394, acc:, 100.00] [G loss: 9.438648]\n",
            "********* 2795 [D loss: 0.008490, acc:, 100.00] [G loss: 9.204229]\n",
            "********* 2796 [D loss: 0.023360, acc:, 99.22] [G loss: 8.136405]\n",
            "********* 2797 [D loss: 0.007329, acc:, 100.00] [G loss: 6.793879]\n",
            "********* 2798 [D loss: 0.008223, acc:, 100.00] [G loss: 6.126831]\n",
            "********* 2799 [D loss: 0.015564, acc:, 100.00] [G loss: 7.464837]\n",
            "********* 2800 [D loss: 0.003957, acc:, 100.00] [G loss: 9.294486]\n",
            "********* 2801 [D loss: 0.012126, acc:, 99.22] [G loss: 9.388229]\n",
            "********* 2802 [D loss: 0.010892, acc:, 100.00] [G loss: 8.035437]\n",
            "********* 2803 [D loss: 0.017570, acc:, 100.00] [G loss: 6.681077]\n",
            "********* 2804 [D loss: 0.013756, acc:, 100.00] [G loss: 6.936379]\n",
            "********* 2805 [D loss: 0.019104, acc:, 100.00] [G loss: 7.124084]\n",
            "********* 2806 [D loss: 0.006251, acc:, 100.00] [G loss: 7.923149]\n",
            "********* 2807 [D loss: 0.008314, acc:, 100.00] [G loss: 8.201445]\n",
            "********* 2808 [D loss: 0.059771, acc:, 96.88] [G loss: 6.415257]\n",
            "********* 2809 [D loss: 0.017872, acc:, 100.00] [G loss: 6.132314]\n",
            "********* 2810 [D loss: 0.007679, acc:, 100.00] [G loss: 7.067475]\n",
            "********* 2811 [D loss: 0.025896, acc:, 98.44] [G loss: 8.943707]\n",
            "********* 2812 [D loss: 0.021845, acc:, 98.44] [G loss: 9.020084]\n",
            "********* 2813 [D loss: 0.015498, acc:, 100.00] [G loss: 6.819889]\n",
            "********* 2814 [D loss: 0.032187, acc:, 99.22] [G loss: 6.670527]\n",
            "********* 2815 [D loss: 0.047167, acc:, 98.44] [G loss: 5.660257]\n",
            "********* 2816 [D loss: 0.028762, acc:, 99.22] [G loss: 8.082243]\n",
            "********* 2817 [D loss: 0.011662, acc:, 99.22] [G loss: 10.690073]\n",
            "********* 2818 [D loss: 0.060690, acc:, 97.66] [G loss: 7.722063]\n",
            "********* 2819 [D loss: 0.095183, acc:, 97.66] [G loss: 5.694243]\n",
            "********* 2820 [D loss: 0.016228, acc:, 99.22] [G loss: 8.653067]\n",
            "********* 2821 [D loss: 0.009339, acc:, 100.00] [G loss: 10.676980]\n",
            "********* 2822 [D loss: 0.010756, acc:, 100.00] [G loss: 11.856139]\n",
            "********* 2823 [D loss: 0.020887, acc:, 98.44] [G loss: 9.251331]\n",
            "********* 2824 [D loss: 0.027952, acc:, 99.22] [G loss: 5.368566]\n",
            "********* 2825 [D loss: 0.086387, acc:, 96.88] [G loss: 10.461540]\n",
            "********* 2826 [D loss: 0.006894, acc:, 100.00] [G loss: 19.269293]\n",
            "********* 2827 [D loss: 0.180929, acc:, 95.31] [G loss: 14.397835]\n",
            "********* 2828 [D loss: 0.036270, acc:, 99.22] [G loss: 8.704775]\n",
            "********* 2829 [D loss: 0.035037, acc:, 98.44] [G loss: 6.079180]\n",
            "********* 2830 [D loss: 0.013549, acc:, 100.00] [G loss: 6.638813]\n",
            "********* 2831 [D loss: 0.012255, acc:, 99.22] [G loss: 8.779378]\n",
            "********* 2832 [D loss: 0.013977, acc:, 99.22] [G loss: 10.974245]\n",
            "********* 2833 [D loss: 0.008339, acc:, 100.00] [G loss: 11.328795]\n",
            "********* 2834 [D loss: 0.018612, acc:, 98.44] [G loss: 10.992207]\n",
            "********* 2835 [D loss: 0.019735, acc:, 99.22] [G loss: 9.372804]\n",
            "********* 2836 [D loss: 0.007992, acc:, 100.00] [G loss: 9.714419]\n",
            "********* 2837 [D loss: 0.001213, acc:, 100.00] [G loss: 10.274258]\n",
            "********* 2838 [D loss: 0.008658, acc:, 100.00] [G loss: 9.794488]\n",
            "********* 2839 [D loss: 0.016851, acc:, 99.22] [G loss: 8.645604]\n",
            "********* 2840 [D loss: 0.022329, acc:, 99.22] [G loss: 7.672535]\n",
            "********* 2841 [D loss: 0.017286, acc:, 100.00] [G loss: 7.035412]\n",
            "********* 2842 [D loss: 0.005379, acc:, 100.00] [G loss: 7.993363]\n",
            "********* 2843 [D loss: 0.001927, acc:, 100.00] [G loss: 9.201083]\n",
            "********* 2844 [D loss: 0.003024, acc:, 100.00] [G loss: 9.337222]\n",
            "********* 2845 [D loss: 0.007851, acc:, 100.00] [G loss: 9.392040]\n",
            "********* 2846 [D loss: 0.019138, acc:, 100.00] [G loss: 9.125689]\n",
            "********* 2847 [D loss: 0.025612, acc:, 100.00] [G loss: 8.991308]\n",
            "********* 2848 [D loss: 0.032581, acc:, 98.44] [G loss: 8.407948]\n",
            "********* 2849 [D loss: 0.004059, acc:, 100.00] [G loss: 7.920843]\n",
            "********* 2850 [D loss: 0.025161, acc:, 98.44] [G loss: 7.226322]\n",
            "********* 2851 [D loss: 0.038534, acc:, 97.66] [G loss: 8.603951]\n",
            "********* 2852 [D loss: 0.032895, acc:, 99.22] [G loss: 9.774003]\n",
            "********* 2853 [D loss: 0.018307, acc:, 99.22] [G loss: 9.476381]\n",
            "********* 2854 [D loss: 0.085603, acc:, 95.31] [G loss: 7.613998]\n",
            "********* 2855 [D loss: 0.027448, acc:, 99.22] [G loss: 8.813537]\n",
            "********* 2856 [D loss: 0.005371, acc:, 100.00] [G loss: 9.525862]\n",
            "********* 2857 [D loss: 0.018270, acc:, 99.22] [G loss: 9.072260]\n",
            "********* 2858 [D loss: 0.035791, acc:, 98.44] [G loss: 6.883181]\n",
            "********* 2859 [D loss: 0.026754, acc:, 99.22] [G loss: 7.464174]\n",
            "********* 2860 [D loss: 0.013245, acc:, 100.00] [G loss: 9.692869]\n",
            "********* 2861 [D loss: 0.022443, acc:, 98.44] [G loss: 9.475375]\n",
            "********* 2862 [D loss: 0.037737, acc:, 98.44] [G loss: 9.996637]\n",
            "********* 2863 [D loss: 0.025130, acc:, 99.22] [G loss: 9.411432]\n",
            "********* 2864 [D loss: 0.005735, acc:, 100.00] [G loss: 9.804480]\n",
            "********* 2865 [D loss: 0.058824, acc:, 97.66] [G loss: 8.111479]\n",
            "********* 2866 [D loss: 0.017676, acc:, 99.22] [G loss: 9.418145]\n",
            "********* 2867 [D loss: 0.041436, acc:, 98.44] [G loss: 9.145319]\n",
            "********* 2868 [D loss: 0.006657, acc:, 100.00] [G loss: 10.702695]\n",
            "********* 2869 [D loss: 0.032409, acc:, 98.44] [G loss: 10.105482]\n",
            "********* 2870 [D loss: 0.039987, acc:, 99.22] [G loss: 7.335548]\n",
            "********* 2871 [D loss: 0.044611, acc:, 99.22] [G loss: 8.131880]\n",
            "********* 2872 [D loss: 0.016691, acc:, 100.00] [G loss: 10.218107]\n",
            "********* 2873 [D loss: 0.016982, acc:, 99.22] [G loss: 10.727740]\n",
            "********* 2874 [D loss: 0.078016, acc:, 98.44] [G loss: 7.978578]\n",
            "********* 2875 [D loss: 0.022162, acc:, 100.00] [G loss: 6.799319]\n",
            "********* 2876 [D loss: 0.028715, acc:, 98.44] [G loss: 8.881849]\n",
            "********* 2877 [D loss: 0.001347, acc:, 100.00] [G loss: 12.495615]\n",
            "********* 2878 [D loss: 0.090215, acc:, 95.31] [G loss: 6.567106]\n",
            "********* 2879 [D loss: 0.070657, acc:, 97.66] [G loss: 6.661763]\n",
            "********* 2880 [D loss: 0.050269, acc:, 99.22] [G loss: 11.865223]\n",
            "********* 2881 [D loss: 0.071751, acc:, 97.66] [G loss: 13.122076]\n",
            "********* 2882 [D loss: 0.072093, acc:, 96.09] [G loss: 14.645399]\n",
            "********* 2883 [D loss: 0.040662, acc:, 99.22] [G loss: 11.751633]\n",
            "********* 2884 [D loss: 0.060177, acc:, 97.66] [G loss: 9.248419]\n",
            "********* 2885 [D loss: 0.030353, acc:, 99.22] [G loss: 8.590218]\n",
            "********* 2886 [D loss: 0.018430, acc:, 99.22] [G loss: 10.485355]\n",
            "********* 2887 [D loss: 0.010142, acc:, 99.22] [G loss: 13.379163]\n",
            "********* 2888 [D loss: 0.041497, acc:, 99.22] [G loss: 13.777485]\n",
            "********* 2889 [D loss: 0.029714, acc:, 99.22] [G loss: 13.474855]\n",
            "********* 2890 [D loss: 0.012523, acc:, 100.00] [G loss: 11.161320]\n",
            "********* 2891 [D loss: 0.043859, acc:, 99.22] [G loss: 8.092709]\n",
            "********* 2892 [D loss: 0.112561, acc:, 97.66] [G loss: 7.740965]\n",
            "********* 2893 [D loss: 0.002528, acc:, 100.00] [G loss: 9.909093]\n",
            "********* 2894 [D loss: 0.049828, acc:, 99.22] [G loss: 12.042440]\n",
            "********* 2895 [D loss: 0.006647, acc:, 100.00] [G loss: 12.689472]\n",
            "********* 2896 [D loss: 0.010631, acc:, 100.00] [G loss: 11.724495]\n",
            "********* 2897 [D loss: 0.051074, acc:, 97.66] [G loss: 10.121655]\n",
            "********* 2898 [D loss: 0.014088, acc:, 100.00] [G loss: 9.842030]\n",
            "********* 2899 [D loss: 0.057605, acc:, 99.22] [G loss: 11.134110]\n",
            "********* 2900 [D loss: 0.062007, acc:, 97.66] [G loss: 10.922091]\n",
            "********* 2901 [D loss: 0.046602, acc:, 98.44] [G loss: 7.670694]\n",
            "********* 2902 [D loss: 0.034486, acc:, 98.44] [G loss: 7.190440]\n",
            "********* 2903 [D loss: 0.017080, acc:, 100.00] [G loss: 8.169001]\n",
            "********* 2904 [D loss: 0.072202, acc:, 98.44] [G loss: 8.505411]\n",
            "********* 2905 [D loss: 0.105575, acc:, 94.53] [G loss: 10.903462]\n",
            "********* 2906 [D loss: 0.044556, acc:, 97.66] [G loss: 8.415968]\n",
            "********* 2907 [D loss: 0.022629, acc:, 100.00] [G loss: 9.346304]\n",
            "********* 2908 [D loss: 0.185828, acc:, 97.66] [G loss: 8.545325]\n",
            "********* 2909 [D loss: 0.158038, acc:, 96.88] [G loss: 11.358369]\n",
            "********* 2910 [D loss: 0.042852, acc:, 99.22] [G loss: 12.108107]\n",
            "********* 2911 [D loss: 0.069709, acc:, 97.66] [G loss: 12.384396]\n",
            "********* 2912 [D loss: 0.052118, acc:, 98.44] [G loss: 10.703308]\n",
            "********* 2913 [D loss: 0.019760, acc:, 99.22] [G loss: 7.699339]\n",
            "********* 2914 [D loss: 0.135870, acc:, 94.53] [G loss: 11.371422]\n",
            "********* 2915 [D loss: 0.010665, acc:, 100.00] [G loss: 19.608461]\n",
            "********* 2916 [D loss: 0.202457, acc:, 94.53] [G loss: 12.870062]\n",
            "********* 2917 [D loss: 0.116799, acc:, 95.31] [G loss: 6.286191]\n",
            "********* 2918 [D loss: 0.130604, acc:, 92.97] [G loss: 9.118465]\n",
            "********* 2919 [D loss: 0.000685, acc:, 100.00] [G loss: 16.691582]\n",
            "********* 2920 [D loss: 0.088473, acc:, 96.09] [G loss: 18.589510]\n",
            "********* 2921 [D loss: 0.038022, acc:, 98.44] [G loss: 14.994110]\n",
            "********* 2922 [D loss: 0.018727, acc:, 98.44] [G loss: 12.366954]\n",
            "********* 2923 [D loss: 0.018975, acc:, 99.22] [G loss: 10.370465]\n",
            "********* 2924 [D loss: 0.151327, acc:, 96.09] [G loss: 11.081322]\n",
            "********* 2925 [D loss: 0.042085, acc:, 98.44] [G loss: 13.750393]\n",
            "********* 2926 [D loss: 0.040805, acc:, 98.44] [G loss: 14.119923]\n",
            "********* 2927 [D loss: 0.019547, acc:, 99.22] [G loss: 12.486044]\n",
            "********* 2928 [D loss: 0.013802, acc:, 99.22] [G loss: 11.464134]\n",
            "********* 2929 [D loss: 0.039299, acc:, 98.44] [G loss: 9.637924]\n",
            "********* 2930 [D loss: 0.025614, acc:, 99.22] [G loss: 10.647756]\n",
            "********* 2931 [D loss: 0.002581, acc:, 100.00] [G loss: 12.857602]\n",
            "********* 2932 [D loss: 0.019152, acc:, 98.44] [G loss: 11.968630]\n",
            "********* 2933 [D loss: 0.008958, acc:, 100.00] [G loss: 10.520463]\n",
            "********* 2934 [D loss: 0.009581, acc:, 100.00] [G loss: 8.897418]\n",
            "********* 2935 [D loss: 0.011853, acc:, 100.00] [G loss: 8.315331]\n",
            "********* 2936 [D loss: 0.026223, acc:, 99.22] [G loss: 8.493456]\n",
            "********* 2937 [D loss: 0.026888, acc:, 100.00] [G loss: 11.475075]\n",
            "********* 2938 [D loss: 0.015689, acc:, 99.22] [G loss: 12.284900]\n",
            "********* 2939 [D loss: 0.040007, acc:, 97.66] [G loss: 12.227358]\n",
            "********* 2940 [D loss: 0.052502, acc:, 98.44] [G loss: 8.279537]\n",
            "********* 2941 [D loss: 0.062215, acc:, 97.66] [G loss: 9.026730]\n",
            "********* 2942 [D loss: 0.067167, acc:, 99.22] [G loss: 10.781402]\n",
            "********* 2943 [D loss: 0.036921, acc:, 99.22] [G loss: 11.232780]\n",
            "********* 2944 [D loss: 0.003225, acc:, 100.00] [G loss: 13.778650]\n",
            "********* 2945 [D loss: 0.055379, acc:, 96.09] [G loss: 7.581371]\n",
            "********* 2946 [D loss: 0.247111, acc:, 84.38] [G loss: 15.019828]\n",
            "********* 2947 [D loss: 0.170182, acc:, 95.31] [G loss: 28.708403]\n",
            "********* 2948 [D loss: 0.739518, acc:, 86.72] [G loss: 10.715451]\n",
            "********* 2949 [D loss: 0.265063, acc:, 89.84] [G loss: 5.850269]\n",
            "********* 2950 [D loss: 0.099124, acc:, 96.09] [G loss: 10.019080]\n",
            "********* 2951 [D loss: 0.041509, acc:, 99.22] [G loss: 15.844748]\n",
            "********* 2952 [D loss: 0.038659, acc:, 98.44] [G loss: 19.624516]\n",
            "********* 2953 [D loss: 0.245251, acc:, 93.75] [G loss: 15.232841]\n",
            "********* 2954 [D loss: 0.014094, acc:, 100.00] [G loss: 12.803617]\n",
            "********* 2955 [D loss: 0.012049, acc:, 99.22] [G loss: 10.933138]\n",
            "********* 2956 [D loss: 0.102606, acc:, 94.53] [G loss: 12.876395]\n",
            "********* 2957 [D loss: 0.026793, acc:, 99.22] [G loss: 18.117069]\n",
            "********* 2958 [D loss: 0.031443, acc:, 99.22] [G loss: 20.459511]\n",
            "********* 2959 [D loss: 0.035166, acc:, 99.22] [G loss: 19.337852]\n",
            "********* 2960 [D loss: 0.183109, acc:, 92.97] [G loss: 12.992050]\n",
            "********* 2961 [D loss: 0.010683, acc:, 99.22] [G loss: 9.394351]\n",
            "********* 2962 [D loss: 0.098811, acc:, 96.88] [G loss: 9.257847]\n",
            "********* 2963 [D loss: 0.028095, acc:, 98.44] [G loss: 14.738939]\n",
            "********* 2964 [D loss: 0.010064, acc:, 100.00] [G loss: 18.299923]\n",
            "********* 2965 [D loss: 0.054519, acc:, 97.66] [G loss: 18.824532]\n",
            "********* 2966 [D loss: 0.101279, acc:, 97.66] [G loss: 15.427566]\n",
            "********* 2967 [D loss: 0.100557, acc:, 96.09] [G loss: 12.123507]\n",
            "********* 2968 [D loss: 0.018663, acc:, 99.22] [G loss: 11.004084]\n",
            "********* 2969 [D loss: 0.019590, acc:, 98.44] [G loss: 10.048826]\n",
            "********* 2970 [D loss: 0.034041, acc:, 97.66] [G loss: 9.392935]\n",
            "********* 2971 [D loss: 0.014382, acc:, 100.00] [G loss: 9.027224]\n",
            "********* 2972 [D loss: 0.050314, acc:, 99.22] [G loss: 8.644069]\n",
            "********* 2973 [D loss: 0.021657, acc:, 99.22] [G loss: 8.950230]\n",
            "********* 2974 [D loss: 0.020788, acc:, 99.22] [G loss: 8.930353]\n",
            "********* 2975 [D loss: 0.014465, acc:, 100.00] [G loss: 9.092478]\n",
            "********* 2976 [D loss: 0.015466, acc:, 99.22] [G loss: 10.740952]\n",
            "********* 2977 [D loss: 0.007732, acc:, 100.00] [G loss: 12.132545]\n",
            "********* 2978 [D loss: 0.004094, acc:, 100.00] [G loss: 11.178965]\n",
            "********* 2979 [D loss: 0.045670, acc:, 97.66] [G loss: 9.188375]\n",
            "********* 2980 [D loss: 0.023295, acc:, 99.22] [G loss: 7.035414]\n",
            "********* 2981 [D loss: 0.020868, acc:, 99.22] [G loss: 6.837139]\n",
            "********* 2982 [D loss: 0.069503, acc:, 96.88] [G loss: 8.923124]\n",
            "********* 2983 [D loss: 0.028244, acc:, 99.22] [G loss: 9.526412]\n",
            "********* 2984 [D loss: 0.061258, acc:, 98.44] [G loss: 9.523587]\n",
            "********* 2985 [D loss: 0.031509, acc:, 98.44] [G loss: 8.140856]\n",
            "********* 2986 [D loss: 0.021724, acc:, 99.22] [G loss: 6.553330]\n",
            "********* 2987 [D loss: 0.041202, acc:, 100.00] [G loss: 6.249249]\n",
            "********* 2988 [D loss: 0.041918, acc:, 98.44] [G loss: 7.926445]\n",
            "********* 2989 [D loss: 0.075435, acc:, 96.88] [G loss: 7.543999]\n",
            "********* 2990 [D loss: 0.071744, acc:, 96.09] [G loss: 8.636107]\n",
            "********* 2991 [D loss: 0.047049, acc:, 97.66] [G loss: 9.063882]\n",
            "********* 2992 [D loss: 0.056962, acc:, 98.44] [G loss: 7.864564]\n",
            "********* 2993 [D loss: 0.092391, acc:, 96.88] [G loss: 7.422086]\n",
            "********* 2994 [D loss: 0.011364, acc:, 100.00] [G loss: 9.740292]\n",
            "********* 2995 [D loss: 0.013033, acc:, 100.00] [G loss: 10.741220]\n",
            "********* 2996 [D loss: 0.078064, acc:, 96.88] [G loss: 8.680623]\n",
            "********* 2997 [D loss: 0.053216, acc:, 99.22] [G loss: 6.824074]\n",
            "********* 2998 [D loss: 0.143025, acc:, 93.75] [G loss: 5.678842]\n",
            "********* 2999 [D loss: 0.090174, acc:, 98.44] [G loss: 8.080473]\n",
            "********* 3000 [D loss: 0.039179, acc:, 98.44] [G loss: 11.760366]\n",
            "********* 3001 [D loss: 0.123989, acc:, 96.09] [G loss: 9.531056]\n",
            "********* 3002 [D loss: 0.125878, acc:, 96.88] [G loss: 9.569416]\n",
            "********* 3003 [D loss: 0.029261, acc:, 98.44] [G loss: 10.979855]\n",
            "********* 3004 [D loss: 0.023993, acc:, 99.22] [G loss: 10.141755]\n",
            "********* 3005 [D loss: 0.176667, acc:, 93.75] [G loss: 8.353902]\n",
            "********* 3006 [D loss: 0.093922, acc:, 96.09] [G loss: 9.121248]\n",
            "********* 3007 [D loss: 0.113784, acc:, 97.66] [G loss: 13.343954]\n",
            "********* 3008 [D loss: 0.073264, acc:, 98.44] [G loss: 12.955124]\n",
            "********* 3009 [D loss: 0.176918, acc:, 94.53] [G loss: 5.708820]\n",
            "********* 3010 [D loss: 0.215118, acc:, 91.41] [G loss: 6.850881]\n",
            "********* 3011 [D loss: 0.004262, acc:, 100.00] [G loss: 13.992560]\n",
            "********* 3012 [D loss: 0.022409, acc:, 98.44] [G loss: 17.671198]\n",
            "********* 3013 [D loss: 0.428659, acc:, 89.84] [G loss: 9.817755]\n",
            "********* 3014 [D loss: 0.079682, acc:, 96.88] [G loss: 5.258659]\n",
            "********* 3015 [D loss: 0.222975, acc:, 94.53] [G loss: 7.953773]\n",
            "********* 3016 [D loss: 0.059589, acc:, 98.44] [G loss: 13.599973]\n",
            "********* 3017 [D loss: 0.150858, acc:, 95.31] [G loss: 11.471950]\n",
            "********* 3018 [D loss: 0.043562, acc:, 99.22] [G loss: 8.869254]\n",
            "********* 3019 [D loss: 0.035205, acc:, 98.44] [G loss: 8.815500]\n",
            "********* 3020 [D loss: 0.028761, acc:, 98.44] [G loss: 9.043406]\n",
            "********* 3021 [D loss: 0.007977, acc:, 100.00] [G loss: 10.368218]\n",
            "********* 3022 [D loss: 0.092980, acc:, 96.09] [G loss: 9.803565]\n",
            "********* 3023 [D loss: 0.038562, acc:, 99.22] [G loss: 8.749678]\n",
            "********* 3024 [D loss: 0.037227, acc:, 99.22] [G loss: 8.075496]\n",
            "********* 3025 [D loss: 0.011109, acc:, 100.00] [G loss: 8.102809]\n",
            "********* 3026 [D loss: 0.014481, acc:, 100.00] [G loss: 8.949732]\n",
            "********* 3027 [D loss: 0.021414, acc:, 99.22] [G loss: 8.933893]\n",
            "********* 3028 [D loss: 0.030533, acc:, 98.44] [G loss: 8.915553]\n",
            "********* 3029 [D loss: 0.011982, acc:, 99.22] [G loss: 9.999277]\n",
            "********* 3030 [D loss: 0.057316, acc:, 96.88] [G loss: 7.982109]\n",
            "********* 3031 [D loss: 0.029440, acc:, 98.44] [G loss: 6.017820]\n",
            "********* 3032 [D loss: 0.042555, acc:, 98.44] [G loss: 6.919997]\n",
            "********* 3033 [D loss: 0.018341, acc:, 99.22] [G loss: 7.993837]\n",
            "********* 3034 [D loss: 0.006913, acc:, 100.00] [G loss: 9.630144]\n",
            "********* 3035 [D loss: 0.016422, acc:, 100.00] [G loss: 9.823809]\n",
            "********* 3036 [D loss: 0.090261, acc:, 96.88] [G loss: 6.648031]\n",
            "********* 3037 [D loss: 0.055516, acc:, 99.22] [G loss: 5.242086]\n",
            "********* 3038 [D loss: 0.060518, acc:, 96.88] [G loss: 7.671897]\n",
            "********* 3039 [D loss: 0.010332, acc:, 100.00] [G loss: 11.262089]\n",
            "********* 3040 [D loss: 0.034039, acc:, 98.44] [G loss: 11.381377]\n",
            "********* 3041 [D loss: 0.032082, acc:, 98.44] [G loss: 8.994621]\n",
            "********* 3042 [D loss: 0.030284, acc:, 98.44] [G loss: 7.575024]\n",
            "********* 3043 [D loss: 0.023459, acc:, 100.00] [G loss: 7.531542]\n",
            "********* 3044 [D loss: 0.036341, acc:, 99.22] [G loss: 7.273802]\n",
            "********* 3045 [D loss: 0.030525, acc:, 99.22] [G loss: 7.910728]\n",
            "********* 3046 [D loss: 0.033213, acc:, 98.44] [G loss: 6.276344]\n",
            "********* 3047 [D loss: 0.030466, acc:, 99.22] [G loss: 5.649108]\n",
            "********* 3048 [D loss: 0.043320, acc:, 99.22] [G loss: 7.198551]\n",
            "********* 3049 [D loss: 0.006646, acc:, 100.00] [G loss: 8.829861]\n",
            "********* 3050 [D loss: 0.020525, acc:, 99.22] [G loss: 10.153576]\n",
            "********* 3051 [D loss: 0.021848, acc:, 99.22] [G loss: 10.106401]\n",
            "********* 3052 [D loss: 0.036680, acc:, 98.44] [G loss: 8.080935]\n",
            "********* 3053 [D loss: 0.068099, acc:, 96.88] [G loss: 8.643301]\n",
            "********* 3054 [D loss: 0.036856, acc:, 98.44] [G loss: 9.319214]\n",
            "********* 3055 [D loss: 0.004133, acc:, 100.00] [G loss: 10.454344]\n",
            "********* 3056 [D loss: 0.054340, acc:, 96.88] [G loss: 9.379622]\n",
            "********* 3057 [D loss: 0.064777, acc:, 98.44] [G loss: 6.661456]\n",
            "********* 3058 [D loss: 0.019032, acc:, 100.00] [G loss: 5.847086]\n",
            "********* 3059 [D loss: 0.009446, acc:, 100.00] [G loss: 6.711044]\n",
            "********* 3060 [D loss: 0.062586, acc:, 96.09] [G loss: 8.470238]\n",
            "********* 3061 [D loss: 0.013125, acc:, 99.22] [G loss: 10.425029]\n",
            "********* 3062 [D loss: 0.010446, acc:, 100.00] [G loss: 11.427777]\n",
            "********* 3063 [D loss: 0.045217, acc:, 98.44] [G loss: 8.553374]\n",
            "********* 3064 [D loss: 0.136355, acc:, 96.88] [G loss: 5.553529]\n",
            "********* 3065 [D loss: 0.044726, acc:, 99.22] [G loss: 7.073296]\n",
            "********* 3066 [D loss: 0.013531, acc:, 100.00] [G loss: 9.233229]\n",
            "********* 3067 [D loss: 0.078415, acc:, 96.09] [G loss: 8.509091]\n",
            "********* 3068 [D loss: 0.076264, acc:, 96.88] [G loss: 5.560311]\n",
            "********* 3069 [D loss: 0.048794, acc:, 98.44] [G loss: 6.415848]\n",
            "********* 3070 [D loss: 0.005816, acc:, 100.00] [G loss: 9.992817]\n",
            "********* 3071 [D loss: 0.007022, acc:, 100.00] [G loss: 12.858352]\n",
            "********* 3072 [D loss: 0.047470, acc:, 97.66] [G loss: 11.888510]\n",
            "********* 3073 [D loss: 0.062626, acc:, 97.66] [G loss: 7.605107]\n",
            "********* 3074 [D loss: 0.072496, acc:, 97.66] [G loss: 6.003299]\n",
            "********* 3075 [D loss: 0.064831, acc:, 97.66] [G loss: 8.132637]\n",
            "********* 3076 [D loss: 0.008267, acc:, 100.00] [G loss: 10.812519]\n",
            "********* 3077 [D loss: 0.005497, acc:, 100.00] [G loss: 12.948775]\n",
            "********* 3078 [D loss: 0.030463, acc:, 98.44] [G loss: 10.980824]\n",
            "********* 3079 [D loss: 0.015419, acc:, 99.22] [G loss: 9.388172]\n",
            "********* 3080 [D loss: 0.066465, acc:, 98.44] [G loss: 8.536510]\n",
            "********* 3081 [D loss: 0.037524, acc:, 98.44] [G loss: 8.746592]\n",
            "********* 3082 [D loss: 0.043303, acc:, 98.44] [G loss: 7.975495]\n",
            "********* 3083 [D loss: 0.008071, acc:, 100.00] [G loss: 9.011474]\n",
            "********* 3084 [D loss: 0.014554, acc:, 99.22] [G loss: 9.510954]\n",
            "********* 3085 [D loss: 0.086046, acc:, 97.66] [G loss: 7.110501]\n",
            "********* 3086 [D loss: 0.033796, acc:, 99.22] [G loss: 7.895774]\n",
            "********* 3087 [D loss: 0.046644, acc:, 98.44] [G loss: 8.489893]\n",
            "********* 3088 [D loss: 0.018775, acc:, 99.22] [G loss: 10.127262]\n",
            "********* 3089 [D loss: 0.004999, acc:, 100.00] [G loss: 9.681479]\n",
            "********* 3090 [D loss: 0.017423, acc:, 99.22] [G loss: 9.480297]\n",
            "********* 3091 [D loss: 0.057999, acc:, 97.66] [G loss: 7.847185]\n",
            "********* 3092 [D loss: 0.015653, acc:, 100.00] [G loss: 7.441734]\n",
            "********* 3093 [D loss: 0.025357, acc:, 100.00] [G loss: 7.438122]\n",
            "********* 3094 [D loss: 0.003572, acc:, 100.00] [G loss: 9.226046]\n",
            "********* 3095 [D loss: 0.008299, acc:, 100.00] [G loss: 9.631014]\n",
            "********* 3096 [D loss: 0.035721, acc:, 98.44] [G loss: 8.574558]\n",
            "********* 3097 [D loss: 0.034283, acc:, 99.22] [G loss: 7.666953]\n",
            "********* 3098 [D loss: 0.017683, acc:, 99.22] [G loss: 10.025061]\n",
            "********* 3099 [D loss: 0.016619, acc:, 99.22] [G loss: 9.792408]\n",
            "********* 3100 [D loss: 0.046332, acc:, 98.44] [G loss: 8.344025]\n",
            "********* 3101 [D loss: 0.014933, acc:, 100.00] [G loss: 7.668774]\n",
            "********* 3102 [D loss: 0.056817, acc:, 98.44] [G loss: 7.311466]\n",
            "********* 3103 [D loss: 0.015207, acc:, 100.00] [G loss: 7.914017]\n",
            "********* 3104 [D loss: 0.050502, acc:, 99.22] [G loss: 8.834090]\n",
            "********* 3105 [D loss: 0.088089, acc:, 96.88] [G loss: 7.157519]\n",
            "********* 3106 [D loss: 0.039571, acc:, 97.66] [G loss: 7.339277]\n",
            "********* 3107 [D loss: 0.017735, acc:, 99.22] [G loss: 9.772829]\n",
            "********* 3108 [D loss: 0.064855, acc:, 96.88] [G loss: 7.621327]\n",
            "********* 3109 [D loss: 0.022530, acc:, 100.00] [G loss: 6.476660]\n",
            "********* 3110 [D loss: 0.026238, acc:, 99.22] [G loss: 7.329501]\n",
            "********* 3111 [D loss: 0.007322, acc:, 100.00] [G loss: 8.868314]\n",
            "********* 3112 [D loss: 0.001377, acc:, 100.00] [G loss: 10.851757]\n",
            "********* 3113 [D loss: 0.046306, acc:, 99.22] [G loss: 9.836564]\n",
            "********* 3114 [D loss: 0.012114, acc:, 100.00] [G loss: 8.131567]\n",
            "********* 3115 [D loss: 0.021119, acc:, 99.22] [G loss: 7.069542]\n",
            "********* 3116 [D loss: 0.020561, acc:, 99.22] [G loss: 7.718534]\n",
            "********* 3117 [D loss: 0.017623, acc:, 99.22] [G loss: 8.421992]\n",
            "********* 3118 [D loss: 0.006881, acc:, 100.00] [G loss: 9.550165]\n",
            "********* 3119 [D loss: 0.034486, acc:, 99.22] [G loss: 9.236370]\n",
            "********* 3120 [D loss: 0.056569, acc:, 99.22] [G loss: 9.945543]\n",
            "********* 3121 [D loss: 0.069451, acc:, 96.88] [G loss: 7.402394]\n",
            "********* 3122 [D loss: 0.026832, acc:, 100.00] [G loss: 7.128800]\n",
            "********* 3123 [D loss: 0.017270, acc:, 100.00] [G loss: 8.652016]\n",
            "********* 3124 [D loss: 0.026494, acc:, 100.00] [G loss: 7.081753]\n",
            "********* 3125 [D loss: 0.047065, acc:, 97.66] [G loss: 8.047283]\n",
            "********* 3126 [D loss: 0.010532, acc:, 100.00] [G loss: 8.079953]\n",
            "********* 3127 [D loss: 0.014536, acc:, 99.22] [G loss: 8.165435]\n",
            "********* 3128 [D loss: 0.009538, acc:, 99.22] [G loss: 8.816433]\n",
            "********* 3129 [D loss: 0.011774, acc:, 100.00] [G loss: 9.774537]\n",
            "********* 3130 [D loss: 0.008582, acc:, 100.00] [G loss: 9.250405]\n",
            "********* 3131 [D loss: 0.034578, acc:, 98.44] [G loss: 7.213230]\n",
            "********* 3132 [D loss: 0.031457, acc:, 99.22] [G loss: 8.399090]\n",
            "********* 3133 [D loss: 0.021483, acc:, 99.22] [G loss: 9.489435]\n",
            "********* 3134 [D loss: 0.018103, acc:, 99.22] [G loss: 9.909021]\n",
            "********* 3135 [D loss: 0.014764, acc:, 100.00] [G loss: 9.249598]\n",
            "********* 3136 [D loss: 0.025491, acc:, 98.44] [G loss: 7.847779]\n",
            "********* 3137 [D loss: 0.025130, acc:, 99.22] [G loss: 7.178516]\n",
            "********* 3138 [D loss: 0.039827, acc:, 99.22] [G loss: 9.459892]\n",
            "********* 3139 [D loss: 0.005069, acc:, 100.00] [G loss: 12.920037]\n",
            "********* 3140 [D loss: 0.006950, acc:, 100.00] [G loss: 14.503790]\n",
            "********* 3141 [D loss: 0.239163, acc:, 95.31] [G loss: 5.264614]\n",
            "********* 3142 [D loss: 0.675886, acc:, 78.12] [G loss: 15.093898]\n",
            "********* 3143 [D loss: 0.081930, acc:, 96.88] [G loss: 39.589935]\n",
            "********* 3144 [D loss: 3.775373, acc:, 64.06] [G loss: 4.258839]\n",
            "********* 3145 [D loss: 2.420225, acc:, 63.28] [G loss: 2.079130]\n",
            "********* 3146 [D loss: 0.895640, acc:, 78.12] [G loss: 10.474643]\n",
            "********* 3147 [D loss: 0.128091, acc:, 95.31] [G loss: 24.957783]\n",
            "********* 3148 [D loss: 1.611742, acc:, 78.91] [G loss: 21.170355]\n",
            "********* 3149 [D loss: 0.558071, acc:, 92.19] [G loss: 16.075378]\n",
            "********* 3150 [D loss: 0.231454, acc:, 95.31] [G loss: 10.758133]\n",
            "********* 3151 [D loss: 0.180656, acc:, 92.97] [G loss: 8.058698]\n",
            "********* 3152 [D loss: 0.168489, acc:, 94.53] [G loss: 7.837405]\n",
            "********* 3153 [D loss: 0.215616, acc:, 96.88] [G loss: 8.386347]\n",
            "********* 3154 [D loss: 0.057274, acc:, 99.22] [G loss: 9.706775]\n",
            "********* 3155 [D loss: 0.048576, acc:, 98.44] [G loss: 10.862598]\n",
            "********* 3156 [D loss: 0.066185, acc:, 97.66] [G loss: 10.545277]\n",
            "********* 3157 [D loss: 0.046982, acc:, 99.22] [G loss: 10.429312]\n",
            "********* 3158 [D loss: 0.066022, acc:, 98.44] [G loss: 10.272867]\n",
            "********* 3159 [D loss: 0.004000, acc:, 100.00] [G loss: 11.144709]\n",
            "********* 3160 [D loss: 0.017211, acc:, 100.00] [G loss: 10.186714]\n",
            "********* 3161 [D loss: 0.007738, acc:, 100.00] [G loss: 10.007403]\n",
            "********* 3162 [D loss: 0.012287, acc:, 100.00] [G loss: 9.245070]\n",
            "********* 3163 [D loss: 0.014124, acc:, 100.00] [G loss: 10.099916]\n",
            "********* 3164 [D loss: 0.017781, acc:, 100.00] [G loss: 10.098026]\n",
            "********* 3165 [D loss: 0.023601, acc:, 99.22] [G loss: 10.682980]\n",
            "********* 3166 [D loss: 0.094364, acc:, 98.44] [G loss: 8.736603]\n",
            "********* 3167 [D loss: 0.076446, acc:, 97.66] [G loss: 9.292375]\n",
            "********* 3168 [D loss: 0.036478, acc:, 98.44] [G loss: 9.318468]\n",
            "********* 3169 [D loss: 0.053421, acc:, 98.44] [G loss: 8.849976]\n",
            "********* 3170 [D loss: 0.093446, acc:, 96.09] [G loss: 9.046204]\n",
            "********* 3171 [D loss: 0.135415, acc:, 95.31] [G loss: 10.124481]\n",
            "********* 3172 [D loss: 0.064501, acc:, 98.44] [G loss: 8.127577]\n",
            "********* 3173 [D loss: 0.035788, acc:, 98.44] [G loss: 5.943855]\n",
            "********* 3174 [D loss: 0.074391, acc:, 99.22] [G loss: 5.916194]\n",
            "********* 3175 [D loss: 0.126144, acc:, 94.53] [G loss: 7.243090]\n",
            "********* 3176 [D loss: 0.107436, acc:, 98.44] [G loss: 8.353454]\n",
            "********* 3177 [D loss: 0.035470, acc:, 98.44] [G loss: 8.114269]\n",
            "********* 3178 [D loss: 0.105923, acc:, 95.31] [G loss: 6.449237]\n",
            "********* 3179 [D loss: 0.038709, acc:, 99.22] [G loss: 5.076077]\n",
            "********* 3180 [D loss: 0.075052, acc:, 99.22] [G loss: 4.869666]\n",
            "********* 3181 [D loss: 0.060592, acc:, 97.66] [G loss: 6.880648]\n",
            "********* 3182 [D loss: 0.022320, acc:, 99.22] [G loss: 8.542612]\n",
            "********* 3183 [D loss: 0.066101, acc:, 96.88] [G loss: 8.091253]\n",
            "********* 3184 [D loss: 0.050402, acc:, 98.44] [G loss: 6.384604]\n",
            "********* 3185 [D loss: 0.025892, acc:, 100.00] [G loss: 4.974263]\n",
            "********* 3186 [D loss: 0.084733, acc:, 98.44] [G loss: 4.812289]\n",
            "********* 3187 [D loss: 0.039021, acc:, 99.22] [G loss: 6.214386]\n",
            "********* 3188 [D loss: 0.018335, acc:, 100.00] [G loss: 7.891839]\n",
            "********* 3189 [D loss: 0.118265, acc:, 96.09] [G loss: 6.221326]\n",
            "********* 3190 [D loss: 0.055839, acc:, 99.22] [G loss: 4.787204]\n",
            "********* 3191 [D loss: 0.035191, acc:, 99.22] [G loss: 5.889889]\n",
            "********* 3192 [D loss: 0.040056, acc:, 99.22] [G loss: 6.435854]\n",
            "********* 3193 [D loss: 0.116780, acc:, 96.09] [G loss: 6.698800]\n",
            "********* 3194 [D loss: 0.090621, acc:, 96.88] [G loss: 5.140730]\n",
            "********* 3195 [D loss: 0.104030, acc:, 95.31] [G loss: 5.609233]\n",
            "********* 3196 [D loss: 0.031921, acc:, 99.22] [G loss: 7.088233]\n",
            "********* 3197 [D loss: 0.065272, acc:, 98.44] [G loss: 7.888781]\n",
            "********* 3198 [D loss: 0.029108, acc:, 98.44] [G loss: 7.371643]\n",
            "********* 3199 [D loss: 0.059583, acc:, 96.88] [G loss: 5.713559]\n",
            "********* 3200 [D loss: 0.040376, acc:, 98.44] [G loss: 6.078014]\n",
            "********* 3201 [D loss: 0.022385, acc:, 100.00] [G loss: 6.246791]\n",
            "********* 3202 [D loss: 0.017750, acc:, 100.00] [G loss: 6.782677]\n",
            "********* 3203 [D loss: 0.136702, acc:, 98.44] [G loss: 7.052877]\n",
            "********* 3204 [D loss: 0.058292, acc:, 98.44] [G loss: 6.327181]\n",
            "********* 3205 [D loss: 0.038775, acc:, 99.22] [G loss: 6.224123]\n",
            "********* 3206 [D loss: 0.065458, acc:, 98.44] [G loss: 5.540128]\n",
            "********* 3207 [D loss: 0.069467, acc:, 96.09] [G loss: 6.883127]\n",
            "********* 3208 [D loss: 0.085283, acc:, 96.88] [G loss: 6.705586]\n",
            "********* 3209 [D loss: 0.119914, acc:, 96.09] [G loss: 5.798884]\n",
            "********* 3210 [D loss: 0.047514, acc:, 99.22] [G loss: 5.156226]\n",
            "********* 3211 [D loss: 0.017005, acc:, 100.00] [G loss: 5.893182]\n",
            "********* 3212 [D loss: 0.036306, acc:, 99.22] [G loss: 6.142694]\n",
            "********* 3213 [D loss: 0.096944, acc:, 96.88] [G loss: 7.010030]\n",
            "********* 3214 [D loss: 0.038992, acc:, 99.22] [G loss: 8.080388]\n",
            "********* 3215 [D loss: 0.129294, acc:, 96.88] [G loss: 5.382928]\n",
            "********* 3216 [D loss: 0.052567, acc:, 98.44] [G loss: 4.992585]\n",
            "********* 3217 [D loss: 0.027715, acc:, 99.22] [G loss: 6.182246]\n",
            "********* 3218 [D loss: 0.032640, acc:, 98.44] [G loss: 7.911572]\n",
            "********* 3219 [D loss: 0.050032, acc:, 98.44] [G loss: 6.815615]\n",
            "********* 3220 [D loss: 0.063707, acc:, 98.44] [G loss: 5.307027]\n",
            "********* 3221 [D loss: 0.013576, acc:, 100.00] [G loss: 6.390162]\n",
            "********* 3222 [D loss: 0.022602, acc:, 99.22] [G loss: 7.082787]\n",
            "********* 3223 [D loss: 0.022852, acc:, 100.00] [G loss: 6.919034]\n",
            "********* 3224 [D loss: 0.045861, acc:, 99.22] [G loss: 5.717426]\n",
            "********* 3225 [D loss: 0.034754, acc:, 99.22] [G loss: 6.206464]\n",
            "********* 3226 [D loss: 0.018544, acc:, 100.00] [G loss: 5.966352]\n",
            "********* 3227 [D loss: 0.032638, acc:, 99.22] [G loss: 6.507570]\n",
            "********* 3228 [D loss: 0.042908, acc:, 97.66] [G loss: 5.220959]\n",
            "********* 3229 [D loss: 0.044737, acc:, 99.22] [G loss: 5.736368]\n",
            "********* 3230 [D loss: 0.011461, acc:, 100.00] [G loss: 5.827923]\n",
            "********* 3231 [D loss: 0.052871, acc:, 98.44] [G loss: 6.022581]\n",
            "********* 3232 [D loss: 0.033356, acc:, 100.00] [G loss: 7.149560]\n",
            "********* 3233 [D loss: 0.025693, acc:, 99.22] [G loss: 7.541841]\n",
            "********* 3234 [D loss: 0.027718, acc:, 98.44] [G loss: 8.044282]\n",
            "********* 3235 [D loss: 0.019159, acc:, 100.00] [G loss: 7.212071]\n",
            "********* 3236 [D loss: 0.022702, acc:, 100.00] [G loss: 6.182934]\n",
            "********* 3237 [D loss: 0.036228, acc:, 98.44] [G loss: 6.419885]\n",
            "********* 3238 [D loss: 0.037075, acc:, 98.44] [G loss: 6.543724]\n",
            "********* 3239 [D loss: 0.036564, acc:, 98.44] [G loss: 6.934141]\n",
            "********* 3240 [D loss: 0.015982, acc:, 100.00] [G loss: 7.343717]\n",
            "********* 3241 [D loss: 0.016751, acc:, 100.00] [G loss: 8.062101]\n",
            "********* 3242 [D loss: 0.022151, acc:, 100.00] [G loss: 7.977797]\n",
            "********* 3243 [D loss: 0.039790, acc:, 99.22] [G loss: 8.013311]\n",
            "********* 3244 [D loss: 0.076920, acc:, 96.88] [G loss: 5.801244]\n",
            "********* 3245 [D loss: 0.038194, acc:, 98.44] [G loss: 6.428785]\n",
            "********* 3246 [D loss: 0.031766, acc:, 98.44] [G loss: 7.607653]\n",
            "********* 3247 [D loss: 0.014405, acc:, 100.00] [G loss: 6.909618]\n",
            "********* 3248 [D loss: 0.034651, acc:, 98.44] [G loss: 6.556690]\n",
            "********* 3249 [D loss: 0.028990, acc:, 99.22] [G loss: 7.864590]\n",
            "********* 3250 [D loss: 0.048628, acc:, 98.44] [G loss: 10.188250]\n",
            "********* 3251 [D loss: 0.069675, acc:, 97.66] [G loss: 7.269449]\n",
            "********* 3252 [D loss: 0.018435, acc:, 100.00] [G loss: 6.625066]\n",
            "********* 3253 [D loss: 0.018954, acc:, 100.00] [G loss: 7.176379]\n",
            "********* 3254 [D loss: 0.007093, acc:, 100.00] [G loss: 8.651365]\n",
            "********* 3255 [D loss: 0.015239, acc:, 100.00] [G loss: 9.277660]\n",
            "********* 3256 [D loss: 0.045995, acc:, 99.22] [G loss: 6.344567]\n",
            "********* 3257 [D loss: 0.046316, acc:, 99.22] [G loss: 6.076539]\n",
            "********* 3258 [D loss: 0.007467, acc:, 100.00] [G loss: 8.477823]\n",
            "********* 3259 [D loss: 0.017486, acc:, 100.00] [G loss: 9.341541]\n",
            "********* 3260 [D loss: 0.012650, acc:, 100.00] [G loss: 9.422300]\n",
            "********* 3261 [D loss: 0.013773, acc:, 100.00] [G loss: 9.209993]\n",
            "********* 3262 [D loss: 0.015059, acc:, 100.00] [G loss: 7.176810]\n",
            "********* 3263 [D loss: 0.041027, acc:, 99.22] [G loss: 5.631175]\n",
            "********* 3264 [D loss: 0.041102, acc:, 98.44] [G loss: 7.908107]\n",
            "********* 3265 [D loss: 0.031184, acc:, 99.22] [G loss: 11.023125]\n",
            "********* 3266 [D loss: 0.052611, acc:, 97.66] [G loss: 7.630682]\n",
            "********* 3267 [D loss: 0.015324, acc:, 100.00] [G loss: 7.090205]\n",
            "********* 3268 [D loss: 0.029766, acc:, 99.22] [G loss: 6.406593]\n",
            "********* 3269 [D loss: 0.023689, acc:, 99.22] [G loss: 9.438112]\n",
            "********* 3270 [D loss: 0.061002, acc:, 97.66] [G loss: 6.281199]\n",
            "********* 3271 [D loss: 0.052475, acc:, 97.66] [G loss: 6.110586]\n",
            "********* 3272 [D loss: 0.002937, acc:, 100.00] [G loss: 8.676107]\n",
            "********* 3273 [D loss: 0.013087, acc:, 100.00] [G loss: 9.519099]\n",
            "********* 3274 [D loss: 0.035567, acc:, 99.22] [G loss: 8.109797]\n",
            "********* 3275 [D loss: 0.016967, acc:, 100.00] [G loss: 6.255846]\n",
            "********* 3276 [D loss: 0.026745, acc:, 99.22] [G loss: 8.203989]\n",
            "********* 3277 [D loss: 0.001906, acc:, 100.00] [G loss: 9.555616]\n",
            "********* 3278 [D loss: 0.010121, acc:, 100.00] [G loss: 9.499831]\n",
            "********* 3279 [D loss: 0.015542, acc:, 100.00] [G loss: 8.677805]\n",
            "********* 3280 [D loss: 0.009146, acc:, 100.00] [G loss: 6.961692]\n",
            "********* 3281 [D loss: 0.042533, acc:, 98.44] [G loss: 7.993422]\n",
            "********* 3282 [D loss: 0.022331, acc:, 99.22] [G loss: 9.713688]\n",
            "********* 3283 [D loss: 0.083800, acc:, 98.44] [G loss: 8.617295]\n",
            "********* 3284 [D loss: 0.052173, acc:, 97.66] [G loss: 5.844382]\n",
            "********* 3285 [D loss: 0.055670, acc:, 97.66] [G loss: 7.910313]\n",
            "********* 3286 [D loss: 0.004014, acc:, 100.00] [G loss: 11.987595]\n",
            "********* 3287 [D loss: 0.191963, acc:, 90.62] [G loss: 6.499378]\n",
            "********* 3288 [D loss: 0.078784, acc:, 98.44] [G loss: 7.428036]\n",
            "********* 3289 [D loss: 0.004565, acc:, 100.00] [G loss: 12.599838]\n",
            "********* 3290 [D loss: 0.020848, acc:, 99.22] [G loss: 16.308638]\n",
            "********* 3291 [D loss: 0.026092, acc:, 99.22] [G loss: 17.528038]\n",
            "********* 3292 [D loss: 0.075991, acc:, 98.44] [G loss: 12.654789]\n",
            "********* 3293 [D loss: 0.083626, acc:, 97.66] [G loss: 8.283191]\n",
            "********* 3294 [D loss: 0.039618, acc:, 99.22] [G loss: 5.808879]\n",
            "********* 3295 [D loss: 0.011522, acc:, 100.00] [G loss: 7.970391]\n",
            "********* 3296 [D loss: 0.032012, acc:, 99.22] [G loss: 9.067087]\n",
            "********* 3297 [D loss: 0.004327, acc:, 100.00] [G loss: 10.821093]\n",
            "********* 3298 [D loss: 0.003473, acc:, 100.00] [G loss: 11.609953]\n",
            "********* 3299 [D loss: 0.013013, acc:, 99.22] [G loss: 12.743717]\n",
            "********* 3300 [D loss: 0.041309, acc:, 98.44] [G loss: 10.950539]\n",
            "********* 3301 [D loss: 0.040937, acc:, 99.22] [G loss: 8.435595]\n",
            "********* 3302 [D loss: 0.012947, acc:, 99.22] [G loss: 6.571369]\n",
            "********* 3303 [D loss: 0.025146, acc:, 98.44] [G loss: 7.742909]\n",
            "********* 3304 [D loss: 0.013510, acc:, 100.00] [G loss: 8.911881]\n",
            "********* 3305 [D loss: 0.023713, acc:, 99.22] [G loss: 9.966537]\n",
            "********* 3306 [D loss: 0.015863, acc:, 99.22] [G loss: 10.201280]\n",
            "********* 3307 [D loss: 0.018739, acc:, 99.22] [G loss: 8.860405]\n",
            "********* 3308 [D loss: 0.006325, acc:, 100.00] [G loss: 8.227919]\n",
            "********* 3309 [D loss: 0.012910, acc:, 100.00] [G loss: 7.255193]\n",
            "********* 3310 [D loss: 0.011707, acc:, 100.00] [G loss: 8.370588]\n",
            "********* 3311 [D loss: 0.007569, acc:, 100.00] [G loss: 8.386673]\n",
            "********* 3312 [D loss: 0.020634, acc:, 99.22] [G loss: 7.547385]\n",
            "********* 3313 [D loss: 0.016511, acc:, 100.00] [G loss: 6.829743]\n",
            "********* 3314 [D loss: 0.013174, acc:, 100.00] [G loss: 7.442794]\n",
            "********* 3315 [D loss: 0.006775, acc:, 100.00] [G loss: 8.421972]\n",
            "********* 3316 [D loss: 0.032079, acc:, 99.22] [G loss: 8.075702]\n",
            "********* 3317 [D loss: 0.034683, acc:, 99.22] [G loss: 7.281242]\n",
            "********* 3318 [D loss: 0.006599, acc:, 100.00] [G loss: 7.612919]\n",
            "********* 3319 [D loss: 0.091392, acc:, 96.88] [G loss: 6.603557]\n",
            "********* 3320 [D loss: 0.030904, acc:, 99.22] [G loss: 7.398932]\n",
            "********* 3321 [D loss: 0.053042, acc:, 97.66] [G loss: 6.729062]\n",
            "********* 3322 [D loss: 0.010499, acc:, 100.00] [G loss: 8.682159]\n",
            "********* 3323 [D loss: 0.027286, acc:, 99.22] [G loss: 8.699357]\n",
            "********* 3324 [D loss: 0.034782, acc:, 99.22] [G loss: 7.587821]\n",
            "********* 3325 [D loss: 0.011679, acc:, 100.00] [G loss: 7.202085]\n",
            "********* 3326 [D loss: 0.004943, acc:, 100.00] [G loss: 7.407955]\n",
            "********* 3327 [D loss: 0.040910, acc:, 98.44] [G loss: 6.509516]\n",
            "********* 3328 [D loss: 0.012831, acc:, 100.00] [G loss: 6.720999]\n",
            "********* 3329 [D loss: 0.009556, acc:, 100.00] [G loss: 7.485092]\n",
            "********* 3330 [D loss: 0.005059, acc:, 100.00] [G loss: 8.935500]\n",
            "********* 3331 [D loss: 0.005643, acc:, 100.00] [G loss: 10.243170]\n",
            "********* 3332 [D loss: 0.026584, acc:, 99.22] [G loss: 8.444006]\n",
            "********* 3333 [D loss: 0.087289, acc:, 96.09] [G loss: 6.394245]\n",
            "********* 3334 [D loss: 0.065507, acc:, 99.22] [G loss: 7.630358]\n",
            "********* 3335 [D loss: 0.059127, acc:, 97.66] [G loss: 11.355810]\n",
            "********* 3336 [D loss: 0.137109, acc:, 96.09] [G loss: 9.426162]\n",
            "********* 3337 [D loss: 0.015707, acc:, 100.00] [G loss: 7.521967]\n",
            "********* 3338 [D loss: 0.059677, acc:, 97.66] [G loss: 9.925636]\n",
            "********* 3339 [D loss: 0.011385, acc:, 99.22] [G loss: 14.977671]\n",
            "********* 3340 [D loss: 0.061082, acc:, 98.44] [G loss: 13.061539]\n",
            "********* 3341 [D loss: 0.158386, acc:, 92.19] [G loss: 7.484317]\n",
            "********* 3342 [D loss: 0.034975, acc:, 98.44] [G loss: 7.355071]\n",
            "********* 3343 [D loss: 0.039838, acc:, 98.44] [G loss: 9.859278]\n",
            "********* 3344 [D loss: 0.010814, acc:, 100.00] [G loss: 11.420630]\n",
            "********* 3345 [D loss: 0.025723, acc:, 99.22] [G loss: 12.723352]\n",
            "********* 3346 [D loss: 0.006899, acc:, 100.00] [G loss: 12.052393]\n",
            "********* 3347 [D loss: 0.138380, acc:, 96.09] [G loss: 5.808286]\n",
            "********* 3348 [D loss: 0.222266, acc:, 87.50] [G loss: 10.590651]\n",
            "********* 3349 [D loss: 0.056952, acc:, 97.66] [G loss: 25.207596]\n",
            "********* 3350 [D loss: 0.953187, acc:, 86.72] [G loss: 12.212949]\n",
            "********* 3351 [D loss: 0.037613, acc:, 98.44] [G loss: 5.918852]\n",
            "********* 3352 [D loss: 0.455792, acc:, 83.59] [G loss: 9.543289]\n",
            "********* 3353 [D loss: 0.014953, acc:, 99.22] [G loss: 17.277149]\n",
            "********* 3354 [D loss: 0.140213, acc:, 95.31] [G loss: 22.135509]\n",
            "********* 3355 [D loss: 0.307376, acc:, 92.97] [G loss: 18.535732]\n",
            "********* 3356 [D loss: 0.170425, acc:, 93.75] [G loss: 14.004728]\n",
            "********* 3357 [D loss: 0.229463, acc:, 89.84] [G loss: 10.682484]\n",
            "********* 3358 [D loss: 0.010480, acc:, 99.22] [G loss: 10.963026]\n",
            "********* 3359 [D loss: 0.003414, acc:, 100.00] [G loss: 11.541988]\n",
            "********* 3360 [D loss: 0.024682, acc:, 99.22] [G loss: 12.439156]\n",
            "********* 3361 [D loss: 0.015679, acc:, 100.00] [G loss: 13.641441]\n",
            "********* 3362 [D loss: 0.090144, acc:, 97.66] [G loss: 13.075453]\n",
            "********* 3363 [D loss: 0.107395, acc:, 94.53] [G loss: 11.535011]\n",
            "********* 3364 [D loss: 0.002566, acc:, 100.00] [G loss: 11.527443]\n",
            "********* 3365 [D loss: 0.090978, acc:, 96.88] [G loss: 12.199771]\n",
            "********* 3366 [D loss: 0.046629, acc:, 97.66] [G loss: 12.543812]\n",
            "********* 3367 [D loss: 0.021497, acc:, 98.44] [G loss: 12.853836]\n",
            "********* 3368 [D loss: 0.029418, acc:, 98.44] [G loss: 11.291550]\n",
            "********* 3369 [D loss: 0.013155, acc:, 100.00] [G loss: 9.291422]\n",
            "********* 3370 [D loss: 0.053039, acc:, 99.22] [G loss: 9.801416]\n",
            "********* 3371 [D loss: 0.100190, acc:, 95.31] [G loss: 8.286879]\n",
            "********* 3372 [D loss: 0.023527, acc:, 98.44] [G loss: 9.367922]\n",
            "********* 3373 [D loss: 0.051424, acc:, 96.88] [G loss: 9.029950]\n",
            "********* 3374 [D loss: 0.026461, acc:, 99.22] [G loss: 10.610909]\n",
            "********* 3375 [D loss: 0.034781, acc:, 98.44] [G loss: 10.877233]\n",
            "********* 3376 [D loss: 0.047250, acc:, 97.66] [G loss: 10.091716]\n",
            "********* 3377 [D loss: 0.016733, acc:, 100.00] [G loss: 8.420909]\n",
            "********* 3378 [D loss: 0.061539, acc:, 98.44] [G loss: 7.385406]\n",
            "********* 3379 [D loss: 0.043217, acc:, 97.66] [G loss: 8.647091]\n",
            "********* 3380 [D loss: 0.013631, acc:, 99.22] [G loss: 9.931957]\n",
            "********* 3381 [D loss: 0.025105, acc:, 99.22] [G loss: 11.476445]\n",
            "********* 3382 [D loss: 0.040484, acc:, 97.66] [G loss: 10.447025]\n",
            "********* 3383 [D loss: 0.075860, acc:, 99.22] [G loss: 8.665285]\n",
            "********* 3384 [D loss: 0.030010, acc:, 99.22] [G loss: 7.387946]\n",
            "********* 3385 [D loss: 0.061137, acc:, 97.66] [G loss: 7.495554]\n",
            "********* 3386 [D loss: 0.029903, acc:, 100.00] [G loss: 8.554173]\n",
            "********* 3387 [D loss: 0.023603, acc:, 99.22] [G loss: 11.129477]\n",
            "********* 3388 [D loss: 0.030253, acc:, 99.22] [G loss: 10.530018]\n",
            "********* 3389 [D loss: 0.052015, acc:, 98.44] [G loss: 8.047098]\n",
            "********* 3390 [D loss: 0.050723, acc:, 98.44] [G loss: 8.624039]\n",
            "********* 3391 [D loss: 0.033312, acc:, 99.22] [G loss: 8.969142]\n",
            "********* 3392 [D loss: 0.066124, acc:, 97.66] [G loss: 8.626431]\n",
            "********* 3393 [D loss: 0.051643, acc:, 98.44] [G loss: 8.029806]\n",
            "********* 3394 [D loss: 0.089730, acc:, 98.44] [G loss: 5.295534]\n",
            "********* 3395 [D loss: 0.064289, acc:, 97.66] [G loss: 5.235742]\n",
            "********* 3396 [D loss: 0.024169, acc:, 100.00] [G loss: 8.358654]\n",
            "********* 3397 [D loss: 0.015982, acc:, 100.00] [G loss: 10.045575]\n",
            "********* 3398 [D loss: 0.041998, acc:, 99.22] [G loss: 10.949443]\n",
            "********* 3399 [D loss: 0.168492, acc:, 96.09] [G loss: 7.887623]\n",
            "********* 3400 [D loss: 0.047325, acc:, 97.66] [G loss: 6.074768]\n",
            "********* 3401 [D loss: 0.026917, acc:, 100.00] [G loss: 5.461764]\n",
            "********* 3402 [D loss: 0.033689, acc:, 98.44] [G loss: 7.982655]\n",
            "********* 3403 [D loss: 0.005051, acc:, 100.00] [G loss: 10.369931]\n",
            "********* 3404 [D loss: 0.008160, acc:, 100.00] [G loss: 10.197733]\n",
            "********* 3405 [D loss: 0.048345, acc:, 97.66] [G loss: 9.203480]\n",
            "********* 3406 [D loss: 0.043573, acc:, 97.66] [G loss: 6.977216]\n",
            "********* 3407 [D loss: 0.066387, acc:, 96.88] [G loss: 6.238090]\n",
            "********* 3408 [D loss: 0.046447, acc:, 98.44] [G loss: 9.293649]\n",
            "********* 3409 [D loss: 0.049101, acc:, 96.88] [G loss: 10.103868]\n",
            "********* 3410 [D loss: 0.021795, acc:, 99.22] [G loss: 10.248757]\n",
            "********* 3411 [D loss: 0.056077, acc:, 97.66] [G loss: 7.427309]\n",
            "********* 3412 [D loss: 0.022580, acc:, 98.44] [G loss: 6.234856]\n",
            "********* 3413 [D loss: 0.022592, acc:, 100.00] [G loss: 5.835892]\n",
            "********* 3414 [D loss: 0.014374, acc:, 100.00] [G loss: 7.347017]\n",
            "********* 3415 [D loss: 0.035362, acc:, 99.22] [G loss: 7.992427]\n",
            "********* 3416 [D loss: 0.034090, acc:, 98.44] [G loss: 7.273240]\n",
            "********* 3417 [D loss: 0.025798, acc:, 99.22] [G loss: 5.897335]\n",
            "********* 3418 [D loss: 0.070569, acc:, 96.88] [G loss: 7.971755]\n",
            "********* 3419 [D loss: 0.033253, acc:, 98.44] [G loss: 9.858515]\n",
            "********* 3420 [D loss: 0.158210, acc:, 96.09] [G loss: 6.187349]\n",
            "********* 3421 [D loss: 0.133601, acc:, 94.53] [G loss: 7.070913]\n",
            "********* 3422 [D loss: 0.023837, acc:, 99.22] [G loss: 11.492359]\n",
            "********* 3423 [D loss: 0.064989, acc:, 97.66] [G loss: 12.636572]\n",
            "********* 3424 [D loss: 0.131800, acc:, 96.09] [G loss: 9.318170]\n",
            "********* 3425 [D loss: 0.115626, acc:, 94.53] [G loss: 8.493816]\n",
            "********* 3426 [D loss: 0.063996, acc:, 97.66] [G loss: 6.858367]\n",
            "********* 3427 [D loss: 0.044107, acc:, 99.22] [G loss: 5.828480]\n",
            "********* 3428 [D loss: 0.035421, acc:, 100.00] [G loss: 9.972984]\n",
            "********* 3429 [D loss: 0.024221, acc:, 99.22] [G loss: 11.634716]\n",
            "********* 3430 [D loss: 0.017341, acc:, 100.00] [G loss: 12.525928]\n",
            "********* 3431 [D loss: 0.015840, acc:, 100.00] [G loss: 12.829796]\n",
            "********* 3432 [D loss: 0.007347, acc:, 100.00] [G loss: 10.757448]\n",
            "********* 3433 [D loss: 0.054864, acc:, 99.22] [G loss: 10.230282]\n",
            "********* 3434 [D loss: 0.070078, acc:, 97.66] [G loss: 7.842146]\n",
            "********* 3435 [D loss: 0.039053, acc:, 98.44] [G loss: 8.450008]\n",
            "********* 3436 [D loss: 0.068575, acc:, 96.88] [G loss: 7.006392]\n",
            "********* 3437 [D loss: 0.007553, acc:, 100.00] [G loss: 9.088346]\n",
            "********* 3438 [D loss: 0.015216, acc:, 100.00] [G loss: 10.028819]\n",
            "********* 3439 [D loss: 0.023189, acc:, 99.22] [G loss: 10.852664]\n",
            "********* 3440 [D loss: 0.068273, acc:, 97.66] [G loss: 9.970827]\n",
            "********* 3441 [D loss: 0.043720, acc:, 98.44] [G loss: 7.683727]\n",
            "********* 3442 [D loss: 0.026356, acc:, 99.22] [G loss: 6.335066]\n",
            "********* 3443 [D loss: 0.066324, acc:, 98.44] [G loss: 8.706479]\n",
            "********* 3444 [D loss: 0.008326, acc:, 100.00] [G loss: 9.230515]\n",
            "********* 3445 [D loss: 0.037189, acc:, 99.22] [G loss: 10.079046]\n",
            "********* 3446 [D loss: 0.012967, acc:, 100.00] [G loss: 7.535303]\n",
            "********* 3447 [D loss: 0.036965, acc:, 99.22] [G loss: 7.732931]\n",
            "********* 3448 [D loss: 0.030400, acc:, 99.22] [G loss: 8.931084]\n",
            "********* 3449 [D loss: 0.038780, acc:, 99.22] [G loss: 10.838036]\n",
            "********* 3450 [D loss: 0.031231, acc:, 99.22] [G loss: 8.331456]\n",
            "********* 3451 [D loss: 0.047945, acc:, 97.66] [G loss: 7.286378]\n",
            "********* 3452 [D loss: 0.106440, acc:, 96.09] [G loss: 7.912536]\n",
            "********* 3453 [D loss: 0.006140, acc:, 100.00] [G loss: 10.847597]\n",
            "********* 3454 [D loss: 0.008069, acc:, 100.00] [G loss: 13.594810]\n",
            "********* 3455 [D loss: 0.225701, acc:, 92.97] [G loss: 7.106363]\n",
            "********* 3456 [D loss: 0.049143, acc:, 100.00] [G loss: 6.099143]\n",
            "********* 3457 [D loss: 0.009487, acc:, 100.00] [G loss: 8.881227]\n",
            "********* 3458 [D loss: 0.075818, acc:, 98.44] [G loss: 11.164011]\n",
            "********* 3459 [D loss: 0.047813, acc:, 97.66] [G loss: 11.370407]\n",
            "********* 3460 [D loss: 0.049651, acc:, 98.44] [G loss: 11.375045]\n",
            "********* 3461 [D loss: 0.043695, acc:, 98.44] [G loss: 9.211938]\n",
            "********* 3462 [D loss: 0.017864, acc:, 99.22] [G loss: 9.181559]\n",
            "********* 3463 [D loss: 0.047164, acc:, 97.66] [G loss: 9.665918]\n",
            "********* 3464 [D loss: 0.015250, acc:, 99.22] [G loss: 12.188277]\n",
            "********* 3465 [D loss: 0.009791, acc:, 99.22] [G loss: 12.742878]\n",
            "********* 3466 [D loss: 0.059830, acc:, 98.44] [G loss: 10.841162]\n",
            "********* 3467 [D loss: 0.040787, acc:, 97.66] [G loss: 8.017054]\n",
            "********* 3468 [D loss: 0.051319, acc:, 98.44] [G loss: 7.383608]\n",
            "********* 3469 [D loss: 0.009091, acc:, 100.00] [G loss: 7.070755]\n",
            "********* 3470 [D loss: 0.023485, acc:, 99.22] [G loss: 8.718921]\n",
            "********* 3471 [D loss: 0.005673, acc:, 100.00] [G loss: 9.526630]\n",
            "********* 3472 [D loss: 0.026592, acc:, 98.44] [G loss: 9.684427]\n",
            "********* 3473 [D loss: 0.012609, acc:, 100.00] [G loss: 9.266738]\n",
            "********* 3474 [D loss: 0.031079, acc:, 99.22] [G loss: 8.000209]\n",
            "********* 3475 [D loss: 0.026397, acc:, 98.44] [G loss: 7.283260]\n",
            "********* 3476 [D loss: 0.017193, acc:, 100.00] [G loss: 6.734365]\n",
            "********* 3477 [D loss: 0.065105, acc:, 97.66] [G loss: 6.262745]\n",
            "********* 3478 [D loss: 0.024442, acc:, 99.22] [G loss: 8.093354]\n",
            "********* 3479 [D loss: 0.016310, acc:, 100.00] [G loss: 9.217296]\n",
            "********* 3480 [D loss: 0.080692, acc:, 96.88] [G loss: 9.172527]\n",
            "********* 3481 [D loss: 0.012331, acc:, 100.00] [G loss: 8.186302]\n",
            "********* 3482 [D loss: 0.038714, acc:, 99.22] [G loss: 6.945134]\n",
            "********* 3483 [D loss: 0.012020, acc:, 100.00] [G loss: 6.832481]\n",
            "********* 3484 [D loss: 0.010345, acc:, 100.00] [G loss: 7.734936]\n",
            "********* 3485 [D loss: 0.013520, acc:, 100.00] [G loss: 9.075653]\n",
            "********* 3486 [D loss: 0.019518, acc:, 98.44] [G loss: 8.314119]\n",
            "********* 3487 [D loss: 0.009802, acc:, 100.00] [G loss: 7.679791]\n",
            "********* 3488 [D loss: 0.012947, acc:, 100.00] [G loss: 7.579243]\n",
            "********* 3489 [D loss: 0.017938, acc:, 99.22] [G loss: 8.303357]\n",
            "********* 3490 [D loss: 0.026091, acc:, 99.22] [G loss: 7.180715]\n",
            "********* 3491 [D loss: 0.028286, acc:, 98.44] [G loss: 7.466483]\n",
            "********* 3492 [D loss: 0.012198, acc:, 100.00] [G loss: 8.826830]\n",
            "********* 3493 [D loss: 0.012068, acc:, 100.00] [G loss: 8.148056]\n",
            "********* 3494 [D loss: 0.013352, acc:, 100.00] [G loss: 7.498144]\n",
            "********* 3495 [D loss: 0.062650, acc:, 97.66] [G loss: 6.504262]\n",
            "********* 3496 [D loss: 0.027391, acc:, 100.00] [G loss: 7.822451]\n",
            "********* 3497 [D loss: 0.021794, acc:, 99.22] [G loss: 8.786785]\n",
            "********* 3498 [D loss: 0.047091, acc:, 98.44] [G loss: 8.462202]\n",
            "********* 3499 [D loss: 0.128291, acc:, 97.66] [G loss: 6.918435]\n",
            "********* 3500 [D loss: 0.007775, acc:, 100.00] [G loss: 8.016417]\n",
            "********* 3501 [D loss: 0.020905, acc:, 99.22] [G loss: 7.682437]\n",
            "********* 3502 [D loss: 0.037466, acc:, 98.44] [G loss: 7.292338]\n",
            "********* 3503 [D loss: 0.017427, acc:, 100.00] [G loss: 7.189445]\n",
            "********* 3504 [D loss: 0.044241, acc:, 98.44] [G loss: 8.099874]\n",
            "********* 3505 [D loss: 0.019795, acc:, 99.22] [G loss: 9.862057]\n",
            "********* 3506 [D loss: 0.008916, acc:, 100.00] [G loss: 11.486677]\n",
            "********* 3507 [D loss: 0.030128, acc:, 98.44] [G loss: 9.674065]\n",
            "********* 3508 [D loss: 0.026447, acc:, 99.22] [G loss: 8.643502]\n",
            "********* 3509 [D loss: 0.012377, acc:, 100.00] [G loss: 7.152308]\n",
            "********* 3510 [D loss: 0.087059, acc:, 99.22] [G loss: 8.102388]\n",
            "********* 3511 [D loss: 0.031221, acc:, 99.22] [G loss: 7.378349]\n",
            "********* 3512 [D loss: 0.017183, acc:, 99.22] [G loss: 7.716692]\n",
            "********* 3513 [D loss: 0.046578, acc:, 98.44] [G loss: 9.122519]\n",
            "********* 3514 [D loss: 0.014739, acc:, 100.00] [G loss: 8.041039]\n",
            "********* 3515 [D loss: 0.015331, acc:, 100.00] [G loss: 7.506480]\n",
            "********* 3516 [D loss: 0.018186, acc:, 100.00] [G loss: 6.913816]\n",
            "********* 3517 [D loss: 0.035982, acc:, 99.22] [G loss: 9.404512]\n",
            "********* 3518 [D loss: 0.047977, acc:, 98.44] [G loss: 10.940889]\n",
            "********* 3519 [D loss: 0.014639, acc:, 100.00] [G loss: 10.042298]\n",
            "********* 3520 [D loss: 0.037688, acc:, 98.44] [G loss: 7.733477]\n",
            "********* 3521 [D loss: 0.012158, acc:, 100.00] [G loss: 6.336227]\n",
            "********* 3522 [D loss: 0.117805, acc:, 94.53] [G loss: 10.732697]\n",
            "********* 3523 [D loss: 0.054227, acc:, 96.09] [G loss: 17.394745]\n",
            "********* 3524 [D loss: 0.186128, acc:, 96.09] [G loss: 12.450083]\n",
            "********* 3525 [D loss: 0.117941, acc:, 94.53] [G loss: 7.719961]\n",
            "********* 3526 [D loss: 0.103482, acc:, 96.88] [G loss: 8.140804]\n",
            "********* 3527 [D loss: 0.004263, acc:, 100.00] [G loss: 10.514498]\n",
            "********* 3528 [D loss: 0.013926, acc:, 99.22] [G loss: 12.174688]\n",
            "********* 3529 [D loss: 0.150938, acc:, 95.31] [G loss: 9.556082]\n",
            "********* 3530 [D loss: 0.099488, acc:, 98.44] [G loss: 7.359165]\n",
            "********* 3531 [D loss: 0.034204, acc:, 99.22] [G loss: 9.766647]\n",
            "********* 3532 [D loss: 0.002074, acc:, 100.00] [G loss: 11.943544]\n",
            "********* 3533 [D loss: 0.013198, acc:, 99.22] [G loss: 13.356972]\n",
            "********* 3534 [D loss: 0.105735, acc:, 96.09] [G loss: 10.838734]\n",
            "********* 3535 [D loss: 0.079590, acc:, 97.66] [G loss: 7.099003]\n",
            "********* 3536 [D loss: 0.083751, acc:, 96.09] [G loss: 8.945820]\n",
            "********* 3537 [D loss: 0.036690, acc:, 98.44] [G loss: 12.224409]\n",
            "********* 3538 [D loss: 0.014363, acc:, 98.44] [G loss: 13.660480]\n",
            "********* 3539 [D loss: 0.035705, acc:, 98.44] [G loss: 13.644445]\n",
            "********* 3540 [D loss: 0.037840, acc:, 99.22] [G loss: 10.496525]\n",
            "********* 3541 [D loss: 0.058828, acc:, 96.88] [G loss: 9.386937]\n",
            "********* 3542 [D loss: 0.004308, acc:, 100.00] [G loss: 9.488227]\n",
            "********* 3543 [D loss: 0.019251, acc:, 100.00] [G loss: 8.908102]\n",
            "********* 3544 [D loss: 0.025717, acc:, 98.44] [G loss: 10.911761]\n",
            "********* 3545 [D loss: 0.009535, acc:, 100.00] [G loss: 12.062654]\n",
            "********* 3546 [D loss: 0.040260, acc:, 97.66] [G loss: 11.777998]\n",
            "********* 3547 [D loss: 0.015985, acc:, 99.22] [G loss: 9.329548]\n",
            "********* 3548 [D loss: 0.011993, acc:, 100.00] [G loss: 8.803700]\n",
            "********* 3549 [D loss: 0.047373, acc:, 97.66] [G loss: 8.297926]\n",
            "********* 3550 [D loss: 0.018368, acc:, 99.22] [G loss: 9.605819]\n",
            "********* 3551 [D loss: 0.016033, acc:, 99.22] [G loss: 9.883375]\n",
            "********* 3552 [D loss: 0.007934, acc:, 100.00] [G loss: 10.226404]\n",
            "********* 3553 [D loss: 0.050979, acc:, 97.66] [G loss: 9.093857]\n",
            "********* 3554 [D loss: 0.025442, acc:, 98.44] [G loss: 7.663641]\n",
            "********* 3555 [D loss: 0.059298, acc:, 97.66] [G loss: 7.393603]\n",
            "********* 3556 [D loss: 0.013430, acc:, 99.22] [G loss: 9.464965]\n",
            "********* 3557 [D loss: 0.064587, acc:, 98.44] [G loss: 7.877005]\n",
            "********* 3558 [D loss: 0.016199, acc:, 99.22] [G loss: 7.515578]\n",
            "********* 3559 [D loss: 0.030455, acc:, 99.22] [G loss: 7.993986]\n",
            "********* 3560 [D loss: 0.035965, acc:, 97.66] [G loss: 9.492659]\n",
            "********* 3561 [D loss: 0.033191, acc:, 98.44] [G loss: 12.948448]\n",
            "********* 3562 [D loss: 0.019160, acc:, 99.22] [G loss: 12.497108]\n",
            "********* 3563 [D loss: 0.053485, acc:, 97.66] [G loss: 8.411388]\n",
            "********* 3564 [D loss: 0.061405, acc:, 98.44] [G loss: 8.079231]\n",
            "********* 3565 [D loss: 0.021013, acc:, 99.22] [G loss: 10.355186]\n",
            "********* 3566 [D loss: 0.028486, acc:, 98.44] [G loss: 11.703773]\n",
            "********* 3567 [D loss: 0.033857, acc:, 98.44] [G loss: 10.577187]\n",
            "********* 3568 [D loss: 0.053157, acc:, 98.44] [G loss: 7.821210]\n",
            "********* 3569 [D loss: 0.018765, acc:, 100.00] [G loss: 7.933159]\n",
            "********* 3570 [D loss: 0.013180, acc:, 99.22] [G loss: 9.632985]\n",
            "********* 3571 [D loss: 0.005934, acc:, 100.00] [G loss: 11.260792]\n",
            "********* 3572 [D loss: 0.070173, acc:, 96.88] [G loss: 7.742440]\n",
            "********* 3573 [D loss: 0.015782, acc:, 99.22] [G loss: 8.095757]\n",
            "********* 3574 [D loss: 0.092474, acc:, 96.88] [G loss: 8.042385]\n",
            "********* 3575 [D loss: 0.070811, acc:, 96.88] [G loss: 10.304379]\n",
            "********* 3576 [D loss: 0.057341, acc:, 98.44] [G loss: 10.439100]\n",
            "********* 3577 [D loss: 0.104536, acc:, 97.66] [G loss: 8.429729]\n",
            "********* 3578 [D loss: 0.016990, acc:, 99.22] [G loss: 8.092535]\n",
            "********* 3579 [D loss: 0.045321, acc:, 99.22] [G loss: 9.785294]\n",
            "********* 3580 [D loss: 0.003804, acc:, 100.00] [G loss: 11.885881]\n",
            "********* 3581 [D loss: 0.023549, acc:, 99.22] [G loss: 10.045127]\n",
            "********* 3582 [D loss: 0.073872, acc:, 96.09] [G loss: 7.536956]\n",
            "********* 3583 [D loss: 0.079058, acc:, 97.66] [G loss: 8.067272]\n",
            "********* 3584 [D loss: 0.009605, acc:, 99.22] [G loss: 11.593172]\n",
            "********* 3585 [D loss: 0.048639, acc:, 97.66] [G loss: 9.954382]\n",
            "********* 3586 [D loss: 0.069875, acc:, 96.88] [G loss: 7.368845]\n",
            "********* 3587 [D loss: 0.039438, acc:, 99.22] [G loss: 9.086227]\n",
            "********* 3588 [D loss: 0.004024, acc:, 100.00] [G loss: 11.748899]\n",
            "********* 3589 [D loss: 0.042212, acc:, 97.66] [G loss: 12.967785]\n",
            "********* 3590 [D loss: 0.010839, acc:, 100.00] [G loss: 11.944317]\n",
            "********* 3591 [D loss: 0.011500, acc:, 99.22] [G loss: 11.128318]\n",
            "********* 3592 [D loss: 0.018116, acc:, 99.22] [G loss: 11.997454]\n",
            "********* 3593 [D loss: 0.022960, acc:, 99.22] [G loss: 10.115705]\n",
            "********* 3594 [D loss: 0.004162, acc:, 100.00] [G loss: 8.864767]\n",
            "********* 3595 [D loss: 0.038098, acc:, 99.22] [G loss: 8.366430]\n",
            "********* 3596 [D loss: 0.007563, acc:, 100.00] [G loss: 8.042429]\n",
            "********* 3597 [D loss: 0.024639, acc:, 99.22] [G loss: 8.862055]\n",
            "********* 3598 [D loss: 0.002837, acc:, 100.00] [G loss: 10.804882]\n",
            "********* 3599 [D loss: 0.072473, acc:, 97.66] [G loss: 7.018955]\n",
            "********* 3600 [D loss: 0.032183, acc:, 99.22] [G loss: 6.533520]\n",
            "********* 3601 [D loss: 0.060610, acc:, 98.44] [G loss: 9.141644]\n",
            "********* 3602 [D loss: 0.014974, acc:, 99.22] [G loss: 11.599119]\n",
            "********* 3603 [D loss: 0.103298, acc:, 96.88] [G loss: 6.945811]\n",
            "********* 3604 [D loss: 0.073578, acc:, 98.44] [G loss: 5.932207]\n",
            "********* 3605 [D loss: 0.147309, acc:, 95.31] [G loss: 9.794750]\n",
            "********* 3606 [D loss: 0.007251, acc:, 100.00] [G loss: 17.372257]\n",
            "********* 3607 [D loss: 0.199360, acc:, 91.41] [G loss: 7.841625]\n",
            "********* 3608 [D loss: 0.094417, acc:, 96.88] [G loss: 5.072027]\n",
            "********* 3609 [D loss: 0.065111, acc:, 96.88] [G loss: 8.624510]\n",
            "********* 3610 [D loss: 0.007519, acc:, 100.00] [G loss: 14.258292]\n",
            "********* 3611 [D loss: 0.015789, acc:, 99.22] [G loss: 17.162148]\n",
            "********* 3612 [D loss: 0.150776, acc:, 97.66] [G loss: 16.855864]\n",
            "********* 3613 [D loss: 0.109833, acc:, 96.88] [G loss: 8.866709]\n",
            "********* 3614 [D loss: 0.082351, acc:, 96.88] [G loss: 7.068065]\n",
            "********* 3615 [D loss: 0.049818, acc:, 99.22] [G loss: 8.643281]\n",
            "********* 3616 [D loss: 0.003786, acc:, 100.00] [G loss: 11.019808]\n",
            "********* 3617 [D loss: 0.002287, acc:, 100.00] [G loss: 12.569256]\n",
            "********* 3618 [D loss: 0.109349, acc:, 96.09] [G loss: 12.247746]\n",
            "********* 3619 [D loss: 0.095820, acc:, 96.09] [G loss: 11.308393]\n",
            "********* 3620 [D loss: 0.031649, acc:, 98.44] [G loss: 10.685841]\n",
            "********* 3621 [D loss: 0.008248, acc:, 99.22] [G loss: 12.326486]\n",
            "********* 3622 [D loss: 0.017201, acc:, 99.22] [G loss: 11.413528]\n",
            "********* 3623 [D loss: 0.016346, acc:, 99.22] [G loss: 10.931122]\n",
            "********* 3624 [D loss: 0.072407, acc:, 96.88] [G loss: 9.980429]\n",
            "********* 3625 [D loss: 0.013926, acc:, 100.00] [G loss: 9.173149]\n",
            "********* 3626 [D loss: 0.048080, acc:, 96.88] [G loss: 8.191747]\n",
            "********* 3627 [D loss: 0.071359, acc:, 96.88] [G loss: 9.059802]\n",
            "********* 3628 [D loss: 0.019768, acc:, 99.22] [G loss: 9.222635]\n",
            "********* 3629 [D loss: 0.003578, acc:, 100.00] [G loss: 9.485319]\n",
            "********* 3630 [D loss: 0.193484, acc:, 99.22] [G loss: 9.752880]\n",
            "********* 3631 [D loss: 0.002275, acc:, 100.00] [G loss: 10.423622]\n",
            "********* 3632 [D loss: 0.032644, acc:, 99.22] [G loss: 8.495464]\n",
            "********* 3633 [D loss: 0.010591, acc:, 100.00] [G loss: 7.149880]\n",
            "********* 3634 [D loss: 0.057769, acc:, 98.44] [G loss: 6.324230]\n",
            "********* 3635 [D loss: 0.078558, acc:, 96.09] [G loss: 6.852467]\n",
            "********* 3636 [D loss: 0.013786, acc:, 99.22] [G loss: 9.272356]\n",
            "********* 3637 [D loss: 0.003491, acc:, 100.00] [G loss: 10.847874]\n",
            "********* 3638 [D loss: 0.019354, acc:, 99.22] [G loss: 10.665604]\n",
            "********* 3639 [D loss: 0.075232, acc:, 97.66] [G loss: 8.043535]\n",
            "********* 3640 [D loss: 0.015660, acc:, 100.00] [G loss: 7.163822]\n",
            "********* 3641 [D loss: 0.014426, acc:, 99.22] [G loss: 7.149055]\n",
            "********* 3642 [D loss: 0.012837, acc:, 99.22] [G loss: 8.572380]\n",
            "********* 3643 [D loss: 0.017188, acc:, 99.22] [G loss: 8.202314]\n",
            "********* 3644 [D loss: 0.046481, acc:, 97.66] [G loss: 7.598716]\n",
            "********* 3645 [D loss: 0.013415, acc:, 100.00] [G loss: 7.944160]\n",
            "********* 3646 [D loss: 0.010651, acc:, 100.00] [G loss: 9.293269]\n",
            "********* 3647 [D loss: 0.007877, acc:, 100.00] [G loss: 9.955446]\n",
            "********* 3648 [D loss: 0.041999, acc:, 98.44] [G loss: 7.660411]\n",
            "********* 3649 [D loss: 0.010729, acc:, 100.00] [G loss: 6.201088]\n",
            "********* 3650 [D loss: 0.043487, acc:, 99.22] [G loss: 7.819723]\n",
            "********* 3651 [D loss: 0.004769, acc:, 100.00] [G loss: 9.606049]\n",
            "********* 3652 [D loss: 0.013837, acc:, 99.22] [G loss: 10.425549]\n",
            "********* 3653 [D loss: 0.025621, acc:, 98.44] [G loss: 9.203533]\n",
            "********* 3654 [D loss: 0.080343, acc:, 96.09] [G loss: 7.394956]\n",
            "********* 3655 [D loss: 0.017490, acc:, 100.00] [G loss: 7.754497]\n",
            "********* 3656 [D loss: 0.055853, acc:, 98.44] [G loss: 7.150920]\n",
            "********* 3657 [D loss: 0.032054, acc:, 98.44] [G loss: 9.227278]\n",
            "********* 3658 [D loss: 0.037073, acc:, 99.22] [G loss: 9.912203]\n",
            "********* 3659 [D loss: 0.082851, acc:, 98.44] [G loss: 10.406841]\n",
            "********* 3660 [D loss: 0.089035, acc:, 95.31] [G loss: 6.538796]\n",
            "********* 3661 [D loss: 0.049914, acc:, 98.44] [G loss: 7.263963]\n",
            "********* 3662 [D loss: 0.013375, acc:, 100.00] [G loss: 8.814684]\n",
            "********* 3663 [D loss: 0.100447, acc:, 96.88] [G loss: 9.643420]\n",
            "********* 3664 [D loss: 0.014142, acc:, 99.22] [G loss: 10.214484]\n",
            "********* 3665 [D loss: 0.011633, acc:, 100.00] [G loss: 8.566197]\n",
            "********* 3666 [D loss: 0.003920, acc:, 100.00] [G loss: 8.504959]\n",
            "********* 3667 [D loss: 0.016663, acc:, 99.22] [G loss: 7.986035]\n",
            "********* 3668 [D loss: 0.018885, acc:, 100.00] [G loss: 8.980301]\n",
            "********* 3669 [D loss: 0.010751, acc:, 99.22] [G loss: 9.179111]\n",
            "********* 3670 [D loss: 0.009352, acc:, 100.00] [G loss: 8.860603]\n",
            "********* 3671 [D loss: 0.005867, acc:, 100.00] [G loss: 8.942812]\n",
            "********* 3672 [D loss: 0.023373, acc:, 99.22] [G loss: 7.819230]\n",
            "********* 3673 [D loss: 0.016350, acc:, 99.22] [G loss: 7.803851]\n",
            "********* 3674 [D loss: 0.024384, acc:, 99.22] [G loss: 8.035509]\n",
            "********* 3675 [D loss: 0.098798, acc:, 97.66] [G loss: 6.655315]\n",
            "********* 3676 [D loss: 0.021187, acc:, 100.00] [G loss: 7.546411]\n",
            "********* 3677 [D loss: 0.021523, acc:, 99.22] [G loss: 9.659443]\n",
            "********* 3678 [D loss: 0.040134, acc:, 96.88] [G loss: 8.640674]\n",
            "********* 3679 [D loss: 0.017413, acc:, 99.22] [G loss: 8.915358]\n",
            "********* 3680 [D loss: 0.036412, acc:, 99.22] [G loss: 7.604499]\n",
            "********* 3681 [D loss: 0.047084, acc:, 98.44] [G loss: 6.450918]\n",
            "********* 3682 [D loss: 0.019125, acc:, 99.22] [G loss: 7.857429]\n",
            "********* 3683 [D loss: 0.003585, acc:, 100.00] [G loss: 9.146845]\n",
            "********* 3684 [D loss: 0.010003, acc:, 100.00] [G loss: 9.435654]\n",
            "********* 3685 [D loss: 0.008840, acc:, 100.00] [G loss: 8.346521]\n",
            "********* 3686 [D loss: 0.024932, acc:, 99.22] [G loss: 7.454931]\n",
            "********* 3687 [D loss: 0.032598, acc:, 100.00] [G loss: 6.508258]\n",
            "********* 3688 [D loss: 0.007692, acc:, 100.00] [G loss: 7.882940]\n",
            "********* 3689 [D loss: 0.008074, acc:, 100.00] [G loss: 9.409613]\n",
            "********* 3690 [D loss: 0.019896, acc:, 99.22] [G loss: 8.889499]\n",
            "********* 3691 [D loss: 0.041871, acc:, 99.22] [G loss: 6.974495]\n",
            "********* 3692 [D loss: 0.024944, acc:, 99.22] [G loss: 8.197649]\n",
            "********* 3693 [D loss: 0.033327, acc:, 99.22] [G loss: 8.344569]\n",
            "********* 3694 [D loss: 0.064356, acc:, 96.88] [G loss: 8.246706]\n",
            "********* 3695 [D loss: 0.047813, acc:, 97.66] [G loss: 7.927237]\n",
            "********* 3696 [D loss: 0.039826, acc:, 98.44] [G loss: 11.340199]\n",
            "********* 3697 [D loss: 0.004930, acc:, 100.00] [G loss: 14.046654]\n",
            "********* 3698 [D loss: 0.041258, acc:, 99.22] [G loss: 13.210474]\n",
            "********* 3699 [D loss: 0.021065, acc:, 99.22] [G loss: 9.552094]\n",
            "********* 3700 [D loss: 0.033332, acc:, 99.22] [G loss: 6.900458]\n",
            "********* 3701 [D loss: 0.052563, acc:, 99.22] [G loss: 8.421455]\n",
            "********* 3702 [D loss: 0.006997, acc:, 100.00] [G loss: 13.387104]\n",
            "********* 3703 [D loss: 0.061694, acc:, 97.66] [G loss: 13.737545]\n",
            "********* 3704 [D loss: 0.081001, acc:, 96.09] [G loss: 6.492270]\n",
            "********* 3705 [D loss: 0.526840, acc:, 82.81] [G loss: 15.032421]\n",
            "********* 3706 [D loss: 0.109975, acc:, 96.88] [G loss: 37.556839]\n",
            "********* 3707 [D loss: 3.066733, acc:, 73.44] [G loss: 6.587490]\n",
            "********* 3708 [D loss: 1.977189, acc:, 71.88] [G loss: 4.585978]\n",
            "********* 3709 [D loss: 0.754186, acc:, 82.03] [G loss: 12.338135]\n",
            "********* 3710 [D loss: 0.226189, acc:, 95.31] [G loss: 23.679277]\n",
            "********* 3711 [D loss: 0.544971, acc:, 86.72] [G loss: 24.184076]\n",
            "********* 3712 [D loss: 1.117061, acc:, 77.34] [G loss: 13.462660]\n",
            "********* 3713 [D loss: 0.077041, acc:, 96.88] [G loss: 6.583559]\n",
            "********* 3714 [D loss: 0.394835, acc:, 86.72] [G loss: 5.022162]\n",
            "********* 3715 [D loss: 0.381638, acc:, 89.06] [G loss: 8.239506]\n",
            "********* 3716 [D loss: 0.002838, acc:, 100.00] [G loss: 12.978571]\n",
            "********* 3717 [D loss: 0.065278, acc:, 97.66] [G loss: 15.397141]\n",
            "********* 3718 [D loss: 0.135762, acc:, 96.09] [G loss: 15.857995]\n",
            "********* 3719 [D loss: 0.079215, acc:, 96.88] [G loss: 12.930121]\n",
            "********* 3720 [D loss: 0.038225, acc:, 98.44] [G loss: 12.225430]\n",
            "********* 3721 [D loss: 0.010730, acc:, 100.00] [G loss: 9.717790]\n",
            "********* 3722 [D loss: 0.086714, acc:, 96.09] [G loss: 9.953330]\n",
            "********* 3723 [D loss: 0.072611, acc:, 96.88] [G loss: 9.916964]\n",
            "********* 3724 [D loss: 0.029242, acc:, 98.44] [G loss: 10.082773]\n",
            "********* 3725 [D loss: 0.008030, acc:, 100.00] [G loss: 11.884970]\n",
            "********* 3726 [D loss: 0.015781, acc:, 100.00] [G loss: 11.593058]\n",
            "********* 3727 [D loss: 0.052133, acc:, 97.66] [G loss: 11.110238]\n",
            "********* 3728 [D loss: 0.058650, acc:, 98.44] [G loss: 10.145378]\n",
            "********* 3729 [D loss: 0.043437, acc:, 98.44] [G loss: 9.448384]\n",
            "********* 3730 [D loss: 0.078983, acc:, 96.09] [G loss: 8.374267]\n",
            "********* 3731 [D loss: 0.014484, acc:, 100.00] [G loss: 7.109204]\n",
            "********* 3732 [D loss: 0.016363, acc:, 100.00] [G loss: 6.772481]\n",
            "********* 3733 [D loss: 0.055330, acc:, 97.66] [G loss: 7.420782]\n",
            "********* 3734 [D loss: 0.018109, acc:, 99.22] [G loss: 8.682782]\n",
            "********* 3735 [D loss: 0.058257, acc:, 98.44] [G loss: 9.319334]\n",
            "********* 3736 [D loss: 0.041877, acc:, 98.44] [G loss: 7.761216]\n",
            "********* 3737 [D loss: 0.036232, acc:, 98.44] [G loss: 6.093606]\n",
            "********* 3738 [D loss: 0.037273, acc:, 99.22] [G loss: 5.478757]\n",
            "********* 3739 [D loss: 0.128653, acc:, 96.09] [G loss: 6.091148]\n",
            "********* 3740 [D loss: 0.022235, acc:, 100.00] [G loss: 7.716169]\n",
            "********* 3741 [D loss: 0.044521, acc:, 98.44] [G loss: 8.158765]\n",
            "********* 3742 [D loss: 0.047716, acc:, 98.44] [G loss: 7.699407]\n",
            "********* 3743 [D loss: 0.082072, acc:, 96.88] [G loss: 6.005063]\n",
            "********* 3744 [D loss: 0.042832, acc:, 99.22] [G loss: 5.390513]\n",
            "********* 3745 [D loss: 0.035350, acc:, 100.00] [G loss: 4.804110]\n",
            "********* 3746 [D loss: 0.095123, acc:, 96.09] [G loss: 6.319189]\n",
            "********* 3747 [D loss: 0.029201, acc:, 99.22] [G loss: 8.101585]\n",
            "********* 3748 [D loss: 0.057316, acc:, 97.66] [G loss: 7.414181]\n",
            "********* 3749 [D loss: 0.157322, acc:, 93.75] [G loss: 4.535872]\n",
            "********* 3750 [D loss: 0.205157, acc:, 90.62] [G loss: 6.352693]\n",
            "********* 3751 [D loss: 0.011748, acc:, 100.00] [G loss: 10.644213]\n",
            "********* 3752 [D loss: 0.261328, acc:, 92.19] [G loss: 8.276075]\n",
            "********* 3753 [D loss: 0.106147, acc:, 98.44] [G loss: 5.519723]\n",
            "********* 3754 [D loss: 0.171645, acc:, 90.62] [G loss: 5.119228]\n",
            "********* 3755 [D loss: 0.027219, acc:, 100.00] [G loss: 8.720146]\n",
            "********* 3756 [D loss: 0.100415, acc:, 97.66] [G loss: 9.349713]\n",
            "********* 3757 [D loss: 0.121876, acc:, 94.53] [G loss: 7.612327]\n",
            "********* 3758 [D loss: 0.037641, acc:, 98.44] [G loss: 6.536855]\n",
            "********* 3759 [D loss: 0.068700, acc:, 98.44] [G loss: 5.261702]\n",
            "********* 3760 [D loss: 0.025754, acc:, 100.00] [G loss: 5.536494]\n",
            "********* 3761 [D loss: 0.026315, acc:, 100.00] [G loss: 6.222195]\n",
            "********* 3762 [D loss: 0.054244, acc:, 99.22] [G loss: 5.725836]\n",
            "********* 3763 [D loss: 0.058205, acc:, 97.66] [G loss: 5.960320]\n",
            "********* 3764 [D loss: 0.067752, acc:, 98.44] [G loss: 7.361609]\n",
            "********* 3765 [D loss: 0.084720, acc:, 96.09] [G loss: 8.210938]\n",
            "********* 3766 [D loss: 0.039907, acc:, 98.44] [G loss: 8.071873]\n",
            "********* 3767 [D loss: 0.139789, acc:, 95.31] [G loss: 7.759232]\n",
            "********* 3768 [D loss: 0.061886, acc:, 98.44] [G loss: 6.127656]\n",
            "********* 3769 [D loss: 0.032238, acc:, 99.22] [G loss: 6.065948]\n",
            "********* 3770 [D loss: 0.052414, acc:, 98.44] [G loss: 5.880393]\n",
            "********* 3771 [D loss: 0.025850, acc:, 99.22] [G loss: 7.424063]\n",
            "********* 3772 [D loss: 0.015408, acc:, 100.00] [G loss: 8.577237]\n",
            "********* 3773 [D loss: 0.065057, acc:, 97.66] [G loss: 6.959406]\n",
            "********* 3774 [D loss: 0.033214, acc:, 99.22] [G loss: 5.766901]\n",
            "********* 3775 [D loss: 0.065515, acc:, 97.66] [G loss: 5.459117]\n",
            "********* 3776 [D loss: 0.009487, acc:, 100.00] [G loss: 7.267040]\n",
            "********* 3777 [D loss: 0.043416, acc:, 98.44] [G loss: 6.458120]\n",
            "********* 3778 [D loss: 0.078645, acc:, 96.88] [G loss: 5.788960]\n",
            "********* 3779 [D loss: 0.070495, acc:, 96.09] [G loss: 5.862725]\n",
            "********* 3780 [D loss: 0.030999, acc:, 99.22] [G loss: 5.586978]\n",
            "********* 3781 [D loss: 0.029997, acc:, 99.22] [G loss: 7.181150]\n",
            "********* 3782 [D loss: 0.040768, acc:, 99.22] [G loss: 7.834134]\n",
            "********* 3783 [D loss: 0.051062, acc:, 96.09] [G loss: 7.288788]\n",
            "********* 3784 [D loss: 0.034154, acc:, 98.44] [G loss: 7.223905]\n",
            "********* 3785 [D loss: 0.057448, acc:, 97.66] [G loss: 7.224364]\n",
            "********* 3786 [D loss: 0.016674, acc:, 99.22] [G loss: 8.341726]\n",
            "********* 3787 [D loss: 0.061938, acc:, 96.88] [G loss: 7.085128]\n",
            "********* 3788 [D loss: 0.046813, acc:, 99.22] [G loss: 6.284235]\n",
            "********* 3789 [D loss: 0.081222, acc:, 98.44] [G loss: 7.050189]\n",
            "********* 3790 [D loss: 0.025478, acc:, 99.22] [G loss: 8.672806]\n",
            "********* 3791 [D loss: 0.016339, acc:, 99.22] [G loss: 9.908297]\n",
            "********* 3792 [D loss: 0.082601, acc:, 96.88] [G loss: 6.615078]\n",
            "********* 3793 [D loss: 0.067843, acc:, 96.88] [G loss: 4.919021]\n",
            "********* 3794 [D loss: 0.029777, acc:, 99.22] [G loss: 6.141364]\n",
            "********* 3795 [D loss: 0.036760, acc:, 99.22] [G loss: 7.481341]\n",
            "********* 3796 [D loss: 0.018345, acc:, 99.22] [G loss: 6.901491]\n",
            "********* 3797 [D loss: 0.040203, acc:, 99.22] [G loss: 6.551609]\n",
            "********* 3798 [D loss: 0.060332, acc:, 98.44] [G loss: 5.176235]\n",
            "********* 3799 [D loss: 0.060594, acc:, 96.09] [G loss: 5.525935]\n",
            "********* 3800 [D loss: 0.045437, acc:, 98.44] [G loss: 7.514799]\n",
            "********* 3801 [D loss: 0.092933, acc:, 97.66] [G loss: 7.906604]\n",
            "********* 3802 [D loss: 0.017205, acc:, 99.22] [G loss: 8.303314]\n",
            "********* 3803 [D loss: 0.087360, acc:, 96.88] [G loss: 7.072151]\n",
            "********* 3804 [D loss: 0.021279, acc:, 100.00] [G loss: 7.042675]\n",
            "********* 3805 [D loss: 0.015406, acc:, 99.22] [G loss: 7.927418]\n",
            "********* 3806 [D loss: 0.062144, acc:, 98.44] [G loss: 7.391366]\n",
            "********* 3807 [D loss: 0.037864, acc:, 98.44] [G loss: 6.202704]\n",
            "********* 3808 [D loss: 0.028493, acc:, 98.44] [G loss: 7.886664]\n",
            "********* 3809 [D loss: 0.012591, acc:, 100.00] [G loss: 7.930655]\n",
            "********* 3810 [D loss: 0.026042, acc:, 99.22] [G loss: 7.573262]\n",
            "********* 3811 [D loss: 0.088399, acc:, 99.22] [G loss: 7.941155]\n",
            "********* 3812 [D loss: 0.062422, acc:, 96.88] [G loss: 7.721403]\n",
            "********* 3813 [D loss: 0.039339, acc:, 99.22] [G loss: 7.459389]\n",
            "********* 3814 [D loss: 0.066459, acc:, 97.66] [G loss: 7.246705]\n",
            "********* 3815 [D loss: 0.017939, acc:, 99.22] [G loss: 8.272674]\n",
            "********* 3816 [D loss: 0.033561, acc:, 99.22] [G loss: 8.497198]\n",
            "********* 3817 [D loss: 0.039820, acc:, 98.44] [G loss: 6.982937]\n",
            "********* 3818 [D loss: 0.051621, acc:, 99.22] [G loss: 6.551843]\n",
            "********* 3819 [D loss: 0.087843, acc:, 97.66] [G loss: 7.254398]\n",
            "********* 3820 [D loss: 0.004054, acc:, 100.00] [G loss: 9.602766]\n",
            "********* 3821 [D loss: 0.005871, acc:, 100.00] [G loss: 11.111329]\n",
            "********* 3822 [D loss: 0.059466, acc:, 97.66] [G loss: 8.167459]\n",
            "********* 3823 [D loss: 0.032267, acc:, 99.22] [G loss: 6.834554]\n",
            "********* 3824 [D loss: 0.020030, acc:, 100.00] [G loss: 7.531785]\n",
            "********* 3825 [D loss: 0.005092, acc:, 100.00] [G loss: 8.640105]\n",
            "********* 3826 [D loss: 0.010253, acc:, 100.00] [G loss: 8.587502]\n",
            "********* 3827 [D loss: 0.023468, acc:, 99.22] [G loss: 8.149835]\n",
            "********* 3828 [D loss: 0.053564, acc:, 99.22] [G loss: 6.633934]\n",
            "********* 3829 [D loss: 0.012588, acc:, 100.00] [G loss: 7.772406]\n",
            "********* 3830 [D loss: 0.019224, acc:, 98.44] [G loss: 7.829844]\n",
            "********* 3831 [D loss: 0.014965, acc:, 99.22] [G loss: 7.106787]\n",
            "********* 3832 [D loss: 0.015966, acc:, 100.00] [G loss: 7.863259]\n",
            "********* 3833 [D loss: 0.015926, acc:, 99.22] [G loss: 8.958742]\n",
            "********* 3834 [D loss: 0.043182, acc:, 98.44] [G loss: 8.229680]\n",
            "********* 3835 [D loss: 0.014755, acc:, 100.00] [G loss: 8.216072]\n",
            "********* 3836 [D loss: 0.044604, acc:, 97.66] [G loss: 8.279091]\n",
            "********* 3837 [D loss: 0.017699, acc:, 99.22] [G loss: 10.074638]\n",
            "********* 3838 [D loss: 0.026453, acc:, 98.44] [G loss: 9.623492]\n",
            "********* 3839 [D loss: 0.059332, acc:, 98.44] [G loss: 6.513054]\n",
            "********* 3840 [D loss: 0.045290, acc:, 100.00] [G loss: 7.728737]\n",
            "********* 3841 [D loss: 0.016169, acc:, 99.22] [G loss: 11.300244]\n",
            "********* 3842 [D loss: 0.118357, acc:, 97.66] [G loss: 6.587928]\n",
            "********* 3843 [D loss: 0.122415, acc:, 94.53] [G loss: 8.314558]\n",
            "********* 3844 [D loss: 0.007789, acc:, 100.00] [G loss: 14.710647]\n",
            "********* 3845 [D loss: 0.167908, acc:, 96.09] [G loss: 12.818832]\n",
            "********* 3846 [D loss: 0.050805, acc:, 99.22] [G loss: 9.320724]\n",
            "********* 3847 [D loss: 0.067707, acc:, 96.88] [G loss: 7.457835]\n",
            "********* 3848 [D loss: 0.091395, acc:, 95.31] [G loss: 8.442775]\n",
            "********* 3849 [D loss: 0.047717, acc:, 96.88] [G loss: 12.956280]\n",
            "********* 3850 [D loss: 0.037715, acc:, 99.22] [G loss: 15.395464]\n",
            "********* 3851 [D loss: 0.060115, acc:, 97.66] [G loss: 11.898642]\n",
            "********* 3852 [D loss: 0.025601, acc:, 100.00] [G loss: 9.150542]\n",
            "********* 3853 [D loss: 0.028375, acc:, 99.22] [G loss: 8.541848]\n",
            "********* 3854 [D loss: 0.012281, acc:, 100.00] [G loss: 8.619759]\n",
            "********* 3855 [D loss: 0.006507, acc:, 100.00] [G loss: 9.802996]\n",
            "********* 3856 [D loss: 0.015363, acc:, 99.22] [G loss: 10.549998]\n",
            "********* 3857 [D loss: 0.025866, acc:, 98.44] [G loss: 9.944889]\n",
            "********* 3858 [D loss: 0.010364, acc:, 100.00] [G loss: 8.057560]\n",
            "********* 3859 [D loss: 0.058118, acc:, 98.44] [G loss: 7.714511]\n",
            "********* 3860 [D loss: 0.033414, acc:, 98.44] [G loss: 7.815562]\n",
            "********* 3861 [D loss: 0.008356, acc:, 100.00] [G loss: 9.586384]\n",
            "********* 3862 [D loss: 0.002713, acc:, 100.00] [G loss: 10.172833]\n",
            "********* 3863 [D loss: 0.072919, acc:, 96.88] [G loss: 7.520154]\n",
            "********* 3864 [D loss: 0.025321, acc:, 99.22] [G loss: 6.586704]\n",
            "********* 3865 [D loss: 0.097005, acc:, 96.88] [G loss: 8.748134]\n",
            "********* 3866 [D loss: 0.003651, acc:, 100.00] [G loss: 12.262488]\n",
            "********* 3867 [D loss: 0.174724, acc:, 93.75] [G loss: 9.673292]\n",
            "********* 3868 [D loss: 0.051290, acc:, 98.44] [G loss: 6.810201]\n",
            "********* 3869 [D loss: 0.054418, acc:, 99.22] [G loss: 7.291347]\n",
            "********* 3870 [D loss: 0.009246, acc:, 100.00] [G loss: 8.746211]\n",
            "********* 3871 [D loss: 0.012612, acc:, 100.00] [G loss: 10.838636]\n",
            "********* 3872 [D loss: 0.009496, acc:, 100.00] [G loss: 12.071552]\n",
            "********* 3873 [D loss: 0.060528, acc:, 97.66] [G loss: 9.077991]\n",
            "********* 3874 [D loss: 0.011501, acc:, 100.00] [G loss: 7.445122]\n",
            "********* 3875 [D loss: 0.015994, acc:, 100.00] [G loss: 6.252535]\n",
            "********* 3876 [D loss: 0.018116, acc:, 100.00] [G loss: 6.566937]\n",
            "********* 3877 [D loss: 0.010297, acc:, 100.00] [G loss: 8.453700]\n",
            "********* 3878 [D loss: 0.015679, acc:, 99.22] [G loss: 9.301893]\n",
            "********* 3879 [D loss: 0.029312, acc:, 99.22] [G loss: 8.136628]\n",
            "********* 3880 [D loss: 0.056447, acc:, 98.44] [G loss: 6.400430]\n",
            "********* 3881 [D loss: 0.044590, acc:, 98.44] [G loss: 6.771371]\n",
            "********* 3882 [D loss: 0.007783, acc:, 100.00] [G loss: 9.084188]\n",
            "********* 3883 [D loss: 0.024348, acc:, 98.44] [G loss: 8.756327]\n",
            "********* 3884 [D loss: 0.015865, acc:, 100.00] [G loss: 8.505375]\n",
            "********* 3885 [D loss: 0.017420, acc:, 100.00] [G loss: 7.984897]\n",
            "********* 3886 [D loss: 0.049888, acc:, 96.88] [G loss: 7.372790]\n",
            "********* 3887 [D loss: 0.013559, acc:, 100.00] [G loss: 7.893109]\n",
            "********* 3888 [D loss: 0.015774, acc:, 100.00] [G loss: 7.591680]\n",
            "********* 3889 [D loss: 0.044410, acc:, 99.22] [G loss: 7.861588]\n",
            "********* 3890 [D loss: 0.024262, acc:, 99.22] [G loss: 8.215443]\n",
            "********* 3891 [D loss: 0.009270, acc:, 100.00] [G loss: 8.955302]\n",
            "********* 3892 [D loss: 0.008804, acc:, 100.00] [G loss: 9.461636]\n",
            "********* 3893 [D loss: 0.056058, acc:, 97.66] [G loss: 7.672071]\n",
            "********* 3894 [D loss: 0.049509, acc:, 99.22] [G loss: 7.608525]\n",
            "********* 3895 [D loss: 0.010013, acc:, 100.00] [G loss: 7.712279]\n",
            "********* 3896 [D loss: 0.003659, acc:, 100.00] [G loss: 9.066639]\n",
            "********* 3897 [D loss: 0.021419, acc:, 99.22] [G loss: 9.263935]\n",
            "********* 3898 [D loss: 0.055103, acc:, 98.44] [G loss: 7.276296]\n",
            "********* 3899 [D loss: 0.075451, acc:, 96.88] [G loss: 7.832815]\n",
            "********* 3900 [D loss: 0.005480, acc:, 100.00] [G loss: 10.443054]\n",
            "********* 3901 [D loss: 0.031438, acc:, 98.44] [G loss: 10.077766]\n",
            "********* 3902 [D loss: 0.012132, acc:, 100.00] [G loss: 9.662506]\n",
            "********* 3903 [D loss: 0.006411, acc:, 100.00] [G loss: 8.601583]\n",
            "********* 3904 [D loss: 0.026846, acc:, 98.44] [G loss: 7.676010]\n",
            "********* 3905 [D loss: 0.012201, acc:, 100.00] [G loss: 9.001431]\n",
            "********* 3906 [D loss: 0.033627, acc:, 98.44] [G loss: 9.000741]\n",
            "********* 3907 [D loss: 0.051340, acc:, 99.22] [G loss: 8.184953]\n",
            "********* 3908 [D loss: 0.021350, acc:, 99.22] [G loss: 7.319637]\n",
            "********* 3909 [D loss: 0.077049, acc:, 96.88] [G loss: 6.711082]\n",
            "********* 3910 [D loss: 0.023648, acc:, 100.00] [G loss: 7.013698]\n",
            "********* 3911 [D loss: 0.062479, acc:, 99.22] [G loss: 8.494471]\n",
            "********* 3912 [D loss: 0.015988, acc:, 99.22] [G loss: 10.121560]\n",
            "********* 3913 [D loss: 0.039221, acc:, 97.66] [G loss: 9.527412]\n",
            "********* 3914 [D loss: 0.006698, acc:, 100.00] [G loss: 9.201780]\n",
            "********* 3915 [D loss: 0.018948, acc:, 99.22] [G loss: 6.596072]\n",
            "********* 3916 [D loss: 0.027941, acc:, 99.22] [G loss: 6.769553]\n",
            "********* 3917 [D loss: 0.010033, acc:, 100.00] [G loss: 9.598207]\n",
            "********* 3918 [D loss: 0.005128, acc:, 100.00] [G loss: 11.115065]\n",
            "********* 3919 [D loss: 0.048318, acc:, 99.22] [G loss: 10.107904]\n",
            "********* 3920 [D loss: 0.005799, acc:, 100.00] [G loss: 9.079857]\n",
            "********* 3921 [D loss: 0.036288, acc:, 97.66] [G loss: 7.826158]\n",
            "********* 3922 [D loss: 0.019752, acc:, 99.22] [G loss: 8.637794]\n",
            "********* 3923 [D loss: 0.056194, acc:, 99.22] [G loss: 9.898331]\n",
            "********* 3924 [D loss: 0.020141, acc:, 99.22] [G loss: 11.683302]\n",
            "********* 3925 [D loss: 0.080573, acc:, 96.88] [G loss: 5.713818]\n",
            "********* 3926 [D loss: 0.131930, acc:, 93.75] [G loss: 9.770615]\n",
            "********* 3927 [D loss: 0.039689, acc:, 98.44] [G loss: 19.629429]\n",
            "********* 3928 [D loss: 0.534242, acc:, 87.50] [G loss: 7.432261]\n",
            "********* 3929 [D loss: 0.133954, acc:, 93.75] [G loss: 6.677588]\n",
            "********* 3930 [D loss: 0.043042, acc:, 99.22] [G loss: 10.692990]\n",
            "********* 3931 [D loss: 0.083513, acc:, 96.88] [G loss: 14.404936]\n",
            "********* 3932 [D loss: 0.037461, acc:, 98.44] [G loss: 15.775970]\n",
            "********* 3933 [D loss: 0.067354, acc:, 98.44] [G loss: 13.780422]\n",
            "********* 3934 [D loss: 0.045352, acc:, 99.22] [G loss: 11.901163]\n",
            "********* 3935 [D loss: 0.009857, acc:, 100.00] [G loss: 9.301835]\n",
            "********* 3936 [D loss: 0.016377, acc:, 99.22] [G loss: 7.962943]\n",
            "********* 3937 [D loss: 0.031926, acc:, 99.22] [G loss: 8.177749]\n",
            "********* 3938 [D loss: 0.024080, acc:, 100.00] [G loss: 7.524271]\n",
            "********* 3939 [D loss: 0.039537, acc:, 99.22] [G loss: 7.185007]\n",
            "********* 3940 [D loss: 0.008192, acc:, 100.00] [G loss: 8.973818]\n",
            "********* 3941 [D loss: 0.007713, acc:, 100.00] [G loss: 10.044970]\n",
            "********* 3942 [D loss: 0.010961, acc:, 99.22] [G loss: 11.517511]\n",
            "********* 3943 [D loss: 0.013928, acc:, 99.22] [G loss: 12.539873]\n",
            "********* 3944 [D loss: 0.034833, acc:, 97.66] [G loss: 10.425068]\n",
            "********* 3945 [D loss: 0.067928, acc:, 96.88] [G loss: 9.579329]\n",
            "********* 3946 [D loss: 0.062910, acc:, 97.66] [G loss: 7.489738]\n",
            "********* 3947 [D loss: 0.050630, acc:, 97.66] [G loss: 7.893720]\n",
            "********* 3948 [D loss: 0.033673, acc:, 98.44] [G loss: 10.335297]\n",
            "********* 3949 [D loss: 0.023391, acc:, 99.22] [G loss: 11.406209]\n",
            "********* 3950 [D loss: 0.142616, acc:, 96.09] [G loss: 7.588315]\n",
            "********* 3951 [D loss: 0.046847, acc:, 99.22] [G loss: 5.963077]\n",
            "********* 3952 [D loss: 0.069664, acc:, 96.88] [G loss: 9.936945]\n",
            "********* 3953 [D loss: 0.033684, acc:, 98.44] [G loss: 13.929748]\n",
            "********* 3954 [D loss: 0.035120, acc:, 99.22] [G loss: 16.641270]\n",
            "********* 3955 [D loss: 0.140665, acc:, 97.66] [G loss: 11.441197]\n",
            "********* 3956 [D loss: 0.142818, acc:, 96.09] [G loss: 5.802053]\n",
            "********* 3957 [D loss: 0.202648, acc:, 89.06] [G loss: 8.781395]\n",
            "********* 3958 [D loss: 0.005100, acc:, 100.00] [G loss: 16.393160]\n",
            "********* 3959 [D loss: 0.139706, acc:, 94.53] [G loss: 14.249004]\n",
            "********* 3960 [D loss: 0.169645, acc:, 93.75] [G loss: 7.032356]\n",
            "********* 3961 [D loss: 0.249817, acc:, 91.41] [G loss: 8.435266]\n",
            "********* 3962 [D loss: 0.010849, acc:, 100.00] [G loss: 12.694932]\n",
            "********* 3963 [D loss: 0.004432, acc:, 100.00] [G loss: 18.400448]\n",
            "********* 3964 [D loss: 0.152684, acc:, 96.09] [G loss: 15.472740]\n",
            "********* 3965 [D loss: 0.041623, acc:, 97.66] [G loss: 11.169209]\n",
            "********* 3966 [D loss: 0.006103, acc:, 100.00] [G loss: 8.326836]\n",
            "********* 3967 [D loss: 0.105148, acc:, 96.09] [G loss: 7.586977]\n",
            "********* 3968 [D loss: 0.007083, acc:, 100.00] [G loss: 9.628551]\n",
            "********* 3969 [D loss: 0.011437, acc:, 99.22] [G loss: 11.965233]\n",
            "********* 3970 [D loss: 0.045475, acc:, 99.22] [G loss: 12.028792]\n",
            "********* 3971 [D loss: 0.001801, acc:, 100.00] [G loss: 10.919759]\n",
            "********* 3972 [D loss: 0.006478, acc:, 100.00] [G loss: 11.924795]\n",
            "********* 3973 [D loss: 0.036591, acc:, 98.44] [G loss: 9.026009]\n",
            "********* 3974 [D loss: 0.085771, acc:, 96.09] [G loss: 9.277322]\n",
            "********* 3975 [D loss: 0.031080, acc:, 99.22] [G loss: 10.162770]\n",
            "********* 3976 [D loss: 0.042811, acc:, 98.44] [G loss: 12.042405]\n",
            "********* 3977 [D loss: 0.079552, acc:, 97.66] [G loss: 8.633893]\n",
            "********* 3978 [D loss: 0.007246, acc:, 100.00] [G loss: 8.733736]\n",
            "********* 3979 [D loss: 0.028459, acc:, 98.44] [G loss: 6.600239]\n",
            "********* 3980 [D loss: 0.044536, acc:, 97.66] [G loss: 6.709455]\n",
            "********* 3981 [D loss: 0.051254, acc:, 99.22] [G loss: 8.027456]\n",
            "********* 3982 [D loss: 0.025673, acc:, 99.22] [G loss: 9.729397]\n",
            "********* 3983 [D loss: 0.014963, acc:, 100.00] [G loss: 9.446004]\n",
            "********* 3984 [D loss: 0.122846, acc:, 95.31] [G loss: 6.640426]\n",
            "********* 3985 [D loss: 0.012841, acc:, 100.00] [G loss: 8.850327]\n",
            "********* 3986 [D loss: 0.084795, acc:, 96.09] [G loss: 9.279324]\n",
            "********* 3987 [D loss: 0.011896, acc:, 99.22] [G loss: 8.856043]\n",
            "********* 3988 [D loss: 0.033848, acc:, 97.66] [G loss: 9.379520]\n",
            "********* 3989 [D loss: 0.094542, acc:, 98.44] [G loss: 8.424096]\n",
            "********* 3990 [D loss: 0.013853, acc:, 100.00] [G loss: 6.645987]\n",
            "********* 3991 [D loss: 0.019959, acc:, 100.00] [G loss: 6.551440]\n",
            "********* 3992 [D loss: 0.046824, acc:, 98.44] [G loss: 7.570961]\n",
            "********* 3993 [D loss: 0.007799, acc:, 100.00] [G loss: 9.954140]\n",
            "********* 3994 [D loss: 0.020952, acc:, 98.44] [G loss: 10.372927]\n",
            "********* 3995 [D loss: 0.048762, acc:, 99.22] [G loss: 7.490920]\n",
            "********* 3996 [D loss: 0.068751, acc:, 96.88] [G loss: 5.066225]\n",
            "********* 3997 [D loss: 0.248643, acc:, 89.06] [G loss: 8.406138]\n",
            "********* 3998 [D loss: 0.027050, acc:, 98.44] [G loss: 13.160810]\n",
            "********* 3999 [D loss: 0.379732, acc:, 94.53] [G loss: 12.509027]\n",
            "********* 4000 [D loss: 0.085661, acc:, 98.44] [G loss: 8.754076]\n",
            "********* 4001 [D loss: 0.018077, acc:, 100.00] [G loss: 8.581668]\n",
            "********* 4002 [D loss: 0.013880, acc:, 99.22] [G loss: 7.087692]\n",
            "********* 4003 [D loss: 0.095036, acc:, 95.31] [G loss: 9.805652]\n",
            "********* 4004 [D loss: 0.045598, acc:, 97.66] [G loss: 10.029297]\n",
            "********* 4005 [D loss: 0.025010, acc:, 100.00] [G loss: 11.010380]\n",
            "********* 4006 [D loss: 0.055178, acc:, 97.66] [G loss: 10.007753]\n",
            "********* 4007 [D loss: 0.123825, acc:, 96.09] [G loss: 10.472145]\n",
            "********* 4008 [D loss: 0.040739, acc:, 98.44] [G loss: 12.435857]\n",
            "********* 4009 [D loss: 0.015886, acc:, 100.00] [G loss: 13.353688]\n",
            "********* 4010 [D loss: 0.091630, acc:, 96.88] [G loss: 10.373800]\n",
            "********* 4011 [D loss: 0.053524, acc:, 97.66] [G loss: 7.913184]\n",
            "********* 4012 [D loss: 0.017504, acc:, 100.00] [G loss: 7.351078]\n",
            "********* 4013 [D loss: 0.041912, acc:, 99.22] [G loss: 7.596367]\n",
            "********* 4014 [D loss: 0.014941, acc:, 100.00] [G loss: 8.871571]\n",
            "********* 4015 [D loss: 0.020857, acc:, 99.22] [G loss: 8.765369]\n",
            "********* 4016 [D loss: 0.006724, acc:, 100.00] [G loss: 10.395273]\n",
            "********* 4017 [D loss: 0.042086, acc:, 98.44] [G loss: 9.474092]\n",
            "********* 4018 [D loss: 0.011891, acc:, 100.00] [G loss: 8.459438]\n",
            "********* 4019 [D loss: 0.040235, acc:, 97.66] [G loss: 8.530009]\n",
            "********* 4020 [D loss: 0.012638, acc:, 100.00] [G loss: 8.948837]\n",
            "********* 4021 [D loss: 0.005626, acc:, 100.00] [G loss: 10.412333]\n",
            "********* 4022 [D loss: 0.030185, acc:, 97.66] [G loss: 9.149788]\n",
            "********* 4023 [D loss: 0.034312, acc:, 98.44] [G loss: 8.761909]\n",
            "********* 4024 [D loss: 0.043424, acc:, 96.88] [G loss: 8.753851]\n",
            "********* 4025 [D loss: 0.042746, acc:, 98.44] [G loss: 8.607627]\n",
            "********* 4026 [D loss: 0.018380, acc:, 99.22] [G loss: 10.505274]\n",
            "********* 4027 [D loss: 0.038661, acc:, 98.44] [G loss: 9.481475]\n",
            "********* 4028 [D loss: 0.060571, acc:, 96.88] [G loss: 8.268466]\n",
            "********* 4029 [D loss: 0.020024, acc:, 99.22] [G loss: 7.075807]\n",
            "********* 4030 [D loss: 0.020449, acc:, 99.22] [G loss: 6.287625]\n",
            "********* 4031 [D loss: 0.008080, acc:, 100.00] [G loss: 7.274095]\n",
            "********* 4032 [D loss: 0.015014, acc:, 100.00] [G loss: 7.443913]\n",
            "********* 4033 [D loss: 0.010405, acc:, 100.00] [G loss: 7.996344]\n",
            "********* 4034 [D loss: 0.007105, acc:, 100.00] [G loss: 8.393069]\n",
            "********* 4035 [D loss: 0.046810, acc:, 98.44] [G loss: 8.813829]\n",
            "********* 4036 [D loss: 0.019142, acc:, 100.00] [G loss: 7.617213]\n",
            "********* 4037 [D loss: 0.064926, acc:, 96.88] [G loss: 6.480069]\n",
            "********* 4038 [D loss: 0.027182, acc:, 99.22] [G loss: 7.066615]\n",
            "********* 4039 [D loss: 0.014608, acc:, 100.00] [G loss: 7.748925]\n",
            "********* 4040 [D loss: 0.100021, acc:, 97.66] [G loss: 6.846928]\n",
            "********* 4041 [D loss: 0.092826, acc:, 96.88] [G loss: 7.968853]\n",
            "********* 4042 [D loss: 0.004879, acc:, 100.00] [G loss: 10.539171]\n",
            "********* 4043 [D loss: 0.172813, acc:, 96.09] [G loss: 7.541821]\n",
            "********* 4044 [D loss: 0.019697, acc:, 100.00] [G loss: 6.344443]\n",
            "********* 4045 [D loss: 0.049154, acc:, 99.22] [G loss: 8.262486]\n",
            "********* 4046 [D loss: 0.006647, acc:, 100.00] [G loss: 11.104071]\n",
            "********* 4047 [D loss: 0.039653, acc:, 98.44] [G loss: 12.299432]\n",
            "********* 4048 [D loss: 0.071508, acc:, 97.66] [G loss: 11.157567]\n",
            "********* 4049 [D loss: 0.022524, acc:, 99.22] [G loss: 9.683558]\n",
            "********* 4050 [D loss: 0.045535, acc:, 98.44] [G loss: 8.060472]\n",
            "********* 4051 [D loss: 0.044448, acc:, 97.66] [G loss: 7.308731]\n",
            "********* 4052 [D loss: 0.018114, acc:, 100.00] [G loss: 8.356443]\n",
            "********* 4053 [D loss: 0.017017, acc:, 100.00] [G loss: 9.203369]\n",
            "********* 4054 [D loss: 0.039402, acc:, 97.66] [G loss: 8.747936]\n",
            "********* 4055 [D loss: 0.024888, acc:, 99.22] [G loss: 8.981035]\n",
            "********* 4056 [D loss: 0.019842, acc:, 99.22] [G loss: 8.995814]\n",
            "********* 4057 [D loss: 0.003289, acc:, 100.00] [G loss: 9.749899]\n",
            "********* 4058 [D loss: 0.015256, acc:, 100.00] [G loss: 10.640247]\n",
            "********* 4059 [D loss: 0.062887, acc:, 98.44] [G loss: 9.076406]\n",
            "********* 4060 [D loss: 0.156583, acc:, 96.09] [G loss: 9.440309]\n",
            "********* 4061 [D loss: 0.004901, acc:, 100.00] [G loss: 9.310962]\n",
            "********* 4062 [D loss: 0.016931, acc:, 100.00] [G loss: 10.083860]\n",
            "********* 4063 [D loss: 0.019479, acc:, 100.00] [G loss: 9.734502]\n",
            "********* 4064 [D loss: 0.060551, acc:, 97.66] [G loss: 10.013812]\n",
            "********* 4065 [D loss: 0.028903, acc:, 98.44] [G loss: 8.059251]\n",
            "********* 4066 [D loss: 0.047545, acc:, 98.44] [G loss: 7.613975]\n",
            "********* 4067 [D loss: 0.057694, acc:, 97.66] [G loss: 10.669230]\n",
            "********* 4068 [D loss: 0.016820, acc:, 99.22] [G loss: 15.004253]\n",
            "********* 4069 [D loss: 0.212116, acc:, 90.62] [G loss: 7.648290]\n",
            "********* 4070 [D loss: 0.157386, acc:, 90.62] [G loss: 9.538784]\n",
            "********* 4071 [D loss: 0.025986, acc:, 99.22] [G loss: 17.521198]\n",
            "********* 4072 [D loss: 0.154531, acc:, 97.66] [G loss: 17.687891]\n",
            "********* 4073 [D loss: 0.071123, acc:, 97.66] [G loss: 14.712190]\n",
            "********* 4074 [D loss: 0.010870, acc:, 100.00] [G loss: 11.625526]\n",
            "********* 4075 [D loss: 0.011034, acc:, 100.00] [G loss: 9.441469]\n",
            "********* 4076 [D loss: 0.078005, acc:, 97.66] [G loss: 8.160625]\n",
            "********* 4077 [D loss: 0.018097, acc:, 99.22] [G loss: 7.751583]\n",
            "********* 4078 [D loss: 0.017112, acc:, 99.22] [G loss: 9.238165]\n",
            "********* 4079 [D loss: 0.124843, acc:, 98.44] [G loss: 10.945690]\n",
            "********* 4080 [D loss: 0.015330, acc:, 99.22] [G loss: 10.995138]\n",
            "********* 4081 [D loss: 0.014726, acc:, 100.00] [G loss: 9.455992]\n",
            "********* 4082 [D loss: 0.026060, acc:, 99.22] [G loss: 8.438741]\n",
            "********* 4083 [D loss: 0.018860, acc:, 100.00] [G loss: 7.617034]\n",
            "********* 4084 [D loss: 0.022681, acc:, 99.22] [G loss: 8.811423]\n",
            "********* 4085 [D loss: 0.007940, acc:, 100.00] [G loss: 10.826990]\n",
            "********* 4086 [D loss: 0.002104, acc:, 100.00] [G loss: 11.839718]\n",
            "********* 4087 [D loss: 0.079017, acc:, 98.44] [G loss: 11.709106]\n",
            "********* 4088 [D loss: 0.015303, acc:, 100.00] [G loss: 8.848906]\n",
            "********* 4089 [D loss: 0.052535, acc:, 97.66] [G loss: 6.654448]\n",
            "********* 4090 [D loss: 0.055239, acc:, 99.22] [G loss: 7.727949]\n",
            "********* 4091 [D loss: 0.009174, acc:, 100.00] [G loss: 10.374415]\n",
            "********* 4092 [D loss: 0.013939, acc:, 99.22] [G loss: 11.511968]\n",
            "********* 4093 [D loss: 0.103131, acc:, 96.09] [G loss: 8.405088]\n",
            "********* 4094 [D loss: 0.095922, acc:, 96.09] [G loss: 8.356112]\n",
            "********* 4095 [D loss: 0.005633, acc:, 100.00] [G loss: 12.804069]\n",
            "********* 4096 [D loss: 0.200630, acc:, 92.97] [G loss: 7.174552]\n",
            "********* 4097 [D loss: 0.279280, acc:, 89.06] [G loss: 7.346550]\n",
            "********* 4098 [D loss: 0.015400, acc:, 99.22] [G loss: 11.736694]\n",
            "********* 4099 [D loss: 0.068645, acc:, 97.66] [G loss: 14.390384]\n",
            "********* 4100 [D loss: 0.074274, acc:, 98.44] [G loss: 14.154894]\n",
            "********* 4101 [D loss: 0.163219, acc:, 96.09] [G loss: 10.832111]\n",
            "********* 4102 [D loss: 0.063504, acc:, 97.66] [G loss: 6.881071]\n",
            "********* 4103 [D loss: 0.259023, acc:, 89.06] [G loss: 7.001026]\n",
            "********* 4104 [D loss: 0.028737, acc:, 99.22] [G loss: 11.573471]\n",
            "********* 4105 [D loss: 0.032788, acc:, 99.22] [G loss: 15.737560]\n",
            "********* 4106 [D loss: 0.102115, acc:, 95.31] [G loss: 12.964590]\n",
            "********* 4107 [D loss: 0.010343, acc:, 100.00] [G loss: 12.270466]\n",
            "********* 4108 [D loss: 0.211729, acc:, 91.41] [G loss: 11.226654]\n",
            "********* 4109 [D loss: 0.041403, acc:, 98.44] [G loss: 12.759836]\n",
            "********* 4110 [D loss: 0.023061, acc:, 98.44] [G loss: 11.977935]\n",
            "********* 4111 [D loss: 0.009201, acc:, 100.00] [G loss: 11.634727]\n",
            "********* 4112 [D loss: 0.099894, acc:, 96.88] [G loss: 9.906343]\n",
            "********* 4113 [D loss: 0.146020, acc:, 94.53] [G loss: 9.370361]\n",
            "********* 4114 [D loss: 0.003730, acc:, 100.00] [G loss: 9.602387]\n",
            "********* 4115 [D loss: 0.038880, acc:, 99.22] [G loss: 8.812416]\n",
            "********* 4116 [D loss: 0.005492, acc:, 100.00] [G loss: 9.309006]\n",
            "********* 4117 [D loss: 0.028009, acc:, 98.44] [G loss: 9.764885]\n",
            "********* 4118 [D loss: 0.019886, acc:, 100.00] [G loss: 8.541496]\n",
            "********* 4119 [D loss: 0.031594, acc:, 99.22] [G loss: 9.428015]\n",
            "********* 4120 [D loss: 0.099956, acc:, 97.66] [G loss: 9.983753]\n",
            "********* 4121 [D loss: 0.021729, acc:, 100.00] [G loss: 11.115301]\n",
            "********* 4122 [D loss: 0.108928, acc:, 95.31] [G loss: 9.506215]\n",
            "********* 4123 [D loss: 0.082221, acc:, 97.66] [G loss: 7.466581]\n",
            "********* 4124 [D loss: 0.067441, acc:, 96.09] [G loss: 8.538805]\n",
            "********* 4125 [D loss: 0.016223, acc:, 100.00] [G loss: 10.152196]\n",
            "********* 4126 [D loss: 0.002186, acc:, 100.00] [G loss: 11.082706]\n",
            "********* 4127 [D loss: 0.005522, acc:, 100.00] [G loss: 11.691356]\n",
            "********* 4128 [D loss: 0.033572, acc:, 99.22] [G loss: 11.421250]\n",
            "********* 4129 [D loss: 0.068504, acc:, 96.88] [G loss: 8.858892]\n",
            "********* 4130 [D loss: 0.030118, acc:, 99.22] [G loss: 6.252246]\n",
            "********* 4131 [D loss: 0.157192, acc:, 96.09] [G loss: 6.929272]\n",
            "********* 4132 [D loss: 0.033581, acc:, 98.44] [G loss: 9.567631]\n",
            "********* 4133 [D loss: 0.011180, acc:, 100.00] [G loss: 12.234467]\n",
            "********* 4134 [D loss: 0.051107, acc:, 97.66] [G loss: 11.379604]\n",
            "********* 4135 [D loss: 0.102155, acc:, 96.88] [G loss: 7.955063]\n",
            "********* 4136 [D loss: 0.133589, acc:, 95.31] [G loss: 6.559185]\n",
            "********* 4137 [D loss: 0.103662, acc:, 94.53] [G loss: 9.643795]\n",
            "********* 4138 [D loss: 0.060756, acc:, 97.66] [G loss: 13.059169]\n",
            "********* 4139 [D loss: 0.053612, acc:, 96.88] [G loss: 13.215010]\n",
            "********* 4140 [D loss: 0.121192, acc:, 96.09] [G loss: 9.107193]\n",
            "********* 4141 [D loss: 0.066224, acc:, 97.66] [G loss: 8.412898]\n",
            "********* 4142 [D loss: 0.018579, acc:, 99.22] [G loss: 9.300882]\n",
            "********* 4143 [D loss: 0.034020, acc:, 98.44] [G loss: 11.264128]\n",
            "********* 4144 [D loss: 0.117825, acc:, 96.09] [G loss: 10.669373]\n",
            "********* 4145 [D loss: 0.017533, acc:, 98.44] [G loss: 10.691694]\n",
            "********* 4146 [D loss: 0.009378, acc:, 100.00] [G loss: 8.324318]\n",
            "********* 4147 [D loss: 0.019036, acc:, 100.00] [G loss: 9.641331]\n",
            "********* 4148 [D loss: 0.014205, acc:, 100.00] [G loss: 9.602482]\n",
            "********* 4149 [D loss: 0.031638, acc:, 99.22] [G loss: 9.769876]\n",
            "********* 4150 [D loss: 0.004426, acc:, 100.00] [G loss: 10.782158]\n",
            "********* 4151 [D loss: 0.031731, acc:, 98.44] [G loss: 9.833621]\n",
            "********* 4152 [D loss: 0.032237, acc:, 98.44] [G loss: 9.311842]\n",
            "********* 4153 [D loss: 0.010363, acc:, 100.00] [G loss: 7.681676]\n",
            "********* 4154 [D loss: 0.011354, acc:, 100.00] [G loss: 8.254396]\n",
            "********* 4155 [D loss: 0.045895, acc:, 98.44] [G loss: 7.460989]\n",
            "********* 4156 [D loss: 0.020232, acc:, 100.00] [G loss: 8.912704]\n",
            "********* 4157 [D loss: 0.015270, acc:, 99.22] [G loss: 10.002897]\n",
            "********* 4158 [D loss: 0.030015, acc:, 99.22] [G loss: 8.446767]\n",
            "********* 4159 [D loss: 0.030970, acc:, 99.22] [G loss: 8.224148]\n",
            "********* 4160 [D loss: 0.038444, acc:, 98.44] [G loss: 8.734754]\n",
            "********* 4161 [D loss: 0.039798, acc:, 99.22] [G loss: 7.289100]\n",
            "********* 4162 [D loss: 0.054514, acc:, 98.44] [G loss: 8.073761]\n",
            "********* 4163 [D loss: 0.024901, acc:, 99.22] [G loss: 9.279795]\n",
            "********* 4164 [D loss: 0.016450, acc:, 100.00] [G loss: 9.779390]\n",
            "********* 4165 [D loss: 0.129023, acc:, 96.88] [G loss: 7.864758]\n",
            "********* 4166 [D loss: 0.070452, acc:, 97.66] [G loss: 6.013234]\n",
            "********* 4167 [D loss: 0.094169, acc:, 96.09] [G loss: 5.633361]\n",
            "********* 4168 [D loss: 0.023160, acc:, 99.22] [G loss: 6.944404]\n",
            "********* 4169 [D loss: 0.047244, acc:, 97.66] [G loss: 8.887082]\n",
            "********* 4170 [D loss: 0.027239, acc:, 98.44] [G loss: 10.108850]\n",
            "********* 4171 [D loss: 0.018281, acc:, 99.22] [G loss: 10.044339]\n",
            "********* 4172 [D loss: 0.087039, acc:, 97.66] [G loss: 6.779577]\n",
            "********* 4173 [D loss: 0.128112, acc:, 95.31] [G loss: 10.177977]\n",
            "********* 4174 [D loss: 0.010259, acc:, 100.00] [G loss: 20.017960]\n",
            "********* 4175 [D loss: 0.762762, acc:, 84.38] [G loss: 4.596525]\n",
            "********* 4176 [D loss: 0.953543, acc:, 72.66] [G loss: 6.519879]\n",
            "********* 4177 [D loss: 0.058139, acc:, 97.66] [G loss: 20.116615]\n",
            "********* 4178 [D loss: 0.260783, acc:, 92.19] [G loss: 25.388401]\n",
            "********* 4179 [D loss: 0.681106, acc:, 86.72] [G loss: 12.702904]\n",
            "********* 4180 [D loss: 0.058571, acc:, 97.66] [G loss: 6.342259]\n",
            "********* 4181 [D loss: 0.376004, acc:, 85.94] [G loss: 7.291198]\n",
            "********* 4182 [D loss: 0.053892, acc:, 96.88] [G loss: 13.738353]\n",
            "********* 4183 [D loss: 0.006394, acc:, 100.00] [G loss: 20.723452]\n",
            "********* 4184 [D loss: 0.080212, acc:, 96.88] [G loss: 20.850069]\n",
            "********* 4185 [D loss: 0.385556, acc:, 88.28] [G loss: 15.029091]\n",
            "********* 4186 [D loss: 0.015145, acc:, 99.22] [G loss: 9.258635]\n",
            "********* 4187 [D loss: 0.112493, acc:, 96.09] [G loss: 7.881745]\n",
            "********* 4188 [D loss: 0.115461, acc:, 93.75] [G loss: 9.390940]\n",
            "********* 4189 [D loss: 0.010603, acc:, 99.22] [G loss: 11.559944]\n",
            "********* 4190 [D loss: 0.003389, acc:, 100.00] [G loss: 13.401457]\n",
            "********* 4191 [D loss: 0.033870, acc:, 99.22] [G loss: 14.711529]\n",
            "********* 4192 [D loss: 0.162083, acc:, 95.31] [G loss: 12.755310]\n",
            "********* 4193 [D loss: 0.145310, acc:, 96.09] [G loss: 9.809152]\n",
            "********* 4194 [D loss: 0.023268, acc:, 98.44] [G loss: 8.086279]\n",
            "********* 4195 [D loss: 0.038897, acc:, 99.22] [G loss: 7.867030]\n",
            "********* 4196 [D loss: 0.061030, acc:, 96.88] [G loss: 7.889873]\n",
            "********* 4197 [D loss: 0.077133, acc:, 97.66] [G loss: 7.399486]\n",
            "********* 4198 [D loss: 0.012602, acc:, 99.22] [G loss: 7.526490]\n",
            "********* 4199 [D loss: 0.009077, acc:, 100.00] [G loss: 8.669500]\n",
            "********* 4200 [D loss: 0.022283, acc:, 99.22] [G loss: 8.541779]\n",
            "********* 4201 [D loss: 0.008697, acc:, 100.00] [G loss: 8.505866]\n",
            "********* 4202 [D loss: 0.025933, acc:, 99.22] [G loss: 7.839139]\n",
            "********* 4203 [D loss: 0.026121, acc:, 99.22] [G loss: 7.341355]\n",
            "********* 4204 [D loss: 0.022265, acc:, 100.00] [G loss: 6.830894]\n",
            "********* 4205 [D loss: 0.036803, acc:, 99.22] [G loss: 5.920213]\n",
            "********* 4206 [D loss: 0.038117, acc:, 97.66] [G loss: 6.456058]\n",
            "********* 4207 [D loss: 0.045920, acc:, 99.22] [G loss: 7.072239]\n",
            "********* 4208 [D loss: 0.014651, acc:, 100.00] [G loss: 8.336720]\n",
            "********* 4209 [D loss: 0.039792, acc:, 98.44] [G loss: 7.721269]\n",
            "********* 4210 [D loss: 0.052632, acc:, 99.22] [G loss: 6.843340]\n",
            "********* 4211 [D loss: 0.041567, acc:, 98.44] [G loss: 7.194277]\n",
            "********* 4212 [D loss: 0.036976, acc:, 98.44] [G loss: 7.573115]\n",
            "********* 4213 [D loss: 0.024289, acc:, 99.22] [G loss: 7.441092]\n",
            "********* 4214 [D loss: 0.013828, acc:, 100.00] [G loss: 7.626498]\n",
            "********* 4215 [D loss: 0.018345, acc:, 100.00] [G loss: 6.484284]\n",
            "********* 4216 [D loss: 0.013675, acc:, 100.00] [G loss: 5.375084]\n",
            "********* 4217 [D loss: 0.059712, acc:, 98.44] [G loss: 4.563358]\n",
            "********* 4218 [D loss: 0.019023, acc:, 100.00] [G loss: 6.691719]\n",
            "********* 4219 [D loss: 0.036600, acc:, 98.44] [G loss: 7.880127]\n",
            "********* 4220 [D loss: 0.077352, acc:, 97.66] [G loss: 6.718223]\n",
            "********* 4221 [D loss: 0.030598, acc:, 98.44] [G loss: 6.273669]\n",
            "********* 4222 [D loss: 0.024866, acc:, 100.00] [G loss: 6.918510]\n",
            "********* 4223 [D loss: 0.028238, acc:, 100.00] [G loss: 6.984257]\n",
            "********* 4224 [D loss: 0.050156, acc:, 99.22] [G loss: 5.635059]\n",
            "********* 4225 [D loss: 0.024320, acc:, 100.00] [G loss: 5.727086]\n",
            "********* 4226 [D loss: 0.130921, acc:, 95.31] [G loss: 5.911806]\n",
            "********* 4227 [D loss: 0.007460, acc:, 100.00] [G loss: 8.411563]\n",
            "********* 4228 [D loss: 0.029363, acc:, 99.22] [G loss: 9.047874]\n",
            "********* 4229 [D loss: 0.034335, acc:, 98.44] [G loss: 7.745438]\n",
            "********* 4230 [D loss: 0.017436, acc:, 100.00] [G loss: 6.706537]\n",
            "********* 4231 [D loss: 0.027253, acc:, 100.00] [G loss: 6.472732]\n",
            "********* 4232 [D loss: 0.033607, acc:, 97.66] [G loss: 7.864787]\n",
            "********* 4233 [D loss: 0.015071, acc:, 100.00] [G loss: 9.374901]\n",
            "********* 4234 [D loss: 0.025540, acc:, 98.44] [G loss: 9.456074]\n",
            "********* 4235 [D loss: 0.029236, acc:, 98.44] [G loss: 6.564241]\n",
            "********* 4236 [D loss: 0.064756, acc:, 96.88] [G loss: 4.995363]\n",
            "********* 4237 [D loss: 0.036683, acc:, 99.22] [G loss: 4.432884]\n",
            "********* 4238 [D loss: 0.039909, acc:, 99.22] [G loss: 6.642712]\n",
            "********* 4239 [D loss: 0.021352, acc:, 99.22] [G loss: 8.318326]\n",
            "********* 4240 [D loss: 0.016413, acc:, 100.00] [G loss: 8.220628]\n",
            "********* 4241 [D loss: 0.012759, acc:, 100.00] [G loss: 8.890356]\n",
            "********* 4242 [D loss: 0.026670, acc:, 99.22] [G loss: 7.714647]\n",
            "********* 4243 [D loss: 0.061401, acc:, 96.88] [G loss: 7.289595]\n",
            "********* 4244 [D loss: 0.034840, acc:, 99.22] [G loss: 8.453318]\n",
            "********* 4245 [D loss: 0.033406, acc:, 98.44] [G loss: 9.047024]\n",
            "********* 4246 [D loss: 0.035410, acc:, 98.44] [G loss: 8.941911]\n",
            "********* 4247 [D loss: 0.027140, acc:, 98.44] [G loss: 6.836080]\n",
            "********* 4248 [D loss: 0.027013, acc:, 100.00] [G loss: 6.562990]\n",
            "********* 4249 [D loss: 0.014807, acc:, 99.22] [G loss: 8.040185]\n",
            "********* 4250 [D loss: 0.042063, acc:, 98.44] [G loss: 8.091034]\n",
            "********* 4251 [D loss: 0.019568, acc:, 99.22] [G loss: 7.367333]\n",
            "********* 4252 [D loss: 0.008641, acc:, 100.00] [G loss: 7.147939]\n",
            "********* 4253 [D loss: 0.018891, acc:, 99.22] [G loss: 7.666837]\n",
            "********* 4254 [D loss: 0.019439, acc:, 98.44] [G loss: 6.855017]\n",
            "********* 4255 [D loss: 0.015746, acc:, 99.22] [G loss: 7.470879]\n",
            "********* 4256 [D loss: 0.014404, acc:, 100.00] [G loss: 9.241948]\n",
            "********* 4257 [D loss: 0.029493, acc:, 98.44] [G loss: 7.145937]\n",
            "********* 4258 [D loss: 0.037075, acc:, 99.22] [G loss: 6.560656]\n",
            "********* 4259 [D loss: 0.043799, acc:, 97.66] [G loss: 6.397740]\n",
            "********* 4260 [D loss: 0.008744, acc:, 100.00] [G loss: 9.480972]\n",
            "********* 4261 [D loss: 0.015996, acc:, 100.00] [G loss: 9.718205]\n",
            "********* 4262 [D loss: 0.057720, acc:, 97.66] [G loss: 6.538726]\n",
            "********* 4263 [D loss: 0.021733, acc:, 100.00] [G loss: 5.179689]\n",
            "********* 4264 [D loss: 0.017804, acc:, 100.00] [G loss: 5.816010]\n",
            "********* 4265 [D loss: 0.011357, acc:, 100.00] [G loss: 9.481176]\n",
            "********* 4266 [D loss: 0.013514, acc:, 99.22] [G loss: 10.653571]\n",
            "********* 4267 [D loss: 0.083509, acc:, 95.31] [G loss: 6.324088]\n",
            "********* 4268 [D loss: 0.086533, acc:, 96.09] [G loss: 7.132358]\n",
            "********* 4269 [D loss: 0.020844, acc:, 100.00] [G loss: 10.420116]\n",
            "********* 4270 [D loss: 0.046855, acc:, 97.66] [G loss: 11.425022]\n",
            "********* 4271 [D loss: 0.062011, acc:, 96.88] [G loss: 8.719600]\n",
            "********* 4272 [D loss: 0.010978, acc:, 100.00] [G loss: 6.467725]\n",
            "********* 4273 [D loss: 0.028749, acc:, 99.22] [G loss: 5.855654]\n",
            "********* 4274 [D loss: 0.021106, acc:, 100.00] [G loss: 7.795079]\n",
            "********* 4275 [D loss: 0.013206, acc:, 99.22] [G loss: 9.439747]\n",
            "********* 4276 [D loss: 0.005984, acc:, 100.00] [G loss: 9.824518]\n",
            "********* 4277 [D loss: 0.023824, acc:, 99.22] [G loss: 9.341109]\n",
            "********* 4278 [D loss: 0.010393, acc:, 100.00] [G loss: 7.972687]\n",
            "********* 4279 [D loss: 0.036158, acc:, 99.22] [G loss: 6.664790]\n",
            "********* 4280 [D loss: 0.011440, acc:, 100.00] [G loss: 5.966922]\n",
            "********* 4281 [D loss: 0.019121, acc:, 100.00] [G loss: 6.269835]\n",
            "********* 4282 [D loss: 0.091863, acc:, 96.09] [G loss: 7.195596]\n",
            "********* 4283 [D loss: 0.010643, acc:, 100.00] [G loss: 8.616508]\n",
            "********* 4284 [D loss: 0.059334, acc:, 99.22] [G loss: 11.149076]\n",
            "********* 4285 [D loss: 0.125491, acc:, 96.09] [G loss: 8.389243]\n",
            "********* 4286 [D loss: 0.037086, acc:, 98.44] [G loss: 7.472982]\n",
            "********* 4287 [D loss: 0.040415, acc:, 98.44] [G loss: 7.880469]\n",
            "********* 4288 [D loss: 0.036065, acc:, 99.22] [G loss: 10.096622]\n",
            "********* 4289 [D loss: 0.148139, acc:, 96.88] [G loss: 7.627355]\n",
            "********* 4290 [D loss: 0.059078, acc:, 98.44] [G loss: 7.824634]\n",
            "********* 4291 [D loss: 0.004517, acc:, 100.00] [G loss: 11.099694]\n",
            "********* 4292 [D loss: 0.059387, acc:, 97.66] [G loss: 11.776982]\n",
            "********* 4293 [D loss: 0.059275, acc:, 97.66] [G loss: 9.974544]\n",
            "********* 4294 [D loss: 0.045416, acc:, 98.44] [G loss: 8.350605]\n",
            "********* 4295 [D loss: 0.012048, acc:, 100.00] [G loss: 7.951066]\n",
            "********* 4296 [D loss: 0.015797, acc:, 100.00] [G loss: 8.056045]\n",
            "********* 4297 [D loss: 0.018509, acc:, 99.22] [G loss: 9.792293]\n",
            "********* 4298 [D loss: 0.061987, acc:, 98.44] [G loss: 10.724618]\n",
            "********* 4299 [D loss: 0.062536, acc:, 99.22] [G loss: 11.421099]\n",
            "********* 4300 [D loss: 0.021519, acc:, 99.22] [G loss: 9.257021]\n",
            "********* 4301 [D loss: 0.159100, acc:, 98.44] [G loss: 7.289567]\n",
            "********* 4302 [D loss: 0.055324, acc:, 98.44] [G loss: 7.141315]\n",
            "********* 4303 [D loss: 0.024268, acc:, 99.22] [G loss: 8.657082]\n",
            "********* 4304 [D loss: 0.005161, acc:, 100.00] [G loss: 12.091045]\n",
            "********* 4305 [D loss: 0.008710, acc:, 100.00] [G loss: 14.643588]\n",
            "********* 4306 [D loss: 0.118618, acc:, 94.53] [G loss: 10.334050]\n",
            "********* 4307 [D loss: 0.041130, acc:, 98.44] [G loss: 8.015289]\n",
            "********* 4308 [D loss: 0.251121, acc:, 91.41] [G loss: 11.508448]\n",
            "********* 4309 [D loss: 0.038792, acc:, 99.22] [G loss: 15.803913]\n",
            "********* 4310 [D loss: 0.188290, acc:, 92.97] [G loss: 11.155100]\n",
            "********* 4311 [D loss: 0.039272, acc:, 99.22] [G loss: 7.623113]\n",
            "********* 4312 [D loss: 0.089072, acc:, 96.09] [G loss: 6.289963]\n",
            "********* 4313 [D loss: 0.078045, acc:, 96.88] [G loss: 9.177913]\n",
            "********* 4314 [D loss: 0.001004, acc:, 100.00] [G loss: 13.602575]\n",
            "********* 4315 [D loss: 0.003989, acc:, 100.00] [G loss: 16.050293]\n",
            "********* 4316 [D loss: 0.044524, acc:, 97.66] [G loss: 15.718583]\n",
            "********* 4317 [D loss: 0.043766, acc:, 98.44] [G loss: 13.372519]\n",
            "********* 4318 [D loss: 0.020360, acc:, 99.22] [G loss: 9.837403]\n",
            "********* 4319 [D loss: 0.058438, acc:, 98.44] [G loss: 8.903047]\n",
            "********* 4320 [D loss: 0.010848, acc:, 100.00] [G loss: 10.560705]\n",
            "********* 4321 [D loss: 0.058722, acc:, 97.66] [G loss: 9.631672]\n",
            "********* 4322 [D loss: 0.011884, acc:, 99.22] [G loss: 11.172704]\n",
            "********* 4323 [D loss: 0.008702, acc:, 100.00] [G loss: 11.034778]\n",
            "********* 4324 [D loss: 0.011500, acc:, 100.00] [G loss: 9.743207]\n",
            "********* 4325 [D loss: 0.002947, acc:, 100.00] [G loss: 9.914104]\n",
            "********* 4326 [D loss: 0.027848, acc:, 99.22] [G loss: 11.166121]\n",
            "********* 4327 [D loss: 0.002007, acc:, 100.00] [G loss: 12.305641]\n",
            "********* 4328 [D loss: 0.025219, acc:, 98.44] [G loss: 11.207625]\n",
            "********* 4329 [D loss: 0.027360, acc:, 99.22] [G loss: 9.728476]\n",
            "********* 4330 [D loss: 0.038415, acc:, 98.44] [G loss: 8.679041]\n",
            "********* 4331 [D loss: 0.021744, acc:, 100.00] [G loss: 8.318464]\n",
            "********* 4332 [D loss: 0.011632, acc:, 100.00] [G loss: 10.742864]\n",
            "********* 4333 [D loss: 0.066558, acc:, 98.44] [G loss: 8.563309]\n",
            "********* 4334 [D loss: 0.012676, acc:, 100.00] [G loss: 8.806873]\n",
            "********* 4335 [D loss: 0.026887, acc:, 99.22] [G loss: 8.286629]\n",
            "********* 4336 [D loss: 0.122357, acc:, 95.31] [G loss: 5.581947]\n",
            "********* 4337 [D loss: 0.105141, acc:, 94.53] [G loss: 9.026652]\n",
            "********* 4338 [D loss: 0.032593, acc:, 99.22] [G loss: 13.650486]\n",
            "********* 4339 [D loss: 0.145135, acc:, 95.31] [G loss: 11.775890]\n",
            "********* 4340 [D loss: 0.069204, acc:, 96.88] [G loss: 8.036770]\n",
            "********* 4341 [D loss: 0.058279, acc:, 98.44] [G loss: 7.747549]\n",
            "********* 4342 [D loss: 0.011895, acc:, 99.22] [G loss: 11.226139]\n",
            "********* 4343 [D loss: 0.019593, acc:, 99.22] [G loss: 11.299368]\n",
            "********* 4344 [D loss: 0.040680, acc:, 98.44] [G loss: 9.822679]\n",
            "********* 4345 [D loss: 0.026798, acc:, 99.22] [G loss: 8.163866]\n",
            "********* 4346 [D loss: 0.007412, acc:, 100.00] [G loss: 7.922897]\n",
            "********* 4347 [D loss: 0.014645, acc:, 100.00] [G loss: 8.090700]\n",
            "********* 4348 [D loss: 0.004057, acc:, 100.00] [G loss: 8.706280]\n",
            "********* 4349 [D loss: 0.015706, acc:, 99.22] [G loss: 8.663680]\n",
            "********* 4350 [D loss: 0.052723, acc:, 99.22] [G loss: 8.382393]\n",
            "********* 4351 [D loss: 0.031968, acc:, 99.22] [G loss: 8.586792]\n",
            "********* 4352 [D loss: 0.020821, acc:, 99.22] [G loss: 7.759354]\n",
            "********* 4353 [D loss: 0.104482, acc:, 95.31] [G loss: 8.787081]\n",
            "********* 4354 [D loss: 0.048575, acc:, 97.66] [G loss: 8.555738]\n",
            "********* 4355 [D loss: 0.023217, acc:, 99.22] [G loss: 7.557394]\n",
            "********* 4356 [D loss: 0.109836, acc:, 95.31] [G loss: 6.013329]\n",
            "********* 4357 [D loss: 0.107305, acc:, 96.09] [G loss: 7.888371]\n",
            "********* 4358 [D loss: 0.003671, acc:, 100.00] [G loss: 12.261385]\n",
            "********* 4359 [D loss: 0.031710, acc:, 98.44] [G loss: 10.097543]\n",
            "********* 4360 [D loss: 0.044825, acc:, 97.66] [G loss: 7.233602]\n",
            "********* 4361 [D loss: 0.050146, acc:, 98.44] [G loss: 6.977668]\n",
            "********* 4362 [D loss: 0.009839, acc:, 100.00] [G loss: 9.341869]\n",
            "********* 4363 [D loss: 0.004525, acc:, 100.00] [G loss: 9.319171]\n",
            "********* 4364 [D loss: 0.015473, acc:, 99.22] [G loss: 9.532601]\n",
            "********* 4365 [D loss: 0.015530, acc:, 99.22] [G loss: 8.946039]\n",
            "********* 4366 [D loss: 0.023287, acc:, 99.22] [G loss: 8.907503]\n",
            "********* 4367 [D loss: 0.035851, acc:, 98.44] [G loss: 9.544167]\n",
            "********* 4368 [D loss: 0.043260, acc:, 98.44] [G loss: 11.048409]\n",
            "********* 4369 [D loss: 0.042774, acc:, 98.44] [G loss: 9.154764]\n",
            "********* 4370 [D loss: 0.120469, acc:, 97.66] [G loss: 8.338993]\n",
            "********* 4371 [D loss: 0.019436, acc:, 100.00] [G loss: 8.798340]\n",
            "********* 4372 [D loss: 0.016282, acc:, 100.00] [G loss: 9.865908]\n",
            "********* 4373 [D loss: 0.110329, acc:, 96.88] [G loss: 7.502903]\n",
            "********* 4374 [D loss: 0.054738, acc:, 97.66] [G loss: 7.090961]\n",
            "********* 4375 [D loss: 0.034956, acc:, 98.44] [G loss: 9.100289]\n",
            "********* 4376 [D loss: 0.014229, acc:, 99.22] [G loss: 9.481374]\n",
            "********* 4377 [D loss: 0.028407, acc:, 99.22] [G loss: 9.515459]\n",
            "********* 4378 [D loss: 0.105905, acc:, 96.09] [G loss: 6.838493]\n",
            "********* 4379 [D loss: 0.026444, acc:, 99.22] [G loss: 7.512505]\n",
            "********* 4380 [D loss: 0.156929, acc:, 99.22] [G loss: 7.186839]\n",
            "********* 4381 [D loss: 0.008308, acc:, 100.00] [G loss: 8.997210]\n",
            "********* 4382 [D loss: 0.049924, acc:, 98.44] [G loss: 9.337171]\n",
            "********* 4383 [D loss: 0.079983, acc:, 96.88] [G loss: 10.039949]\n",
            "********* 4384 [D loss: 0.040922, acc:, 98.44] [G loss: 10.178010]\n",
            "********* 4385 [D loss: 0.030819, acc:, 98.44] [G loss: 10.408674]\n",
            "********* 4386 [D loss: 0.022155, acc:, 99.22] [G loss: 9.585321]\n",
            "********* 4387 [D loss: 0.217376, acc:, 89.06] [G loss: 9.682875]\n",
            "********* 4388 [D loss: 0.138844, acc:, 94.53] [G loss: 9.535246]\n",
            "********* 4389 [D loss: 0.246993, acc:, 92.19] [G loss: 8.659551]\n",
            "********* 4390 [D loss: 0.009202, acc:, 100.00] [G loss: 12.836739]\n",
            "********* 4391 [D loss: 0.018348, acc:, 99.22] [G loss: 14.961876]\n",
            "********* 4392 [D loss: 0.155923, acc:, 95.31] [G loss: 14.588989]\n",
            "********* 4393 [D loss: 0.326466, acc:, 94.53] [G loss: 10.667931]\n",
            "********* 4394 [D loss: 0.009349, acc:, 100.00] [G loss: 7.559348]\n",
            "********* 4395 [D loss: 0.079611, acc:, 96.88] [G loss: 7.201896]\n",
            "********* 4396 [D loss: 0.071768, acc:, 98.44] [G loss: 9.277708]\n",
            "********* 4397 [D loss: 0.112954, acc:, 95.31] [G loss: 8.914482]\n",
            "********* 4398 [D loss: 0.004005, acc:, 100.00] [G loss: 10.372679]\n",
            "********* 4399 [D loss: 0.007865, acc:, 100.00] [G loss: 10.726957]\n",
            "********* 4400 [D loss: 0.013382, acc:, 100.00] [G loss: 13.812302]\n",
            "********* 4401 [D loss: 0.010348, acc:, 100.00] [G loss: 14.144922]\n",
            "********* 4402 [D loss: 0.122360, acc:, 95.31] [G loss: 10.626204]\n",
            "********* 4403 [D loss: 0.008106, acc:, 100.00] [G loss: 9.587453]\n",
            "********* 4404 [D loss: 0.036504, acc:, 98.44] [G loss: 9.540519]\n",
            "********* 4405 [D loss: 0.007599, acc:, 100.00] [G loss: 11.015450]\n",
            "********* 4406 [D loss: 0.017661, acc:, 99.22] [G loss: 13.307497]\n",
            "********* 4407 [D loss: 0.067961, acc:, 97.66] [G loss: 10.534575]\n",
            "********* 4408 [D loss: 0.008968, acc:, 100.00] [G loss: 7.935329]\n",
            "********* 4409 [D loss: 0.056710, acc:, 96.88] [G loss: 8.370309]\n",
            "********* 4410 [D loss: 0.002794, acc:, 100.00] [G loss: 9.578944]\n",
            "********* 4411 [D loss: 0.005039, acc:, 100.00] [G loss: 11.114811]\n",
            "********* 4412 [D loss: 0.019693, acc:, 99.22] [G loss: 9.763330]\n",
            "********* 4413 [D loss: 0.059588, acc:, 97.66] [G loss: 9.822248]\n",
            "********* 4414 [D loss: 0.022783, acc:, 99.22] [G loss: 10.695288]\n",
            "********* 4415 [D loss: 0.023014, acc:, 99.22] [G loss: 10.231325]\n",
            "********* 4416 [D loss: 0.016178, acc:, 100.00] [G loss: 10.570932]\n",
            "********* 4417 [D loss: 0.052327, acc:, 96.88] [G loss: 7.834943]\n",
            "********* 4418 [D loss: 0.041245, acc:, 98.44] [G loss: 7.202714]\n",
            "********* 4419 [D loss: 0.024055, acc:, 99.22] [G loss: 8.270235]\n",
            "********* 4420 [D loss: 0.023318, acc:, 99.22] [G loss: 10.304312]\n",
            "********* 4421 [D loss: 0.024854, acc:, 99.22] [G loss: 12.067464]\n",
            "********* 4422 [D loss: 0.175080, acc:, 95.31] [G loss: 6.356570]\n",
            "********* 4423 [D loss: 0.134544, acc:, 95.31] [G loss: 8.231161]\n",
            "********* 4424 [D loss: 0.030596, acc:, 98.44] [G loss: 14.633162]\n",
            "********* 4425 [D loss: 0.123351, acc:, 96.88] [G loss: 12.747210]\n",
            "********* 4426 [D loss: 0.037590, acc:, 98.44] [G loss: 9.446142]\n",
            "********* 4427 [D loss: 0.018965, acc:, 100.00] [G loss: 7.216655]\n",
            "********* 4428 [D loss: 0.039755, acc:, 98.44] [G loss: 7.705713]\n",
            "********* 4429 [D loss: 0.013564, acc:, 100.00] [G loss: 9.064721]\n",
            "********* 4430 [D loss: 0.014371, acc:, 100.00] [G loss: 10.247468]\n",
            "********* 4431 [D loss: 0.091662, acc:, 96.09] [G loss: 8.646534]\n",
            "********* 4432 [D loss: 0.054933, acc:, 97.66] [G loss: 8.232637]\n",
            "********* 4433 [D loss: 0.023018, acc:, 99.22] [G loss: 8.256114]\n",
            "********* 4434 [D loss: 0.013966, acc:, 99.22] [G loss: 8.537767]\n",
            "********* 4435 [D loss: 0.013850, acc:, 100.00] [G loss: 8.386060]\n",
            "********* 4436 [D loss: 0.003761, acc:, 100.00] [G loss: 7.996042]\n",
            "********* 4437 [D loss: 0.018297, acc:, 99.22] [G loss: 7.693068]\n",
            "********* 4438 [D loss: 0.081909, acc:, 96.88] [G loss: 7.185470]\n",
            "********* 4439 [D loss: 0.051033, acc:, 98.44] [G loss: 7.168845]\n",
            "********* 4440 [D loss: 0.007500, acc:, 100.00] [G loss: 9.995773]\n",
            "********* 4441 [D loss: 0.024422, acc:, 99.22] [G loss: 10.815926]\n",
            "********* 4442 [D loss: 0.029446, acc:, 98.44] [G loss: 9.423742]\n",
            "********* 4443 [D loss: 0.008610, acc:, 100.00] [G loss: 9.530031]\n",
            "********* 4444 [D loss: 0.023686, acc:, 99.22] [G loss: 9.297146]\n",
            "********* 4445 [D loss: 0.011553, acc:, 100.00] [G loss: 9.897146]\n",
            "********* 4446 [D loss: 0.015952, acc:, 99.22] [G loss: 11.297050]\n",
            "********* 4447 [D loss: 0.067444, acc:, 98.44] [G loss: 9.289335]\n",
            "********* 4448 [D loss: 0.016792, acc:, 100.00] [G loss: 9.900812]\n",
            "********* 4449 [D loss: 0.008385, acc:, 100.00] [G loss: 10.809340]\n",
            "********* 4450 [D loss: 0.067319, acc:, 97.66] [G loss: 10.057614]\n",
            "********* 4451 [D loss: 0.023479, acc:, 98.44] [G loss: 10.903731]\n",
            "********* 4452 [D loss: 0.037293, acc:, 99.22] [G loss: 9.301688]\n",
            "********* 4453 [D loss: 0.076117, acc:, 98.44] [G loss: 9.891872]\n",
            "********* 4454 [D loss: 0.054918, acc:, 96.88] [G loss: 8.670036]\n",
            "********* 4455 [D loss: 0.043585, acc:, 99.22] [G loss: 8.699810]\n",
            "********* 4456 [D loss: 0.041036, acc:, 98.44] [G loss: 9.983964]\n",
            "********* 4457 [D loss: 0.059655, acc:, 96.88] [G loss: 7.259646]\n",
            "********* 4458 [D loss: 0.012714, acc:, 100.00] [G loss: 8.397603]\n",
            "********* 4459 [D loss: 0.017838, acc:, 100.00] [G loss: 8.437926]\n",
            "********* 4460 [D loss: 0.048620, acc:, 97.66] [G loss: 10.990764]\n",
            "********* 4461 [D loss: 0.049143, acc:, 99.22] [G loss: 13.091356]\n",
            "********* 4462 [D loss: 0.013825, acc:, 100.00] [G loss: 11.401859]\n",
            "********* 4463 [D loss: 0.046988, acc:, 97.66] [G loss: 8.363630]\n",
            "********* 4464 [D loss: 0.136154, acc:, 97.66] [G loss: 5.387208]\n",
            "********* 4465 [D loss: 0.041952, acc:, 97.66] [G loss: 10.020995]\n",
            "********* 4466 [D loss: 0.009619, acc:, 100.00] [G loss: 11.658648]\n",
            "********* 4467 [D loss: 0.001781, acc:, 100.00] [G loss: 13.849957]\n",
            "********* 4468 [D loss: 0.009425, acc:, 100.00] [G loss: 12.431561]\n",
            "********* 4469 [D loss: 0.083046, acc:, 98.44] [G loss: 11.214306]\n",
            "********* 4470 [D loss: 0.034539, acc:, 98.44] [G loss: 8.695752]\n",
            "********* 4471 [D loss: 0.008049, acc:, 100.00] [G loss: 7.864459]\n",
            "********* 4472 [D loss: 0.020498, acc:, 99.22] [G loss: 7.590132]\n",
            "********* 4473 [D loss: 0.016041, acc:, 99.22] [G loss: 8.731691]\n",
            "********* 4474 [D loss: 0.009976, acc:, 100.00] [G loss: 8.980682]\n",
            "********* 4475 [D loss: 0.023859, acc:, 99.22] [G loss: 8.968613]\n",
            "********* 4476 [D loss: 0.021936, acc:, 99.22] [G loss: 8.923317]\n",
            "********* 4477 [D loss: 0.003793, acc:, 100.00] [G loss: 9.942251]\n",
            "********* 4478 [D loss: 0.011331, acc:, 100.00] [G loss: 12.298120]\n",
            "********* 4479 [D loss: 0.039718, acc:, 97.66] [G loss: 10.741603]\n",
            "********* 4480 [D loss: 0.015292, acc:, 100.00] [G loss: 10.154694]\n",
            "********* 4481 [D loss: 0.171816, acc:, 97.66] [G loss: 8.420340]\n",
            "********* 4482 [D loss: 0.089410, acc:, 96.88] [G loss: 11.130022]\n",
            "********* 4483 [D loss: 0.013006, acc:, 99.22] [G loss: 16.745876]\n",
            "********* 4484 [D loss: 0.051957, acc:, 98.44] [G loss: 15.224447]\n",
            "********* 4485 [D loss: 0.031169, acc:, 99.22] [G loss: 10.178181]\n",
            "********* 4486 [D loss: 0.023328, acc:, 99.22] [G loss: 8.251350]\n",
            "********* 4487 [D loss: 0.027255, acc:, 99.22] [G loss: 7.886064]\n",
            "********* 4488 [D loss: 0.025659, acc:, 99.22] [G loss: 12.143377]\n",
            "********* 4489 [D loss: 0.022976, acc:, 99.22] [G loss: 11.984928]\n",
            "********* 4490 [D loss: 0.031306, acc:, 99.22] [G loss: 13.658298]\n",
            "********* 4491 [D loss: 0.011680, acc:, 100.00] [G loss: 13.303094]\n",
            "********* 4492 [D loss: 0.009129, acc:, 99.22] [G loss: 11.054167]\n",
            "********* 4493 [D loss: 0.008770, acc:, 99.22] [G loss: 9.786290]\n",
            "********* 4494 [D loss: 0.024014, acc:, 98.44] [G loss: 9.349391]\n",
            "********* 4495 [D loss: 0.059556, acc:, 97.66] [G loss: 8.923042]\n",
            "********* 4496 [D loss: 0.037458, acc:, 99.22] [G loss: 9.718865]\n",
            "********* 4497 [D loss: 0.034866, acc:, 98.44] [G loss: 11.255499]\n",
            "********* 4498 [D loss: 0.072802, acc:, 97.66] [G loss: 9.431448]\n",
            "********* 4499 [D loss: 0.066348, acc:, 97.66] [G loss: 7.396288]\n",
            "********* 4500 [D loss: 0.081057, acc:, 96.09] [G loss: 11.006470]\n",
            "********* 4501 [D loss: 0.002578, acc:, 100.00] [G loss: 16.954199]\n",
            "********* 4502 [D loss: 0.151894, acc:, 96.09] [G loss: 14.821518]\n",
            "********* 4503 [D loss: 0.065783, acc:, 96.09] [G loss: 9.487966]\n",
            "********* 4504 [D loss: 0.027885, acc:, 98.44] [G loss: 6.694980]\n",
            "********* 4505 [D loss: 0.073173, acc:, 96.09] [G loss: 9.373299]\n",
            "********* 4506 [D loss: 0.001797, acc:, 100.00] [G loss: 17.203201]\n",
            "********* 4507 [D loss: 0.132122, acc:, 96.09] [G loss: 15.635775]\n",
            "********* 4508 [D loss: 0.081489, acc:, 98.44] [G loss: 12.245802]\n",
            "********* 4509 [D loss: 0.008570, acc:, 100.00] [G loss: 9.043650]\n",
            "********* 4510 [D loss: 0.039182, acc:, 97.66] [G loss: 8.430161]\n",
            "********* 4511 [D loss: 0.036846, acc:, 99.22] [G loss: 9.395503]\n",
            "********* 4512 [D loss: 0.007029, acc:, 100.00] [G loss: 10.190853]\n",
            "********* 4513 [D loss: 0.014044, acc:, 99.22] [G loss: 11.462584]\n",
            "********* 4514 [D loss: 0.016146, acc:, 99.22] [G loss: 11.992809]\n",
            "********* 4515 [D loss: 0.004849, acc:, 100.00] [G loss: 13.247724]\n",
            "********* 4516 [D loss: 0.047276, acc:, 99.22] [G loss: 12.106420]\n",
            "********* 4517 [D loss: 0.021572, acc:, 99.22] [G loss: 11.339031]\n",
            "********* 4518 [D loss: 0.005178, acc:, 100.00] [G loss: 11.686637]\n",
            "********* 4519 [D loss: 0.038508, acc:, 99.22] [G loss: 11.189734]\n",
            "********* 4520 [D loss: 0.052861, acc:, 98.44] [G loss: 10.445183]\n",
            "********* 4521 [D loss: 0.044619, acc:, 99.22] [G loss: 8.504084]\n",
            "********* 4522 [D loss: 0.038585, acc:, 99.22] [G loss: 8.860731]\n",
            "********* 4523 [D loss: 0.003699, acc:, 100.00] [G loss: 11.602608]\n",
            "********* 4524 [D loss: 0.019074, acc:, 99.22] [G loss: 10.547686]\n",
            "********* 4525 [D loss: 0.103402, acc:, 97.66] [G loss: 10.041397]\n",
            "********* 4526 [D loss: 0.083575, acc:, 96.09] [G loss: 9.256269]\n",
            "********* 4527 [D loss: 0.008420, acc:, 100.00] [G loss: 10.624041]\n",
            "********* 4528 [D loss: 0.067323, acc:, 96.88] [G loss: 9.797571]\n",
            "********* 4529 [D loss: 0.015836, acc:, 100.00] [G loss: 9.561519]\n",
            "********* 4530 [D loss: 0.046117, acc:, 96.88] [G loss: 8.654947]\n",
            "********* 4531 [D loss: 0.013833, acc:, 100.00] [G loss: 10.025930]\n",
            "********* 4532 [D loss: 0.018098, acc:, 99.22] [G loss: 10.537910]\n",
            "********* 4533 [D loss: 0.045922, acc:, 98.44] [G loss: 10.224313]\n",
            "********* 4534 [D loss: 0.061479, acc:, 97.66] [G loss: 9.987794]\n",
            "********* 4535 [D loss: 0.013713, acc:, 100.00] [G loss: 8.979152]\n",
            "********* 4536 [D loss: 0.048361, acc:, 97.66] [G loss: 9.699839]\n",
            "********* 4537 [D loss: 0.059027, acc:, 96.88] [G loss: 10.370068]\n",
            "********* 4538 [D loss: 0.020934, acc:, 99.22] [G loss: 9.846137]\n",
            "********* 4539 [D loss: 0.072976, acc:, 98.44] [G loss: 8.019789]\n",
            "********* 4540 [D loss: 0.020522, acc:, 99.22] [G loss: 8.709751]\n",
            "********* 4541 [D loss: 0.009824, acc:, 99.22] [G loss: 10.631353]\n",
            "********* 4542 [D loss: 0.021803, acc:, 99.22] [G loss: 11.223537]\n",
            "********* 4543 [D loss: 0.026455, acc:, 99.22] [G loss: 8.785810]\n",
            "********* 4544 [D loss: 0.026243, acc:, 99.22] [G loss: 8.278326]\n",
            "********* 4545 [D loss: 0.014527, acc:, 100.00] [G loss: 10.151550]\n",
            "********* 4546 [D loss: 0.019054, acc:, 99.22] [G loss: 10.476791]\n",
            "********* 4547 [D loss: 0.009274, acc:, 100.00] [G loss: 9.394279]\n",
            "********* 4548 [D loss: 0.070557, acc:, 96.88] [G loss: 9.559662]\n",
            "********* 4549 [D loss: 0.061656, acc:, 97.66] [G loss: 8.205791]\n",
            "********* 4550 [D loss: 0.030553, acc:, 97.66] [G loss: 10.336039]\n",
            "********* 4551 [D loss: 0.019000, acc:, 100.00] [G loss: 11.013010]\n",
            "********* 4552 [D loss: 0.019328, acc:, 100.00] [G loss: 8.451046]\n",
            "********* 4553 [D loss: 0.039148, acc:, 97.66] [G loss: 6.759690]\n",
            "********* 4554 [D loss: 0.061174, acc:, 98.44] [G loss: 11.885485]\n",
            "********* 4555 [D loss: 0.001898, acc:, 100.00] [G loss: 18.358006]\n",
            "********* 4556 [D loss: 0.187312, acc:, 95.31] [G loss: 10.870575]\n",
            "********* 4557 [D loss: 0.131472, acc:, 95.31] [G loss: 4.667829]\n",
            "********* 4558 [D loss: 0.419253, acc:, 85.16] [G loss: 11.913025]\n",
            "********* 4559 [D loss: 0.045085, acc:, 99.22] [G loss: 29.181133]\n",
            "********* 4560 [D loss: 0.843524, acc:, 82.81] [G loss: 10.748299]\n",
            "********* 4561 [D loss: 0.215056, acc:, 92.97] [G loss: 5.127127]\n",
            "********* 4562 [D loss: 0.300736, acc:, 89.06] [G loss: 10.509062]\n",
            "********* 4563 [D loss: 0.000940, acc:, 100.00] [G loss: 22.014874]\n",
            "********* 4564 [D loss: 0.175391, acc:, 96.09] [G loss: 30.458971]\n",
            "********* 4565 [D loss: 0.347837, acc:, 92.97] [G loss: 21.838295]\n",
            "********* 4566 [D loss: 0.273892, acc:, 95.31] [G loss: 10.830342]\n",
            "********* 4567 [D loss: 0.058652, acc:, 97.66] [G loss: 6.635595]\n",
            "********* 4568 [D loss: 0.276493, acc:, 87.50] [G loss: 9.967402]\n",
            "********* 4569 [D loss: 0.012992, acc:, 100.00] [G loss: 17.042780]\n",
            "********* 4570 [D loss: 0.116092, acc:, 96.09] [G loss: 22.931244]\n",
            "********* 4571 [D loss: 0.071199, acc:, 96.88] [G loss: 23.295780]\n",
            "********* 4572 [D loss: 0.331762, acc:, 92.97] [G loss: 17.263336]\n",
            "********* 4573 [D loss: 0.059335, acc:, 98.44] [G loss: 10.731424]\n",
            "********* 4574 [D loss: 0.086937, acc:, 96.88] [G loss: 9.006824]\n",
            "********* 4575 [D loss: 0.110247, acc:, 96.09] [G loss: 10.278336]\n",
            "********* 4576 [D loss: 0.137969, acc:, 97.66] [G loss: 13.061945]\n",
            "********* 4577 [D loss: 0.005999, acc:, 100.00] [G loss: 14.954594]\n",
            "********* 4578 [D loss: 0.122683, acc:, 96.88] [G loss: 13.571393]\n",
            "********* 4579 [D loss: 0.072332, acc:, 98.44] [G loss: 12.729923]\n",
            "********* 4580 [D loss: 0.095617, acc:, 97.66] [G loss: 9.574385]\n",
            "********* 4581 [D loss: 0.137916, acc:, 95.31] [G loss: 9.450333]\n",
            "********* 4582 [D loss: 0.027242, acc:, 99.22] [G loss: 11.415084]\n",
            "********* 4583 [D loss: 0.050556, acc:, 97.66] [G loss: 12.750229]\n",
            "********* 4584 [D loss: 0.020557, acc:, 99.22] [G loss: 9.599394]\n",
            "********* 4585 [D loss: 0.054280, acc:, 99.22] [G loss: 10.505096]\n",
            "********* 4586 [D loss: 0.039662, acc:, 99.22] [G loss: 12.980294]\n",
            "********* 4587 [D loss: 0.062579, acc:, 99.22] [G loss: 9.495517]\n",
            "********* 4588 [D loss: 0.062321, acc:, 99.22] [G loss: 8.692066]\n",
            "********* 4589 [D loss: 0.012025, acc:, 100.00] [G loss: 8.110548]\n",
            "********* 4590 [D loss: 0.028245, acc:, 98.44] [G loss: 9.239704]\n",
            "********* 4591 [D loss: 0.043812, acc:, 97.66] [G loss: 8.615206]\n",
            "********* 4592 [D loss: 0.021757, acc:, 99.22] [G loss: 10.119493]\n",
            "********* 4593 [D loss: 0.039010, acc:, 97.66] [G loss: 9.671171]\n",
            "********* 4594 [D loss: 0.057116, acc:, 97.66] [G loss: 9.067289]\n",
            "********* 4595 [D loss: 0.074460, acc:, 99.22] [G loss: 8.983047]\n",
            "********* 4596 [D loss: 0.018687, acc:, 99.22] [G loss: 7.742489]\n",
            "********* 4597 [D loss: 0.010682, acc:, 100.00] [G loss: 8.473637]\n",
            "********* 4598 [D loss: 0.089025, acc:, 96.88] [G loss: 6.613743]\n",
            "********* 4599 [D loss: 0.040598, acc:, 99.22] [G loss: 8.451561]\n",
            "********* 4600 [D loss: 0.036597, acc:, 99.22] [G loss: 10.448715]\n",
            "********* 4601 [D loss: 0.041034, acc:, 97.66] [G loss: 9.739786]\n",
            "********* 4602 [D loss: 0.052294, acc:, 98.44] [G loss: 8.262287]\n",
            "********* 4603 [D loss: 0.089664, acc:, 96.88] [G loss: 7.102757]\n",
            "********* 4604 [D loss: 0.069109, acc:, 97.66] [G loss: 7.722918]\n",
            "********* 4605 [D loss: 0.021838, acc:, 99.22] [G loss: 9.253555]\n",
            "********* 4606 [D loss: 0.079425, acc:, 98.44] [G loss: 8.517423]\n",
            "********* 4607 [D loss: 0.037123, acc:, 98.44] [G loss: 6.663166]\n",
            "********* 4608 [D loss: 0.082368, acc:, 97.66] [G loss: 6.811711]\n",
            "********* 4609 [D loss: 0.021523, acc:, 98.44] [G loss: 8.834095]\n",
            "********* 4610 [D loss: 0.121352, acc:, 96.88] [G loss: 7.246863]\n",
            "********* 4611 [D loss: 0.036034, acc:, 97.66] [G loss: 6.407549]\n",
            "********* 4612 [D loss: 0.023274, acc:, 99.22] [G loss: 6.094654]\n",
            "********* 4613 [D loss: 0.024656, acc:, 99.22] [G loss: 7.522287]\n",
            "********* 4614 [D loss: 0.022235, acc:, 99.22] [G loss: 9.951796]\n",
            "********* 4615 [D loss: 0.056002, acc:, 97.66] [G loss: 7.470521]\n",
            "********* 4616 [D loss: 0.052071, acc:, 97.66] [G loss: 5.665012]\n",
            "********* 4617 [D loss: 0.048255, acc:, 98.44] [G loss: 7.729775]\n",
            "********* 4618 [D loss: 0.014371, acc:, 99.22] [G loss: 8.953827]\n",
            "********* 4619 [D loss: 0.045214, acc:, 98.44] [G loss: 8.785599]\n",
            "********* 4620 [D loss: 0.029437, acc:, 99.22] [G loss: 7.623419]\n",
            "********* 4621 [D loss: 0.028072, acc:, 99.22] [G loss: 7.974793]\n",
            "********* 4622 [D loss: 0.021143, acc:, 99.22] [G loss: 8.092623]\n",
            "********* 4623 [D loss: 0.016688, acc:, 99.22] [G loss: 8.492444]\n",
            "********* 4624 [D loss: 0.082482, acc:, 98.44] [G loss: 8.646151]\n",
            "********* 4625 [D loss: 0.027126, acc:, 99.22] [G loss: 7.540188]\n",
            "********* 4626 [D loss: 0.018045, acc:, 100.00] [G loss: 6.786510]\n",
            "********* 4627 [D loss: 0.019788, acc:, 100.00] [G loss: 7.278731]\n",
            "********* 4628 [D loss: 0.042491, acc:, 98.44] [G loss: 8.020329]\n",
            "********* 4629 [D loss: 0.014710, acc:, 100.00] [G loss: 8.736247]\n",
            "********* 4630 [D loss: 0.130743, acc:, 95.31] [G loss: 6.414543]\n",
            "********* 4631 [D loss: 0.043765, acc:, 98.44] [G loss: 6.205851]\n",
            "********* 4632 [D loss: 0.120455, acc:, 96.09] [G loss: 6.886152]\n",
            "********* 4633 [D loss: 0.006412, acc:, 100.00] [G loss: 8.941322]\n",
            "********* 4634 [D loss: 0.039985, acc:, 98.44] [G loss: 9.889336]\n",
            "********* 4635 [D loss: 0.126268, acc:, 95.31] [G loss: 6.678600]\n",
            "********* 4636 [D loss: 0.098538, acc:, 96.88] [G loss: 5.455564]\n",
            "********* 4637 [D loss: 0.057881, acc:, 97.66] [G loss: 8.398484]\n",
            "********* 4638 [D loss: 0.003497, acc:, 100.00] [G loss: 12.104565]\n",
            "********* 4639 [D loss: 0.041973, acc:, 98.44] [G loss: 13.150769]\n",
            "********* 4640 [D loss: 0.097742, acc:, 98.44] [G loss: 12.575062]\n",
            "********* 4641 [D loss: 0.056458, acc:, 97.66] [G loss: 7.619146]\n",
            "********* 4642 [D loss: 0.022363, acc:, 99.22] [G loss: 4.455659]\n",
            "********* 4643 [D loss: 0.054637, acc:, 99.22] [G loss: 5.390750]\n",
            "********* 4644 [D loss: 0.011942, acc:, 100.00] [G loss: 8.473085]\n",
            "********* 4645 [D loss: 0.072087, acc:, 98.44] [G loss: 10.608436]\n",
            "********* 4646 [D loss: 0.017342, acc:, 98.44] [G loss: 12.352249]\n",
            "********* 4647 [D loss: 0.023915, acc:, 98.44] [G loss: 11.036175]\n",
            "********* 4648 [D loss: 0.071831, acc:, 98.44] [G loss: 9.381928]\n",
            "********* 4649 [D loss: 0.032068, acc:, 98.44] [G loss: 8.110321]\n",
            "********* 4650 [D loss: 0.026838, acc:, 99.22] [G loss: 8.658785]\n",
            "********* 4651 [D loss: 0.019345, acc:, 99.22] [G loss: 9.488115]\n",
            "********* 4652 [D loss: 0.004831, acc:, 100.00] [G loss: 11.624344]\n",
            "********* 4653 [D loss: 0.015379, acc:, 99.22] [G loss: 11.958668]\n",
            "********* 4654 [D loss: 0.009383, acc:, 100.00] [G loss: 10.717502]\n",
            "********* 4655 [D loss: 0.096215, acc:, 97.66] [G loss: 7.885504]\n",
            "********* 4656 [D loss: 0.033579, acc:, 98.44] [G loss: 6.845580]\n",
            "********* 4657 [D loss: 0.015268, acc:, 100.00] [G loss: 7.347127]\n",
            "********* 4658 [D loss: 0.009116, acc:, 100.00] [G loss: 8.092700]\n",
            "********* 4659 [D loss: 0.022206, acc:, 99.22] [G loss: 10.863327]\n",
            "********* 4660 [D loss: 0.011092, acc:, 100.00] [G loss: 11.230863]\n",
            "********* 4661 [D loss: 0.037914, acc:, 97.66] [G loss: 9.603984]\n",
            "********* 4662 [D loss: 0.025484, acc:, 99.22] [G loss: 5.690747]\n",
            "********* 4663 [D loss: 0.124471, acc:, 94.53] [G loss: 5.367572]\n",
            "********* 4664 [D loss: 0.065228, acc:, 99.22] [G loss: 8.253842]\n",
            "********* 4665 [D loss: 0.016387, acc:, 99.22] [G loss: 11.653503]\n",
            "********* 4666 [D loss: 0.008141, acc:, 100.00] [G loss: 13.295563]\n",
            "********* 4667 [D loss: 0.052342, acc:, 99.22] [G loss: 11.844937]\n",
            "********* 4668 [D loss: 0.189775, acc:, 94.53] [G loss: 4.799964]\n",
            "********* 4669 [D loss: 0.400094, acc:, 82.03] [G loss: 8.608837]\n",
            "********* 4670 [D loss: 0.000591, acc:, 100.00] [G loss: 20.765768]\n",
            "********* 4671 [D loss: 0.633755, acc:, 88.28] [G loss: 12.990702]\n",
            "********* 4672 [D loss: 0.014601, acc:, 100.00] [G loss: 7.723654]\n",
            "********* 4673 [D loss: 0.039193, acc:, 99.22] [G loss: 5.770145]\n",
            "********* 4674 [D loss: 0.031177, acc:, 99.22] [G loss: 5.067990]\n",
            "********* 4675 [D loss: 0.017626, acc:, 99.22] [G loss: 7.492018]\n",
            "********* 4676 [D loss: 0.011502, acc:, 100.00] [G loss: 9.036708]\n",
            "********* 4677 [D loss: 0.009603, acc:, 99.22] [G loss: 11.239271]\n",
            "********* 4678 [D loss: 0.038293, acc:, 98.44] [G loss: 9.857118]\n",
            "********* 4679 [D loss: 0.025927, acc:, 98.44] [G loss: 9.639439]\n",
            "********* 4680 [D loss: 0.012084, acc:, 100.00] [G loss: 8.947709]\n",
            "********* 4681 [D loss: 0.006456, acc:, 100.00] [G loss: 10.175017]\n",
            "********* 4682 [D loss: 0.029675, acc:, 98.44] [G loss: 10.598413]\n",
            "********* 4683 [D loss: 0.017978, acc:, 99.22] [G loss: 10.635487]\n",
            "********* 4684 [D loss: 0.010195, acc:, 100.00] [G loss: 10.805935]\n",
            "********* 4685 [D loss: 0.107764, acc:, 96.88] [G loss: 8.033756]\n",
            "********* 4686 [D loss: 0.037602, acc:, 98.44] [G loss: 7.838384]\n",
            "********* 4687 [D loss: 0.029105, acc:, 98.44] [G loss: 9.314692]\n",
            "********* 4688 [D loss: 0.032077, acc:, 98.44] [G loss: 11.123621]\n",
            "********* 4689 [D loss: 0.034774, acc:, 97.66] [G loss: 10.535604]\n",
            "********* 4690 [D loss: 0.060603, acc:, 97.66] [G loss: 7.233178]\n",
            "********* 4691 [D loss: 0.052131, acc:, 97.66] [G loss: 6.857384]\n",
            "********* 4692 [D loss: 0.019511, acc:, 100.00] [G loss: 8.089479]\n",
            "********* 4693 [D loss: 0.008294, acc:, 100.00] [G loss: 9.830821]\n",
            "********* 4694 [D loss: 0.076293, acc:, 98.44] [G loss: 9.945936]\n",
            "********* 4695 [D loss: 0.102385, acc:, 96.88] [G loss: 5.945386]\n",
            "********* 4696 [D loss: 0.075603, acc:, 97.66] [G loss: 5.426285]\n",
            "********* 4697 [D loss: 0.050179, acc:, 99.22] [G loss: 7.885157]\n",
            "********* 4698 [D loss: 0.087116, acc:, 97.66] [G loss: 10.676571]\n",
            "********* 4699 [D loss: 0.052059, acc:, 99.22] [G loss: 9.537696]\n",
            "********* 4700 [D loss: 0.039201, acc:, 98.44] [G loss: 8.256755]\n",
            "********* 4701 [D loss: 0.014471, acc:, 100.00] [G loss: 7.735708]\n",
            "********* 4702 [D loss: 0.021983, acc:, 100.00] [G loss: 8.004254]\n",
            "********* 4703 [D loss: 0.005002, acc:, 100.00] [G loss: 9.870594]\n",
            "********* 4704 [D loss: 0.014225, acc:, 99.22] [G loss: 10.016223]\n",
            "********* 4705 [D loss: 0.028840, acc:, 99.22] [G loss: 9.079933]\n",
            "********* 4706 [D loss: 0.032400, acc:, 98.44] [G loss: 8.120833]\n",
            "********* 4707 [D loss: 0.056207, acc:, 99.22] [G loss: 8.174546]\n",
            "********* 4708 [D loss: 0.009330, acc:, 99.22] [G loss: 9.209696]\n",
            "********* 4709 [D loss: 0.015051, acc:, 99.22] [G loss: 9.848323]\n",
            "********* 4710 [D loss: 0.110509, acc:, 95.31] [G loss: 7.618715]\n",
            "********* 4711 [D loss: 0.051070, acc:, 97.66] [G loss: 7.136163]\n",
            "********* 4712 [D loss: 0.039188, acc:, 98.44] [G loss: 7.226851]\n",
            "********* 4713 [D loss: 0.006347, acc:, 100.00] [G loss: 10.209807]\n",
            "********* 4714 [D loss: 0.010555, acc:, 100.00] [G loss: 11.801306]\n",
            "********* 4715 [D loss: 0.116298, acc:, 98.44] [G loss: 10.457112]\n",
            "********* 4716 [D loss: 0.055584, acc:, 97.66] [G loss: 8.412142]\n",
            "********* 4717 [D loss: 0.061558, acc:, 97.66] [G loss: 5.338169]\n",
            "********* 4718 [D loss: 0.026739, acc:, 100.00] [G loss: 5.784076]\n",
            "********* 4719 [D loss: 0.013612, acc:, 100.00] [G loss: 7.391488]\n",
            "********* 4720 [D loss: 0.005797, acc:, 100.00] [G loss: 9.360863]\n",
            "********* 4721 [D loss: 0.042795, acc:, 98.44] [G loss: 9.787039]\n",
            "********* 4722 [D loss: 0.029922, acc:, 99.22] [G loss: 8.835588]\n",
            "********* 4723 [D loss: 0.022837, acc:, 99.22] [G loss: 8.149627]\n",
            "********* 4724 [D loss: 0.022927, acc:, 99.22] [G loss: 7.163276]\n",
            "********* 4725 [D loss: 0.014975, acc:, 100.00] [G loss: 6.367748]\n",
            "********* 4726 [D loss: 0.030570, acc:, 100.00] [G loss: 6.346257]\n",
            "********* 4727 [D loss: 0.036796, acc:, 99.22] [G loss: 8.664340]\n",
            "********* 4728 [D loss: 0.046082, acc:, 97.66] [G loss: 8.028208]\n",
            "********* 4729 [D loss: 0.020243, acc:, 100.00] [G loss: 6.943569]\n",
            "********* 4730 [D loss: 0.020528, acc:, 100.00] [G loss: 7.644415]\n",
            "********* 4731 [D loss: 0.019707, acc:, 100.00] [G loss: 8.879163]\n",
            "********* 4732 [D loss: 0.011772, acc:, 100.00] [G loss: 8.543729]\n",
            "********* 4733 [D loss: 0.060813, acc:, 98.44] [G loss: 7.808694]\n",
            "********* 4734 [D loss: 0.093399, acc:, 95.31] [G loss: 7.304678]\n",
            "********* 4735 [D loss: 0.048073, acc:, 97.66] [G loss: 7.350722]\n",
            "********* 4736 [D loss: 0.022967, acc:, 100.00] [G loss: 7.776608]\n",
            "********* 4737 [D loss: 0.050073, acc:, 98.44] [G loss: 8.564331]\n",
            "********* 4738 [D loss: 0.027686, acc:, 99.22] [G loss: 9.860420]\n",
            "********* 4739 [D loss: 0.035990, acc:, 98.44] [G loss: 10.124039]\n",
            "********* 4740 [D loss: 0.009835, acc:, 100.00] [G loss: 8.484943]\n",
            "********* 4741 [D loss: 0.052509, acc:, 97.66] [G loss: 8.598335]\n",
            "********* 4742 [D loss: 0.054509, acc:, 96.88] [G loss: 7.938653]\n",
            "********* 4743 [D loss: 0.037417, acc:, 99.22] [G loss: 8.410460]\n",
            "********* 4744 [D loss: 0.065094, acc:, 95.31] [G loss: 8.754109]\n",
            "********* 4745 [D loss: 0.005504, acc:, 100.00] [G loss: 10.474470]\n",
            "********* 4746 [D loss: 0.011421, acc:, 100.00] [G loss: 9.249205]\n",
            "********* 4747 [D loss: 0.058955, acc:, 98.44] [G loss: 10.076268]\n",
            "********* 4748 [D loss: 0.007561, acc:, 100.00] [G loss: 10.819345]\n",
            "********* 4749 [D loss: 0.128293, acc:, 96.88] [G loss: 8.468400]\n",
            "********* 4750 [D loss: 0.036225, acc:, 98.44] [G loss: 6.254579]\n",
            "********* 4751 [D loss: 0.132845, acc:, 96.09] [G loss: 5.484605]\n",
            "********* 4752 [D loss: 0.071244, acc:, 96.88] [G loss: 7.212825]\n",
            "********* 4753 [D loss: 0.015657, acc:, 100.00] [G loss: 10.962780]\n",
            "********* 4754 [D loss: 0.021813, acc:, 99.22] [G loss: 12.713985]\n",
            "********* 4755 [D loss: 0.046458, acc:, 97.66] [G loss: 8.989109]\n",
            "********* 4756 [D loss: 0.045030, acc:, 96.88] [G loss: 9.187982]\n",
            "********* 4757 [D loss: 0.009465, acc:, 100.00] [G loss: 9.550085]\n",
            "********* 4758 [D loss: 0.013278, acc:, 100.00] [G loss: 9.645842]\n",
            "********* 4759 [D loss: 0.021757, acc:, 99.22] [G loss: 10.567188]\n",
            "********* 4760 [D loss: 0.036641, acc:, 99.22] [G loss: 10.568098]\n",
            "********* 4761 [D loss: 0.021388, acc:, 99.22] [G loss: 9.346102]\n",
            "********* 4762 [D loss: 0.008034, acc:, 100.00] [G loss: 8.741295]\n",
            "********* 4763 [D loss: 0.083986, acc:, 96.88] [G loss: 6.309099]\n",
            "********* 4764 [D loss: 0.064581, acc:, 97.66] [G loss: 7.557898]\n",
            "********* 4765 [D loss: 0.024396, acc:, 98.44] [G loss: 11.669162]\n",
            "********* 4766 [D loss: 0.103106, acc:, 96.88] [G loss: 12.844855]\n",
            "********* 4767 [D loss: 0.042524, acc:, 98.44] [G loss: 9.842983]\n",
            "********* 4768 [D loss: 0.018254, acc:, 99.22] [G loss: 7.628023]\n",
            "********* 4769 [D loss: 0.061052, acc:, 99.22] [G loss: 6.759577]\n",
            "********* 4770 [D loss: 0.064675, acc:, 98.44] [G loss: 10.835979]\n",
            "********* 4771 [D loss: 0.042797, acc:, 98.44] [G loss: 11.874228]\n",
            "********* 4772 [D loss: 0.086081, acc:, 97.66] [G loss: 8.213242]\n",
            "********* 4773 [D loss: 0.019761, acc:, 99.22] [G loss: 7.689393]\n",
            "********* 4774 [D loss: 0.058764, acc:, 99.22] [G loss: 6.443914]\n",
            "********* 4775 [D loss: 0.024940, acc:, 99.22] [G loss: 6.658237]\n",
            "********* 4776 [D loss: 0.005726, acc:, 100.00] [G loss: 9.570578]\n",
            "********* 4777 [D loss: 0.046446, acc:, 96.88] [G loss: 9.097881]\n",
            "********* 4778 [D loss: 0.052768, acc:, 96.09] [G loss: 7.942009]\n",
            "********* 4779 [D loss: 0.017175, acc:, 100.00] [G loss: 9.349747]\n",
            "********* 4780 [D loss: 0.021888, acc:, 99.22] [G loss: 8.377548]\n",
            "********* 4781 [D loss: 0.041961, acc:, 97.66] [G loss: 7.068048]\n",
            "********* 4782 [D loss: 0.046275, acc:, 98.44] [G loss: 7.813132]\n",
            "********* 4783 [D loss: 0.002751, acc:, 100.00] [G loss: 9.051062]\n",
            "********* 4784 [D loss: 0.035648, acc:, 98.44] [G loss: 12.908938]\n",
            "********* 4785 [D loss: 0.015935, acc:, 100.00] [G loss: 11.317613]\n",
            "********* 4786 [D loss: 0.029153, acc:, 98.44] [G loss: 8.887396]\n",
            "********* 4787 [D loss: 0.010752, acc:, 100.00] [G loss: 6.575378]\n",
            "********* 4788 [D loss: 0.083661, acc:, 95.31] [G loss: 8.151872]\n",
            "********* 4789 [D loss: 0.012752, acc:, 99.22] [G loss: 13.884751]\n",
            "********* 4790 [D loss: 0.062467, acc:, 97.66] [G loss: 12.035194]\n",
            "********* 4791 [D loss: 0.042835, acc:, 97.66] [G loss: 7.948140]\n",
            "********* 4792 [D loss: 0.091562, acc:, 96.09] [G loss: 8.462872]\n",
            "********* 4793 [D loss: 0.003879, acc:, 100.00] [G loss: 13.036865]\n",
            "********* 4794 [D loss: 0.019012, acc:, 99.22] [G loss: 16.652323]\n",
            "********* 4795 [D loss: 0.205779, acc:, 92.97] [G loss: 9.585604]\n",
            "********* 4796 [D loss: 0.031116, acc:, 98.44] [G loss: 8.443298]\n",
            "********* 4797 [D loss: 0.050009, acc:, 99.22] [G loss: 7.821886]\n",
            "********* 4798 [D loss: 0.069301, acc:, 98.44] [G loss: 9.651797]\n",
            "********* 4799 [D loss: 0.005915, acc:, 100.00] [G loss: 10.759972]\n",
            "********* 4800 [D loss: 0.008461, acc:, 100.00] [G loss: 11.385935]\n",
            "********* 4801 [D loss: 0.112416, acc:, 99.22] [G loss: 10.803717]\n",
            "********* 4802 [D loss: 0.004549, acc:, 100.00] [G loss: 9.209131]\n",
            "********* 4803 [D loss: 0.065385, acc:, 97.66] [G loss: 8.366673]\n",
            "********* 4804 [D loss: 0.059166, acc:, 96.88] [G loss: 9.289101]\n",
            "********* 4805 [D loss: 0.007501, acc:, 100.00] [G loss: 12.486275]\n",
            "********* 4806 [D loss: 0.050496, acc:, 99.22] [G loss: 11.739649]\n",
            "********* 4807 [D loss: 0.023759, acc:, 98.44] [G loss: 9.876606]\n",
            "********* 4808 [D loss: 0.035680, acc:, 99.22] [G loss: 8.346192]\n",
            "********* 4809 [D loss: 0.101971, acc:, 96.09] [G loss: 9.892427]\n",
            "********* 4810 [D loss: 0.040871, acc:, 99.22] [G loss: 11.649585]\n",
            "********* 4811 [D loss: 0.055967, acc:, 97.66] [G loss: 8.412401]\n",
            "********* 4812 [D loss: 0.087857, acc:, 97.66] [G loss: 8.499781]\n",
            "********* 4813 [D loss: 0.011441, acc:, 100.00] [G loss: 10.841301]\n",
            "********* 4814 [D loss: 0.038643, acc:, 98.44] [G loss: 11.111267]\n",
            "********* 4815 [D loss: 0.025729, acc:, 99.22] [G loss: 11.993294]\n",
            "********* 4816 [D loss: 0.114455, acc:, 96.88] [G loss: 7.470501]\n",
            "********* 4817 [D loss: 0.155962, acc:, 95.31] [G loss: 9.068884]\n",
            "********* 4818 [D loss: 0.004354, acc:, 100.00] [G loss: 13.021629]\n",
            "********* 4819 [D loss: 0.040968, acc:, 99.22] [G loss: 17.265419]\n",
            "********* 4820 [D loss: 0.142367, acc:, 94.53] [G loss: 10.824848]\n",
            "********* 4821 [D loss: 0.031459, acc:, 99.22] [G loss: 6.392176]\n",
            "********* 4822 [D loss: 0.068576, acc:, 97.66] [G loss: 8.174562]\n",
            "********* 4823 [D loss: 0.032429, acc:, 99.22] [G loss: 12.492161]\n",
            "********* 4824 [D loss: 0.006220, acc:, 100.00] [G loss: 15.855431]\n",
            "********* 4825 [D loss: 0.007573, acc:, 100.00] [G loss: 17.875835]\n",
            "********* 4826 [D loss: 0.339434, acc:, 93.75] [G loss: 11.237921]\n",
            "********* 4827 [D loss: 0.076744, acc:, 94.53] [G loss: 8.832066]\n",
            "********* 4828 [D loss: 0.046558, acc:, 97.66] [G loss: 9.927297]\n",
            "********* 4829 [D loss: 0.033389, acc:, 99.22] [G loss: 10.828183]\n",
            "********* 4830 [D loss: 0.001888, acc:, 100.00] [G loss: 12.495005]\n",
            "********* 4831 [D loss: 0.017450, acc:, 99.22] [G loss: 11.209052]\n",
            "********* 4832 [D loss: 0.011113, acc:, 99.22] [G loss: 9.861487]\n",
            "********* 4833 [D loss: 0.043227, acc:, 97.66] [G loss: 10.783630]\n",
            "********* 4834 [D loss: 0.008226, acc:, 100.00] [G loss: 13.792715]\n",
            "********* 4835 [D loss: 0.048599, acc:, 97.66] [G loss: 12.478611]\n",
            "********* 4836 [D loss: 0.142916, acc:, 96.09] [G loss: 9.515449]\n",
            "********* 4837 [D loss: 0.065054, acc:, 97.66] [G loss: 9.368667]\n",
            "********* 4838 [D loss: 0.108638, acc:, 96.88] [G loss: 8.493761]\n",
            "********* 4839 [D loss: 0.020596, acc:, 98.44] [G loss: 8.081167]\n",
            "********* 4840 [D loss: 0.046679, acc:, 98.44] [G loss: 10.811047]\n",
            "********* 4841 [D loss: 0.110721, acc:, 96.09] [G loss: 9.066320]\n",
            "********* 4842 [D loss: 0.023516, acc:, 100.00] [G loss: 8.457007]\n",
            "********* 4843 [D loss: 0.123587, acc:, 95.31] [G loss: 10.403488]\n",
            "********* 4844 [D loss: 0.042128, acc:, 98.44] [G loss: 13.793028]\n",
            "********* 4845 [D loss: 0.140814, acc:, 96.09] [G loss: 9.816505]\n",
            "********* 4846 [D loss: 0.076494, acc:, 96.88] [G loss: 6.444331]\n",
            "********* 4847 [D loss: 0.085832, acc:, 96.09] [G loss: 6.473529]\n",
            "********* 4848 [D loss: 0.012168, acc:, 100.00] [G loss: 9.856328]\n",
            "********* 4849 [D loss: 0.050622, acc:, 97.66] [G loss: 11.385092]\n",
            "********* 4850 [D loss: 0.035532, acc:, 98.44] [G loss: 10.437572]\n",
            "********* 4851 [D loss: 0.038667, acc:, 97.66] [G loss: 10.307017]\n",
            "********* 4852 [D loss: 0.049080, acc:, 98.44] [G loss: 11.467344]\n",
            "********* 4853 [D loss: 0.096676, acc:, 96.88] [G loss: 10.748429]\n",
            "********* 4854 [D loss: 0.075766, acc:, 97.66] [G loss: 11.878542]\n",
            "********* 4855 [D loss: 0.055120, acc:, 98.44] [G loss: 12.335739]\n",
            "********* 4856 [D loss: 0.215712, acc:, 92.97] [G loss: 8.705875]\n",
            "********* 4857 [D loss: 0.015819, acc:, 100.00] [G loss: 9.309803]\n",
            "********* 4858 [D loss: 0.067559, acc:, 96.88] [G loss: 7.934347]\n",
            "********* 4859 [D loss: 0.082666, acc:, 96.09] [G loss: 9.933244]\n",
            "********* 4860 [D loss: 0.025416, acc:, 99.22] [G loss: 12.935421]\n",
            "********* 4861 [D loss: 0.042403, acc:, 98.44] [G loss: 12.567001]\n",
            "********* 4862 [D loss: 0.100400, acc:, 96.09] [G loss: 7.848195]\n",
            "********* 4863 [D loss: 0.052833, acc:, 98.44] [G loss: 7.055593]\n",
            "********* 4864 [D loss: 0.072714, acc:, 96.88] [G loss: 9.731111]\n",
            "********* 4865 [D loss: 0.110034, acc:, 96.88] [G loss: 9.998911]\n",
            "********* 4866 [D loss: 0.055108, acc:, 97.66] [G loss: 8.636776]\n",
            "********* 4867 [D loss: 0.043661, acc:, 98.44] [G loss: 8.031367]\n",
            "********* 4868 [D loss: 0.069674, acc:, 98.44] [G loss: 10.449566]\n",
            "********* 4869 [D loss: 0.049710, acc:, 97.66] [G loss: 11.600505]\n",
            "********* 4870 [D loss: 0.126821, acc:, 96.09] [G loss: 9.896941]\n",
            "********* 4871 [D loss: 0.011874, acc:, 100.00] [G loss: 8.396727]\n",
            "********* 4872 [D loss: 0.122738, acc:, 95.31] [G loss: 8.085304]\n",
            "********* 4873 [D loss: 0.006501, acc:, 100.00] [G loss: 13.569082]\n",
            "********* 4874 [D loss: 0.151187, acc:, 93.75] [G loss: 10.067031]\n",
            "********* 4875 [D loss: 0.133835, acc:, 95.31] [G loss: 7.254942]\n",
            "********* 4876 [D loss: 0.090504, acc:, 96.88] [G loss: 9.521743]\n",
            "********* 4877 [D loss: 0.007316, acc:, 100.00] [G loss: 11.682644]\n",
            "********* 4878 [D loss: 0.063442, acc:, 98.44] [G loss: 12.836407]\n",
            "********* 4879 [D loss: 0.054135, acc:, 97.66] [G loss: 8.884472]\n",
            "********* 4880 [D loss: 0.012126, acc:, 100.00] [G loss: 7.000181]\n",
            "********* 4881 [D loss: 0.081041, acc:, 97.66] [G loss: 8.181105]\n",
            "********* 4882 [D loss: 0.030587, acc:, 99.22] [G loss: 13.498266]\n",
            "********* 4883 [D loss: 0.212177, acc:, 94.53] [G loss: 8.589602]\n",
            "********* 4884 [D loss: 0.017599, acc:, 100.00] [G loss: 7.096405]\n",
            "********* 4885 [D loss: 0.100002, acc:, 95.31] [G loss: 8.811552]\n",
            "********* 4886 [D loss: 0.031038, acc:, 99.22] [G loss: 14.851873]\n",
            "********* 4887 [D loss: 0.071155, acc:, 97.66] [G loss: 14.826754]\n",
            "********* 4888 [D loss: 0.086644, acc:, 96.88] [G loss: 10.914226]\n",
            "********* 4889 [D loss: 0.010514, acc:, 100.00] [G loss: 7.901439]\n",
            "********* 4890 [D loss: 0.011785, acc:, 100.00] [G loss: 6.460817]\n",
            "********* 4891 [D loss: 0.035934, acc:, 99.22] [G loss: 6.449381]\n",
            "********* 4892 [D loss: 0.024674, acc:, 98.44] [G loss: 8.459436]\n",
            "********* 4893 [D loss: 0.012665, acc:, 100.00] [G loss: 11.284786]\n",
            "********* 4894 [D loss: 0.057872, acc:, 97.66] [G loss: 10.142461]\n",
            "********* 4895 [D loss: 0.064532, acc:, 98.44] [G loss: 9.857064]\n",
            "********* 4896 [D loss: 0.067594, acc:, 97.66] [G loss: 9.483716]\n",
            "********* 4897 [D loss: 0.063386, acc:, 97.66] [G loss: 8.545300]\n",
            "********* 4898 [D loss: 0.072497, acc:, 98.44] [G loss: 6.588945]\n",
            "********* 4899 [D loss: 0.070110, acc:, 96.09] [G loss: 8.628529]\n",
            "********* 4900 [D loss: 0.008760, acc:, 100.00] [G loss: 11.651749]\n",
            "********* 4901 [D loss: 0.098608, acc:, 94.53] [G loss: 7.040906]\n",
            "********* 4902 [D loss: 0.071714, acc:, 96.09] [G loss: 7.302812]\n",
            "********* 4903 [D loss: 0.021892, acc:, 99.22] [G loss: 10.493664]\n",
            "********* 4904 [D loss: 0.003324, acc:, 100.00] [G loss: 12.946281]\n",
            "********* 4905 [D loss: 0.050050, acc:, 98.44] [G loss: 14.290506]\n",
            "********* 4906 [D loss: 0.094799, acc:, 96.88] [G loss: 8.879000]\n",
            "********* 4907 [D loss: 0.039872, acc:, 97.66] [G loss: 6.840380]\n",
            "********* 4908 [D loss: 0.067258, acc:, 96.09] [G loss: 7.711927]\n",
            "********* 4909 [D loss: 0.032491, acc:, 99.22] [G loss: 10.273048]\n",
            "********* 4910 [D loss: 0.055089, acc:, 96.88] [G loss: 12.494325]\n",
            "********* 4911 [D loss: 0.083713, acc:, 97.66] [G loss: 10.668549]\n",
            "********* 4912 [D loss: 0.030047, acc:, 98.44] [G loss: 7.840784]\n",
            "********* 4913 [D loss: 0.032895, acc:, 98.44] [G loss: 10.814936]\n",
            "********* 4914 [D loss: 0.044981, acc:, 99.22] [G loss: 10.514112]\n",
            "********* 4915 [D loss: 0.040898, acc:, 98.44] [G loss: 11.964289]\n",
            "********* 4916 [D loss: 0.011558, acc:, 99.22] [G loss: 15.050465]\n",
            "********* 4917 [D loss: 0.034877, acc:, 99.22] [G loss: 12.716359]\n",
            "********* 4918 [D loss: 0.051821, acc:, 96.09] [G loss: 11.074518]\n",
            "********* 4919 [D loss: 0.029380, acc:, 98.44] [G loss: 10.161528]\n",
            "********* 4920 [D loss: 0.021786, acc:, 99.22] [G loss: 10.955571]\n",
            "********* 4921 [D loss: 0.025417, acc:, 99.22] [G loss: 13.261044]\n",
            "********* 4922 [D loss: 0.021618, acc:, 98.44] [G loss: 11.842564]\n",
            "********* 4923 [D loss: 0.084790, acc:, 96.09] [G loss: 8.300358]\n",
            "********* 4924 [D loss: 0.014915, acc:, 100.00] [G loss: 8.353861]\n",
            "********* 4925 [D loss: 0.050890, acc:, 96.88] [G loss: 9.036184]\n",
            "********* 4926 [D loss: 0.011441, acc:, 99.22] [G loss: 12.559975]\n",
            "********* 4927 [D loss: 0.023618, acc:, 99.22] [G loss: 13.856672]\n",
            "********* 4928 [D loss: 0.097231, acc:, 94.53] [G loss: 7.358511]\n",
            "********* 4929 [D loss: 0.127848, acc:, 92.97] [G loss: 8.520347]\n",
            "********* 4930 [D loss: 0.009687, acc:, 100.00] [G loss: 16.975342]\n",
            "********* 4931 [D loss: 0.396218, acc:, 90.62] [G loss: 8.905443]\n",
            "********* 4932 [D loss: 0.181673, acc:, 92.97] [G loss: 7.099990]\n",
            "********* 4933 [D loss: 0.121216, acc:, 95.31] [G loss: 10.872964]\n",
            "********* 4934 [D loss: 0.066001, acc:, 99.22] [G loss: 17.875160]\n",
            "********* 4935 [D loss: 0.265024, acc:, 93.75] [G loss: 16.958357]\n",
            "********* 4936 [D loss: 0.030158, acc:, 98.44] [G loss: 12.362148]\n",
            "********* 4937 [D loss: 0.063327, acc:, 96.09] [G loss: 10.570536]\n",
            "********* 4938 [D loss: 0.054533, acc:, 99.22] [G loss: 11.543613]\n",
            "********* 4939 [D loss: 0.037325, acc:, 98.44] [G loss: 11.780537]\n",
            "********* 4940 [D loss: 0.002612, acc:, 100.00] [G loss: 12.263498]\n",
            "********* 4941 [D loss: 0.076040, acc:, 96.88] [G loss: 12.125069]\n",
            "********* 4942 [D loss: 0.216808, acc:, 93.75] [G loss: 11.199374]\n",
            "********* 4943 [D loss: 0.091380, acc:, 96.88] [G loss: 10.685925]\n",
            "********* 4944 [D loss: 0.058998, acc:, 98.44] [G loss: 11.751558]\n",
            "********* 4945 [D loss: 0.037532, acc:, 98.44] [G loss: 11.406642]\n",
            "********* 4946 [D loss: 0.016593, acc:, 99.22] [G loss: 12.012245]\n",
            "********* 4947 [D loss: 0.046681, acc:, 98.44] [G loss: 11.227393]\n",
            "********* 4948 [D loss: 0.044042, acc:, 99.22] [G loss: 11.600174]\n",
            "********* 4949 [D loss: 0.087987, acc:, 97.66] [G loss: 14.489089]\n",
            "********* 4950 [D loss: 0.062468, acc:, 98.44] [G loss: 12.855445]\n",
            "********* 4951 [D loss: 0.082351, acc:, 99.22] [G loss: 10.599737]\n",
            "********* 4952 [D loss: 0.029720, acc:, 98.44] [G loss: 9.104094]\n",
            "********* 4953 [D loss: 0.114484, acc:, 93.75] [G loss: 9.843018]\n",
            "********* 4954 [D loss: 0.038874, acc:, 98.44] [G loss: 13.023317]\n",
            "********* 4955 [D loss: 0.115442, acc:, 96.88] [G loss: 11.325339]\n",
            "********* 4956 [D loss: 0.039207, acc:, 98.44] [G loss: 9.790752]\n",
            "********* 4957 [D loss: 0.036668, acc:, 98.44] [G loss: 9.222794]\n",
            "********* 4958 [D loss: 0.009406, acc:, 100.00] [G loss: 11.548985]\n",
            "********* 4959 [D loss: 0.073624, acc:, 97.66] [G loss: 10.641872]\n",
            "********* 4960 [D loss: 0.020770, acc:, 99.22] [G loss: 10.256389]\n",
            "********* 4961 [D loss: 0.033971, acc:, 99.22] [G loss: 9.952867]\n",
            "********* 4962 [D loss: 0.022808, acc:, 98.44] [G loss: 10.625995]\n",
            "********* 4963 [D loss: 0.005470, acc:, 100.00] [G loss: 11.547884]\n",
            "********* 4964 [D loss: 0.008730, acc:, 100.00] [G loss: 11.208353]\n",
            "********* 4965 [D loss: 0.043103, acc:, 98.44] [G loss: 8.877918]\n",
            "********* 4966 [D loss: 0.048857, acc:, 98.44] [G loss: 8.566036]\n",
            "********* 4967 [D loss: 0.101068, acc:, 96.09] [G loss: 7.347431]\n",
            "********* 4968 [D loss: 0.032649, acc:, 99.22] [G loss: 7.763253]\n",
            "********* 4969 [D loss: 0.021415, acc:, 99.22] [G loss: 9.786674]\n",
            "********* 4970 [D loss: 0.124754, acc:, 96.88] [G loss: 9.169792]\n",
            "********* 4971 [D loss: 0.028616, acc:, 99.22] [G loss: 9.041669]\n",
            "********* 4972 [D loss: 0.034098, acc:, 99.22] [G loss: 10.224901]\n",
            "********* 4973 [D loss: 0.039290, acc:, 98.44] [G loss: 11.028708]\n",
            "********* 4974 [D loss: 0.036195, acc:, 99.22] [G loss: 8.300774]\n",
            "********* 4975 [D loss: 0.055960, acc:, 98.44] [G loss: 8.002828]\n",
            "********* 4976 [D loss: 0.012651, acc:, 100.00] [G loss: 8.901474]\n",
            "********* 4977 [D loss: 0.017621, acc:, 100.00] [G loss: 9.510133]\n",
            "********* 4978 [D loss: 0.021115, acc:, 99.22] [G loss: 10.294338]\n",
            "********* 4979 [D loss: 0.037303, acc:, 98.44] [G loss: 9.366213]\n",
            "********* 4980 [D loss: 0.051613, acc:, 96.88] [G loss: 6.575336]\n",
            "********* 4981 [D loss: 0.053548, acc:, 97.66] [G loss: 8.861519]\n",
            "********* 4982 [D loss: 0.034820, acc:, 99.22] [G loss: 11.133440]\n",
            "********* 4983 [D loss: 0.147921, acc:, 95.31] [G loss: 7.534716]\n",
            "********* 4984 [D loss: 0.126667, acc:, 92.97] [G loss: 10.108210]\n",
            "********* 4985 [D loss: 0.039025, acc:, 99.22] [G loss: 16.617338]\n",
            "********* 4986 [D loss: 0.107775, acc:, 96.09] [G loss: 16.587145]\n",
            "********* 4987 [D loss: 0.336875, acc:, 89.84] [G loss: 4.934660]\n",
            "********* 4988 [D loss: 0.797879, acc:, 75.00] [G loss: 10.043623]\n",
            "********* 4989 [D loss: 0.013850, acc:, 99.22] [G loss: 29.244301]\n",
            "********* 4990 [D loss: 1.948749, acc:, 74.22] [G loss: 11.541206]\n",
            "********* 4991 [D loss: 0.100100, acc:, 96.09] [G loss: 5.676925]\n",
            "********* 4992 [D loss: 0.791069, acc:, 82.03] [G loss: 5.853014]\n",
            "********* 4993 [D loss: 0.121699, acc:, 94.53] [G loss: 11.403463]\n",
            "********* 4994 [D loss: 0.166052, acc:, 97.66] [G loss: 18.368172]\n",
            "********* 4995 [D loss: 0.195491, acc:, 96.88] [G loss: 22.269102]\n",
            "********* 4996 [D loss: 0.421290, acc:, 90.62] [G loss: 15.531390]\n",
            "********* 4997 [D loss: 0.028708, acc:, 99.22] [G loss: 12.648191]\n",
            "********* 4998 [D loss: 0.023499, acc:, 99.22] [G loss: 10.403362]\n",
            "********* 4999 [D loss: 0.053180, acc:, 98.44] [G loss: 7.617699]\n",
            "********* 5000 [D loss: 0.045772, acc:, 98.44] [G loss: 6.602795]\n",
            "********* 5001 [D loss: 0.036965, acc:, 99.22] [G loss: 6.072323]\n",
            "********* 5002 [D loss: 0.051101, acc:, 98.44] [G loss: 6.602331]\n",
            "********* 5003 [D loss: 0.009684, acc:, 100.00] [G loss: 8.746457]\n",
            "********* 5004 [D loss: 0.032208, acc:, 99.22] [G loss: 9.736261]\n",
            "********* 5005 [D loss: 0.015413, acc:, 99.22] [G loss: 11.121191]\n",
            "********* 5006 [D loss: 0.018890, acc:, 99.22] [G loss: 12.432933]\n",
            "********* 5007 [D loss: 0.052935, acc:, 98.44] [G loss: 11.125599]\n",
            "********* 5008 [D loss: 0.045503, acc:, 98.44] [G loss: 9.227973]\n",
            "********* 5009 [D loss: 0.094783, acc:, 96.88] [G loss: 7.368279]\n",
            "********* 5010 [D loss: 0.015220, acc:, 100.00] [G loss: 6.597092]\n",
            "********* 5011 [D loss: 0.058931, acc:, 98.44] [G loss: 6.053658]\n",
            "********* 5012 [D loss: 0.064340, acc:, 97.66] [G loss: 7.348648]\n",
            "********* 5013 [D loss: 0.020505, acc:, 100.00] [G loss: 8.354741]\n",
            "********* 5014 [D loss: 0.007495, acc:, 100.00] [G loss: 10.079833]\n",
            "********* 5015 [D loss: 0.054464, acc:, 97.66] [G loss: 9.180354]\n",
            "********* 5016 [D loss: 0.058821, acc:, 98.44] [G loss: 8.108118]\n",
            "********* 5017 [D loss: 0.070770, acc:, 97.66] [G loss: 6.429142]\n",
            "********* 5018 [D loss: 0.117247, acc:, 95.31] [G loss: 6.867244]\n",
            "********* 5019 [D loss: 0.036680, acc:, 99.22] [G loss: 7.211519]\n",
            "********* 5020 [D loss: 0.067038, acc:, 99.22] [G loss: 8.635168]\n",
            "********* 5021 [D loss: 0.048982, acc:, 97.66] [G loss: 7.306198]\n",
            "********* 5022 [D loss: 0.044315, acc:, 99.22] [G loss: 6.362906]\n",
            "********* 5023 [D loss: 0.049847, acc:, 98.44] [G loss: 5.526645]\n",
            "********* 5024 [D loss: 0.105237, acc:, 95.31] [G loss: 7.129922]\n",
            "********* 5025 [D loss: 0.078440, acc:, 97.66] [G loss: 8.524682]\n",
            "********* 5026 [D loss: 0.073716, acc:, 98.44] [G loss: 7.803350]\n",
            "********* 5027 [D loss: 0.105199, acc:, 95.31] [G loss: 6.747285]\n",
            "********* 5028 [D loss: 0.041114, acc:, 98.44] [G loss: 6.466565]\n",
            "********* 5029 [D loss: 0.017853, acc:, 100.00] [G loss: 6.670054]\n",
            "********* 5030 [D loss: 0.043881, acc:, 97.66] [G loss: 7.885156]\n",
            "********* 5031 [D loss: 0.021701, acc:, 99.22] [G loss: 9.461040]\n",
            "********* 5032 [D loss: 0.057519, acc:, 98.44] [G loss: 8.705105]\n",
            "********* 5033 [D loss: 0.018950, acc:, 99.22] [G loss: 7.880567]\n",
            "********* 5034 [D loss: 0.094120, acc:, 96.09] [G loss: 6.522114]\n",
            "********* 5035 [D loss: 0.036995, acc:, 99.22] [G loss: 6.158401]\n",
            "********* 5036 [D loss: 0.020591, acc:, 99.22] [G loss: 7.452896]\n",
            "********* 5037 [D loss: 0.021238, acc:, 100.00] [G loss: 8.336367]\n",
            "********* 5038 [D loss: 0.055806, acc:, 99.22] [G loss: 6.644231]\n",
            "********* 5039 [D loss: 0.076369, acc:, 96.09] [G loss: 4.737935]\n",
            "********* 5040 [D loss: 0.105735, acc:, 96.88] [G loss: 6.948803]\n",
            "********* 5041 [D loss: 0.020976, acc:, 98.44] [G loss: 10.236368]\n",
            "********* 5042 [D loss: 0.086364, acc:, 96.88] [G loss: 9.020540]\n",
            "********* 5043 [D loss: 0.128002, acc:, 94.53] [G loss: 6.002858]\n",
            "********* 5044 [D loss: 0.093189, acc:, 96.88] [G loss: 5.378884]\n",
            "********* 5045 [D loss: 0.021504, acc:, 100.00] [G loss: 6.990711]\n",
            "********* 5046 [D loss: 0.011434, acc:, 100.00] [G loss: 9.187397]\n",
            "********* 5047 [D loss: 0.066865, acc:, 98.44] [G loss: 8.857803]\n",
            "********* 5048 [D loss: 0.074199, acc:, 96.88] [G loss: 8.091187]\n",
            "********* 5049 [D loss: 0.034959, acc:, 98.44] [G loss: 6.964504]\n",
            "********* 5050 [D loss: 0.009441, acc:, 100.00] [G loss: 6.430509]\n",
            "********* 5051 [D loss: 0.019812, acc:, 100.00] [G loss: 7.148595]\n",
            "********* 5052 [D loss: 0.012660, acc:, 100.00] [G loss: 6.648408]\n",
            "********* 5053 [D loss: 0.029013, acc:, 98.44] [G loss: 7.039701]\n",
            "********* 5054 [D loss: 0.021759, acc:, 100.00] [G loss: 7.555658]\n",
            "********* 5055 [D loss: 0.010029, acc:, 100.00] [G loss: 7.907542]\n",
            "********* 5056 [D loss: 0.047738, acc:, 96.88] [G loss: 7.431905]\n",
            "********* 5057 [D loss: 0.062077, acc:, 98.44] [G loss: 7.060542]\n",
            "********* 5058 [D loss: 0.041092, acc:, 99.22] [G loss: 8.293377]\n",
            "********* 5059 [D loss: 0.024947, acc:, 99.22] [G loss: 8.888410]\n",
            "********* 5060 [D loss: 0.074147, acc:, 97.66] [G loss: 7.657790]\n",
            "********* 5061 [D loss: 0.035343, acc:, 98.44] [G loss: 6.956067]\n",
            "********* 5062 [D loss: 0.042486, acc:, 98.44] [G loss: 6.784094]\n",
            "********* 5063 [D loss: 0.016272, acc:, 100.00] [G loss: 7.691967]\n",
            "********* 5064 [D loss: 0.036904, acc:, 98.44] [G loss: 7.012575]\n",
            "********* 5065 [D loss: 0.016282, acc:, 100.00] [G loss: 7.574275]\n",
            "********* 5066 [D loss: 0.010405, acc:, 100.00] [G loss: 7.602830]\n",
            "********* 5067 [D loss: 0.013900, acc:, 100.00] [G loss: 7.109977]\n",
            "********* 5068 [D loss: 0.014207, acc:, 100.00] [G loss: 7.480799]\n",
            "********* 5069 [D loss: 0.025184, acc:, 99.22] [G loss: 6.532425]\n",
            "********* 5070 [D loss: 0.085009, acc:, 97.66] [G loss: 6.474695]\n",
            "********* 5071 [D loss: 0.016459, acc:, 100.00] [G loss: 6.725693]\n",
            "********* 5072 [D loss: 0.053742, acc:, 97.66] [G loss: 8.136136]\n",
            "********* 5073 [D loss: 0.040576, acc:, 98.44] [G loss: 8.825304]\n",
            "********* 5074 [D loss: 0.066530, acc:, 97.66] [G loss: 6.722640]\n",
            "********* 5075 [D loss: 0.085178, acc:, 95.31] [G loss: 5.679843]\n",
            "********* 5076 [D loss: 0.029185, acc:, 98.44] [G loss: 6.033737]\n",
            "********* 5077 [D loss: 0.018839, acc:, 100.00] [G loss: 7.925225]\n",
            "********* 5078 [D loss: 0.070571, acc:, 97.66] [G loss: 6.983145]\n",
            "********* 5079 [D loss: 0.039899, acc:, 98.44] [G loss: 7.033680]\n",
            "********* 5080 [D loss: 0.024267, acc:, 99.22] [G loss: 6.474273]\n",
            "********* 5081 [D loss: 0.035463, acc:, 99.22] [G loss: 7.672566]\n",
            "********* 5082 [D loss: 0.017777, acc:, 100.00] [G loss: 8.503261]\n",
            "********* 5083 [D loss: 0.055222, acc:, 97.66] [G loss: 8.161722]\n",
            "********* 5084 [D loss: 0.109497, acc:, 95.31] [G loss: 6.745947]\n",
            "********* 5085 [D loss: 0.030280, acc:, 100.00] [G loss: 7.200167]\n",
            "********* 5086 [D loss: 0.014392, acc:, 99.22] [G loss: 9.142702]\n",
            "********* 5087 [D loss: 0.070827, acc:, 96.09] [G loss: 7.501835]\n",
            "********* 5088 [D loss: 0.068839, acc:, 97.66] [G loss: 7.080048]\n",
            "********* 5089 [D loss: 0.043122, acc:, 99.22] [G loss: 5.204613]\n",
            "********* 5090 [D loss: 0.044139, acc:, 98.44] [G loss: 6.069594]\n",
            "********* 5091 [D loss: 0.015688, acc:, 100.00] [G loss: 8.752760]\n",
            "********* 5092 [D loss: 0.034420, acc:, 99.22] [G loss: 10.276985]\n",
            "********* 5093 [D loss: 0.050681, acc:, 97.66] [G loss: 8.031595]\n",
            "********* 5094 [D loss: 0.006768, acc:, 100.00] [G loss: 7.315148]\n",
            "********* 5095 [D loss: 0.012487, acc:, 100.00] [G loss: 5.777306]\n",
            "********* 5096 [D loss: 0.023306, acc:, 100.00] [G loss: 8.347994]\n",
            "********* 5097 [D loss: 0.005080, acc:, 100.00] [G loss: 9.193929]\n",
            "********* 5098 [D loss: 0.057894, acc:, 96.09] [G loss: 9.307224]\n",
            "********* 5099 [D loss: 0.033022, acc:, 98.44] [G loss: 8.198719]\n",
            "********* 5100 [D loss: 0.030738, acc:, 98.44] [G loss: 6.171659]\n",
            "********* 5101 [D loss: 0.045736, acc:, 99.22] [G loss: 7.355036]\n",
            "********* 5102 [D loss: 0.005006, acc:, 100.00] [G loss: 7.971570]\n",
            "********* 5103 [D loss: 0.011180, acc:, 100.00] [G loss: 8.523623]\n",
            "********* 5104 [D loss: 0.036301, acc:, 98.44] [G loss: 7.528091]\n",
            "********* 5105 [D loss: 0.041051, acc:, 98.44] [G loss: 7.397362]\n",
            "********* 5106 [D loss: 0.015886, acc:, 100.00] [G loss: 7.305929]\n",
            "********* 5107 [D loss: 0.028109, acc:, 99.22] [G loss: 7.667861]\n",
            "********* 5108 [D loss: 0.059395, acc:, 97.66] [G loss: 8.472971]\n",
            "********* 5109 [D loss: 0.134388, acc:, 97.66] [G loss: 10.797468]\n",
            "********* 5110 [D loss: 0.045181, acc:, 98.44] [G loss: 8.940165]\n",
            "********* 5111 [D loss: 0.059381, acc:, 97.66] [G loss: 7.454161]\n",
            "********* 5112 [D loss: 0.101965, acc:, 96.09] [G loss: 7.195519]\n",
            "********* 5113 [D loss: 0.005153, acc:, 100.00] [G loss: 9.813468]\n",
            "********* 5114 [D loss: 0.013626, acc:, 99.22] [G loss: 11.602598]\n",
            "********* 5115 [D loss: 0.024320, acc:, 98.44] [G loss: 11.796212]\n",
            "********* 5116 [D loss: 0.101660, acc:, 94.53] [G loss: 8.380089]\n",
            "********* 5117 [D loss: 0.023324, acc:, 99.22] [G loss: 6.529625]\n",
            "********* 5118 [D loss: 0.054531, acc:, 98.44] [G loss: 6.626929]\n",
            "********* 5119 [D loss: 0.003799, acc:, 100.00] [G loss: 8.572170]\n",
            "********* 5120 [D loss: 0.058082, acc:, 97.66] [G loss: 9.067598]\n",
            "********* 5121 [D loss: 0.010769, acc:, 100.00] [G loss: 9.066971]\n",
            "********* 5122 [D loss: 0.012267, acc:, 100.00] [G loss: 8.448095]\n",
            "********* 5123 [D loss: 0.048183, acc:, 98.44] [G loss: 8.340904]\n",
            "********* 5124 [D loss: 0.022122, acc:, 100.00] [G loss: 7.918915]\n",
            "********* 5125 [D loss: 0.021618, acc:, 99.22] [G loss: 9.369257]\n",
            "********* 5126 [D loss: 0.033497, acc:, 98.44] [G loss: 7.724030]\n",
            "********* 5127 [D loss: 0.056403, acc:, 99.22] [G loss: 8.408756]\n",
            "********* 5128 [D loss: 0.054961, acc:, 97.66] [G loss: 8.803175]\n",
            "********* 5129 [D loss: 0.012706, acc:, 100.00] [G loss: 9.514490]\n",
            "********* 5130 [D loss: 0.043677, acc:, 98.44] [G loss: 8.482105]\n",
            "********* 5131 [D loss: 0.064878, acc:, 98.44] [G loss: 6.285543]\n",
            "********* 5132 [D loss: 0.078566, acc:, 97.66] [G loss: 6.986444]\n",
            "********* 5133 [D loss: 0.024154, acc:, 99.22] [G loss: 9.755262]\n",
            "********* 5134 [D loss: 0.017945, acc:, 99.22] [G loss: 11.302601]\n",
            "********* 5135 [D loss: 0.026152, acc:, 99.22] [G loss: 10.986814]\n",
            "********* 5136 [D loss: 0.025499, acc:, 99.22] [G loss: 8.663219]\n",
            "********* 5137 [D loss: 0.035016, acc:, 99.22] [G loss: 7.480103]\n",
            "********* 5138 [D loss: 0.039869, acc:, 99.22] [G loss: 7.093498]\n",
            "********* 5139 [D loss: 0.043919, acc:, 97.66] [G loss: 7.979579]\n",
            "********* 5140 [D loss: 0.010309, acc:, 100.00] [G loss: 10.023715]\n",
            "********* 5141 [D loss: 0.028872, acc:, 99.22] [G loss: 10.178394]\n",
            "********* 5142 [D loss: 0.082521, acc:, 98.44] [G loss: 8.157332]\n",
            "********* 5143 [D loss: 0.115069, acc:, 95.31] [G loss: 7.639933]\n",
            "********* 5144 [D loss: 0.036954, acc:, 99.22] [G loss: 10.250125]\n",
            "********* 5145 [D loss: 0.013926, acc:, 100.00] [G loss: 10.168215]\n",
            "********* 5146 [D loss: 0.043641, acc:, 98.44] [G loss: 10.314299]\n",
            "********* 5147 [D loss: 0.059341, acc:, 98.44] [G loss: 8.558931]\n",
            "********* 5148 [D loss: 0.029635, acc:, 100.00] [G loss: 8.070740]\n",
            "********* 5149 [D loss: 0.018052, acc:, 100.00] [G loss: 9.186263]\n",
            "********* 5150 [D loss: 0.009867, acc:, 99.22] [G loss: 10.994333]\n",
            "********* 5151 [D loss: 0.011047, acc:, 100.00] [G loss: 10.432085]\n",
            "********* 5152 [D loss: 0.028487, acc:, 99.22] [G loss: 9.487007]\n",
            "********* 5153 [D loss: 0.009123, acc:, 100.00] [G loss: 9.644226]\n",
            "********* 5154 [D loss: 0.022328, acc:, 100.00] [G loss: 10.129819]\n",
            "********* 5155 [D loss: 0.023421, acc:, 99.22] [G loss: 9.773148]\n",
            "********* 5156 [D loss: 0.056638, acc:, 99.22] [G loss: 10.753328]\n",
            "********* 5157 [D loss: 0.034625, acc:, 98.44] [G loss: 7.653427]\n",
            "********* 5158 [D loss: 0.015653, acc:, 100.00] [G loss: 8.985435]\n",
            "********* 5159 [D loss: 0.026345, acc:, 98.44] [G loss: 9.684608]\n",
            "********* 5160 [D loss: 0.025406, acc:, 99.22] [G loss: 6.711342]\n",
            "********* 5161 [D loss: 0.012752, acc:, 100.00] [G loss: 7.253779]\n",
            "********* 5162 [D loss: 0.057406, acc:, 97.66] [G loss: 7.810510]\n",
            "********* 5163 [D loss: 0.022084, acc:, 99.22] [G loss: 7.216950]\n",
            "********* 5164 [D loss: 0.028036, acc:, 98.44] [G loss: 9.409416]\n",
            "********* 5165 [D loss: 0.038251, acc:, 99.22] [G loss: 8.677514]\n",
            "********* 5166 [D loss: 0.025132, acc:, 100.00] [G loss: 9.205105]\n",
            "********* 5167 [D loss: 0.011609, acc:, 100.00] [G loss: 10.980930]\n",
            "********* 5168 [D loss: 0.028963, acc:, 99.22] [G loss: 9.944291]\n",
            "********* 5169 [D loss: 0.047586, acc:, 97.66] [G loss: 8.341617]\n",
            "********* 5170 [D loss: 0.130681, acc:, 96.09] [G loss: 7.141804]\n",
            "********* 5171 [D loss: 0.007825, acc:, 100.00] [G loss: 8.878956]\n",
            "********* 5172 [D loss: 0.025986, acc:, 99.22] [G loss: 10.572277]\n",
            "********* 5173 [D loss: 0.017460, acc:, 100.00] [G loss: 8.702306]\n",
            "********* 5174 [D loss: 0.026303, acc:, 99.22] [G loss: 8.818722]\n",
            "********* 5175 [D loss: 0.016215, acc:, 100.00] [G loss: 8.681128]\n",
            "********* 5176 [D loss: 0.043423, acc:, 98.44] [G loss: 9.655068]\n",
            "********* 5177 [D loss: 0.018250, acc:, 99.22] [G loss: 10.881598]\n",
            "********* 5178 [D loss: 0.064077, acc:, 98.44] [G loss: 8.140396]\n",
            "********* 5179 [D loss: 0.052249, acc:, 98.44] [G loss: 8.274410]\n",
            "********* 5180 [D loss: 0.029663, acc:, 99.22] [G loss: 9.101415]\n",
            "********* 5181 [D loss: 0.121388, acc:, 94.53] [G loss: 10.030600]\n",
            "********* 5182 [D loss: 0.018711, acc:, 99.22] [G loss: 12.317965]\n",
            "********* 5183 [D loss: 0.035166, acc:, 98.44] [G loss: 11.548180]\n",
            "********* 5184 [D loss: 0.068594, acc:, 95.31] [G loss: 10.014189]\n",
            "********* 5185 [D loss: 0.031899, acc:, 98.44] [G loss: 10.155762]\n",
            "********* 5186 [D loss: 0.010524, acc:, 100.00] [G loss: 11.293637]\n",
            "********* 5187 [D loss: 0.071254, acc:, 95.31] [G loss: 14.042001]\n",
            "********* 5188 [D loss: 0.318703, acc:, 95.31] [G loss: 10.367182]\n",
            "********* 5189 [D loss: 0.112429, acc:, 94.53] [G loss: 12.391239]\n",
            "********* 5190 [D loss: 0.204172, acc:, 94.53] [G loss: 13.294533]\n",
            "********* 5191 [D loss: 0.025656, acc:, 98.44] [G loss: 15.325829]\n",
            "********* 5192 [D loss: 0.373805, acc:, 91.41] [G loss: 13.381699]\n",
            "********* 5193 [D loss: 0.033147, acc:, 97.66] [G loss: 11.537189]\n",
            "********* 5194 [D loss: 0.095483, acc:, 95.31] [G loss: 13.492267]\n",
            "********* 5195 [D loss: 0.009372, acc:, 100.00] [G loss: 13.896980]\n",
            "********* 5196 [D loss: 0.033660, acc:, 98.44] [G loss: 15.192037]\n",
            "********* 5197 [D loss: 0.100660, acc:, 98.44] [G loss: 13.518198]\n",
            "********* 5198 [D loss: 0.063593, acc:, 97.66] [G loss: 10.380234]\n",
            "********* 5199 [D loss: 0.147508, acc:, 96.88] [G loss: 9.872072]\n",
            "********* 5200 [D loss: 0.038028, acc:, 98.44] [G loss: 11.273273]\n",
            "********* 5201 [D loss: 0.029181, acc:, 99.22] [G loss: 14.753746]\n",
            "********* 5202 [D loss: 0.076903, acc:, 96.09] [G loss: 12.898376]\n",
            "********* 5203 [D loss: 0.082920, acc:, 97.66] [G loss: 8.967865]\n",
            "********* 5204 [D loss: 0.118297, acc:, 94.53] [G loss: 8.834491]\n",
            "********* 5205 [D loss: 0.035510, acc:, 99.22] [G loss: 13.744650]\n",
            "********* 5206 [D loss: 0.117628, acc:, 96.88] [G loss: 10.437315]\n",
            "********* 5207 [D loss: 0.109135, acc:, 96.88] [G loss: 5.068924]\n",
            "********* 5208 [D loss: 0.343737, acc:, 85.16] [G loss: 9.504084]\n",
            "********* 5209 [D loss: 0.011464, acc:, 99.22] [G loss: 23.490873]\n",
            "********* 5210 [D loss: 1.109279, acc:, 85.94] [G loss: 7.034189]\n",
            "********* 5211 [D loss: 0.301034, acc:, 85.94] [G loss: 5.212971]\n",
            "********* 5212 [D loss: 0.060631, acc:, 96.88] [G loss: 10.179567]\n",
            "********* 5213 [D loss: 0.016864, acc:, 99.22] [G loss: 14.657030]\n",
            "********* 5214 [D loss: 0.093884, acc:, 95.31] [G loss: 16.762575]\n",
            "********* 5215 [D loss: 0.260093, acc:, 89.84] [G loss: 10.858213]\n",
            "********* 5216 [D loss: 0.059432, acc:, 97.66] [G loss: 9.365650]\n",
            "********* 5217 [D loss: 0.098349, acc:, 96.09] [G loss: 8.800837]\n",
            "********* 5218 [D loss: 0.053635, acc:, 96.88] [G loss: 10.068180]\n",
            "********* 5219 [D loss: 0.033246, acc:, 98.44] [G loss: 11.105266]\n",
            "********* 5220 [D loss: 0.072151, acc:, 98.44] [G loss: 13.254126]\n",
            "********* 5221 [D loss: 0.014558, acc:, 99.22] [G loss: 13.010575]\n",
            "********* 5222 [D loss: 0.010135, acc:, 100.00] [G loss: 11.751148]\n",
            "********* 5223 [D loss: 0.085833, acc:, 95.31] [G loss: 7.392778]\n",
            "********* 5224 [D loss: 0.062725, acc:, 96.88] [G loss: 6.704601]\n",
            "********* 5225 [D loss: 0.142830, acc:, 93.75] [G loss: 10.698750]\n",
            "********* 5226 [D loss: 0.077407, acc:, 97.66] [G loss: 13.286567]\n",
            "********* 5227 [D loss: 0.686370, acc:, 82.81] [G loss: 4.444756]\n",
            "********* 5228 [D loss: 0.856878, acc:, 77.34] [G loss: 5.779424]\n",
            "********* 5229 [D loss: 0.036498, acc:, 98.44] [G loss: 12.320166]\n",
            "********* 5230 [D loss: 0.023890, acc:, 99.22] [G loss: 19.929703]\n",
            "********* 5231 [D loss: 0.268752, acc:, 92.19] [G loss: 17.941277]\n",
            "********* 5232 [D loss: 0.207456, acc:, 92.97] [G loss: 12.245813]\n",
            "********* 5233 [D loss: 0.041594, acc:, 99.22] [G loss: 10.768171]\n",
            "********* 5234 [D loss: 0.043460, acc:, 98.44] [G loss: 8.247584]\n",
            "********* 5235 [D loss: 0.203442, acc:, 92.19] [G loss: 7.809403]\n",
            "********* 5236 [D loss: 0.029752, acc:, 99.22] [G loss: 9.569425]\n",
            "********* 5237 [D loss: 0.021884, acc:, 99.22] [G loss: 11.033984]\n",
            "********* 5238 [D loss: 0.030638, acc:, 99.22] [G loss: 10.014782]\n",
            "********* 5239 [D loss: 0.009303, acc:, 100.00] [G loss: 8.809101]\n",
            "********* 5240 [D loss: 0.005599, acc:, 100.00] [G loss: 8.727082]\n",
            "********* 5241 [D loss: 0.027828, acc:, 99.22] [G loss: 8.795685]\n",
            "********* 5242 [D loss: 0.016071, acc:, 100.00] [G loss: 6.865744]\n",
            "********* 5243 [D loss: 0.022390, acc:, 99.22] [G loss: 6.974507]\n",
            "********* 5244 [D loss: 0.069079, acc:, 99.22] [G loss: 8.051403]\n",
            "********* 5245 [D loss: 0.035714, acc:, 99.22] [G loss: 9.127508]\n",
            "********* 5246 [D loss: 0.046046, acc:, 98.44] [G loss: 8.284307]\n",
            "********* 5247 [D loss: 0.028563, acc:, 98.44] [G loss: 9.018422]\n",
            "********* 5248 [D loss: 0.092499, acc:, 96.88] [G loss: 6.494114]\n",
            "********* 5249 [D loss: 0.089138, acc:, 96.88] [G loss: 8.995943]\n",
            "********* 5250 [D loss: 0.115949, acc:, 95.31] [G loss: 7.725145]\n",
            "********* 5251 [D loss: 0.035991, acc:, 98.44] [G loss: 7.727494]\n",
            "********* 5252 [D loss: 0.077060, acc:, 96.88] [G loss: 7.950890]\n",
            "********* 5253 [D loss: 0.052297, acc:, 99.22] [G loss: 8.146278]\n",
            "********* 5254 [D loss: 0.030953, acc:, 98.44] [G loss: 8.391507]\n",
            "********* 5255 [D loss: 0.049499, acc:, 97.66] [G loss: 7.792109]\n",
            "********* 5256 [D loss: 0.026481, acc:, 99.22] [G loss: 7.592387]\n",
            "********* 5257 [D loss: 0.037687, acc:, 99.22] [G loss: 7.524226]\n",
            "********* 5258 [D loss: 0.047344, acc:, 98.44] [G loss: 10.081467]\n",
            "********* 5259 [D loss: 0.031358, acc:, 98.44] [G loss: 8.976275]\n",
            "********* 5260 [D loss: 0.035653, acc:, 99.22] [G loss: 8.351332]\n",
            "********* 5261 [D loss: 0.017583, acc:, 100.00] [G loss: 6.431393]\n",
            "********* 5262 [D loss: 0.024146, acc:, 100.00] [G loss: 7.048610]\n",
            "********* 5263 [D loss: 0.009784, acc:, 100.00] [G loss: 8.574863]\n",
            "********* 5264 [D loss: 0.020035, acc:, 98.44] [G loss: 9.632895]\n",
            "********* 5265 [D loss: 0.068831, acc:, 96.09] [G loss: 6.618533]\n",
            "********* 5266 [D loss: 0.131919, acc:, 92.97] [G loss: 9.053172]\n",
            "********* 5267 [D loss: 0.032638, acc:, 99.22] [G loss: 13.066638]\n",
            "********* 5268 [D loss: 0.059267, acc:, 97.66] [G loss: 12.791527]\n",
            "********* 5269 [D loss: 0.017637, acc:, 100.00] [G loss: 11.789787]\n",
            "********* 5270 [D loss: 0.071595, acc:, 97.66] [G loss: 7.593577]\n",
            "********* 5271 [D loss: 0.172528, acc:, 93.75] [G loss: 7.458457]\n",
            "********* 5272 [D loss: 0.039998, acc:, 97.66] [G loss: 8.484159]\n",
            "********* 5273 [D loss: 0.044621, acc:, 97.66] [G loss: 10.302176]\n",
            "********* 5274 [D loss: 0.051707, acc:, 96.88] [G loss: 10.630216]\n",
            "********* 5275 [D loss: 0.016806, acc:, 100.00] [G loss: 9.733585]\n",
            "********* 5276 [D loss: 0.022821, acc:, 100.00] [G loss: 7.705466]\n",
            "********* 5277 [D loss: 0.050904, acc:, 96.88] [G loss: 7.791118]\n",
            "********* 5278 [D loss: 0.016904, acc:, 100.00] [G loss: 8.153587]\n",
            "********* 5279 [D loss: 0.010121, acc:, 100.00] [G loss: 9.148613]\n",
            "********* 5280 [D loss: 0.012144, acc:, 100.00] [G loss: 9.721407]\n",
            "********* 5281 [D loss: 0.083655, acc:, 96.88] [G loss: 7.446382]\n",
            "********* 5282 [D loss: 0.096224, acc:, 97.66] [G loss: 7.425427]\n",
            "********* 5283 [D loss: 0.019396, acc:, 100.00] [G loss: 9.007764]\n",
            "********* 5284 [D loss: 0.011379, acc:, 100.00] [G loss: 11.352606]\n",
            "********* 5285 [D loss: 0.072366, acc:, 97.66] [G loss: 10.392552]\n",
            "********* 5286 [D loss: 0.123088, acc:, 95.31] [G loss: 4.933132]\n",
            "********* 5287 [D loss: 0.431797, acc:, 81.25] [G loss: 11.689309]\n",
            "********* 5288 [D loss: 0.045856, acc:, 97.66] [G loss: 22.068008]\n",
            "********* 5289 [D loss: 0.532137, acc:, 85.16] [G loss: 14.032304]\n",
            "********* 5290 [D loss: 0.062819, acc:, 96.88] [G loss: 9.273050]\n",
            "********* 5291 [D loss: 0.183867, acc:, 92.19] [G loss: 8.325238]\n",
            "********* 5292 [D loss: 0.010904, acc:, 99.22] [G loss: 10.980635]\n",
            "********* 5293 [D loss: 0.039740, acc:, 97.66] [G loss: 15.018736]\n",
            "********* 5294 [D loss: 0.038068, acc:, 98.44] [G loss: 16.112082]\n",
            "********* 5295 [D loss: 0.173085, acc:, 93.75] [G loss: 11.568565]\n",
            "********* 5296 [D loss: 0.013241, acc:, 100.00] [G loss: 8.859779]\n",
            "********* 5297 [D loss: 0.046918, acc:, 97.66] [G loss: 7.490218]\n",
            "********* 5298 [D loss: 0.054839, acc:, 98.44] [G loss: 8.278633]\n",
            "********* 5299 [D loss: 0.010753, acc:, 100.00] [G loss: 10.731281]\n",
            "********* 5300 [D loss: 0.008706, acc:, 100.00] [G loss: 12.038119]\n",
            "********* 5301 [D loss: 0.012886, acc:, 99.22] [G loss: 12.571162]\n",
            "********* 5302 [D loss: 0.019485, acc:, 100.00] [G loss: 12.133652]\n",
            "********* 5303 [D loss: 0.035089, acc:, 98.44] [G loss: 9.529675]\n",
            "********* 5304 [D loss: 0.121184, acc:, 96.09] [G loss: 6.244251]\n",
            "********* 5305 [D loss: 0.129620, acc:, 93.75] [G loss: 6.698807]\n",
            "********* 5306 [D loss: 0.029755, acc:, 99.22] [G loss: 9.501593]\n",
            "********* 5307 [D loss: 0.047080, acc:, 99.22] [G loss: 11.065068]\n",
            "********* 5308 [D loss: 0.111549, acc:, 96.09] [G loss: 9.429092]\n",
            "********* 5309 [D loss: 0.037114, acc:, 98.44] [G loss: 6.993663]\n",
            "********* 5310 [D loss: 0.058118, acc:, 97.66] [G loss: 6.150635]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8f6df0f32a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m  \u001b[0;31m# print(X_train.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-8f6df0f32a7b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mgen_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m#Train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1766\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     dataset = tf.data.Dataset.zip((\n\u001b[1;32m    352\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m     ))\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    699\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \"\"\"\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m   4646\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4647\u001b[0m         \u001b[0moutput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flat_tensor_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4648\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m   4649\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mtensor_dataset\u001b[0;34m(components, output_shapes, metadata, name)\u001b[0m\n\u001b[1;32m   7373\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   7374\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TensorDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7375\u001b[0;31m         output_shapes, \"metadata\", metadata)\n\u001b[0m\u001b[1;32m   7376\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7377\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-jSQoN1Azl"
      },
      "source": [
        "### **8) Making GIF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPShgQpg1EMy"
      },
      "source": [
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('generated_images/*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh37uv1torG5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}